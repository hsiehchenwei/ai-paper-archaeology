<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <!-- Primary Meta Tags -->
    <title>AI Paper Archaeology - 深入考古改變世界的 AI 論文</title>
    <meta
      name="title"
      content="AI Paper Archaeology - 深入考古改變世界的 AI 論文"
    />
    <meta
      name="description"
      content="深入解析改變 AI 世界的經典論文，從 Transformer 到 GPT-3、BERT、InstructGPT，用白話文帶你考古 AI 發展史。包含論文逐句翻譯、技術深度解析與生活化類比。"
    />
    <meta
      name="keywords"
      content="AI, 人工智慧, 論文解析, Transformer, BERT, GPT-3, InstructGPT, 機器學習, 深度學習, NLP, 自然語言處理, 白話文教學, 技術科普"
    />
    <meta name="author" content="謝承緯 (Chen Wei Hsieh)" />
    <meta name="robots" content="index, follow" />
    <link
      rel="canonical"
      href="https://hsiehchenwei.github.io/ai-paper-archaeology/"
    />

    <!-- Google tag (gtag.js) -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-70B8N72F6F"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-70B8N72F6F");
    </script>

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website" />
    <meta
      property="og:url"
      content="https://hsiehchenwei.github.io/ai-paper-archaeology/"
    />
    <meta
      property="og:title"
      content="AI Paper Archaeology - 深入考古改變世界的 AI 論文"
    />
    <meta
      property="og:description"
      content="深入解析改變 AI 世界的經典論文，從 Transformer 到 GPT，用白話文帶你考古 AI 發展史"
    />
    <meta
      property="og:image"
      content="https://hsiehchenwei.github.io/ai-paper-archaeology/og-image.png"
    />
    <meta property="og:locale" content="zh_TW" />
    <meta property="og:site_name" content="AI Paper Archaeology" />

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image" />
    <meta
      property="twitter:url"
      content="https://hsiehchenwei.github.io/ai-paper-archaeology/"
    />
    <meta
      property="twitter:title"
      content="AI Paper Archaeology - 深入考古改變世界的 AI 論文"
    />
    <meta
      property="twitter:description"
      content="深入解析改變 AI 世界的經典論文，從 Transformer 到 GPT，用白話文帶你考古 AI 發展史"
    />
    <meta
      property="twitter:image"
      content="https://hsiehchenwei.github.io/ai-paper-archaeology/og-image.png"
    />

    <!-- Structured Data / Schema.org -->
    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "WebSite",
        "name": "AI Paper Archaeology",
        "alternateName": "AI 論文考古",
        "url": "https://hsiehchenwei.github.io/ai-paper-archaeology/",
        "description": "深入解析改變 AI 世界的經典論文，從 Transformer 到 GPT，用白話文帶你考古 AI 發展史",
        "author": {
          "@type": "Person",
          "name": "謝承緯",
          "email": "pushy.chordal_0s@icloud.com"
        },
        "inLanguage": "zh-TW",
        "potentialAction": {
          "@type": "SearchAction",
          "target": "https://hsiehchenwei.github.io/ai-paper-archaeology/?q={search_term_string}",
          "query-input": "required name=search_term_string"
        }
      }
    </script>

    <link rel="stylesheet" href="styles/global.css" />
    <link rel="stylesheet" href="styles/home.css" />
  </head>
  <body>
    <!-- Hero Section -->
    <div class="container">
      <div class="hero">
        <div class="hero-content">
          <h1>考古 AI 的<br />黃金年代</h1>
          <p class="hero-subtitle">
            從 2017 Transformer 到 2023 GPT-4<br />
            用白話文解析那些改變世界的經典論文
          </p>
        </div>
      </div>

      <!-- Latest Updates Section -->
      <div class="latest-updates">
        <div class="updates-header">
          <span class="updates-icon">🔔</span>
          <span class="updates-title">最新更新</span>
        </div>
        <div class="updates-list">
          <a href="nested-learning-tutorial/index.html" class="update-item">
            <span class="update-date">2026-01-12</span>
            <span class="update-text"
              >Nested Learning 論文深度解析 - 打破「深度」學習的迷思，重新定義 AI 學習本質</span
            >
            <span class="update-badge">NEW</span>
          </a>
          <a href="swin-transformer-tutorial/index.html" class="update-item">
            <span class="update-date">2026-01-10</span>
            <span class="update-text"
              >Swin Transformer 論文深度解析 - 層次化視覺 Transformer
              的開創</span
            >
          </a>
          <a href="gemini-tutorial/index.html" class="update-item">
            <span class="update-date">2026-01-10</span>
            <span class="update-text"
              >Gemini 論文深度解析 - 首個在 MMLU
              達到人類專家水準的多模態模型</span
            >
          </a>
          <a href="vit-tutorial/index.html" class="update-item">
            <span class="update-date">2026-01-09</span>
            <span class="update-text"
              >Vision Transformer (ViT) 論文深度解析 - 圖片就是 16x16
              個詞彙</span
            >
          </a>
          <a href="llava-tutorial/index.html" class="update-item">
            <span class="update-date">2026-01-08</span>
            <span class="update-text"
              >LLaVA 論文深度解析 - 視覺指令微調的開創性工作</span
            >
          </a>
        </div>
      </div>

      <!-- Tab Navigation -->
      <div class="tab-container" id="featured">
        <div class="tab-nav">
          <button class="tab-btn active" onclick="switchTab('timeline')">
            論文時間軸
          </button>
          <button class="tab-btn" onclick="switchTab('topics')">
            精選專題
          </button>
          <button class="tab-btn" onclick="switchTab('articles')">
            知識筆記
          </button>
          <button class="tab-btn" onclick="switchTab('about')">關於本站</button>
        </div>

        <!-- Tab 2: Topics (Bento Grid) -->
        <div id="topics" class="tab-content">
          <div class="bento-grid">
            <!-- Featured Card: LLM Visualization Tutorial -->
            <div
              class="bento-card featured"
              onclick="location.href='llm-visualization-zh/index.html'"
              style="cursor: pointer"
            >
              <div
                class="card-icon"
                style="
                  background: transparent;
                  width: 100px;
                  height: 100px;
                  margin-bottom: 20px;
                  padding: 0;
                "
              >
                <img
                  src="images/llm_architecture_full_20260105210724.png"
                  alt="LLM Visualization Icon"
                  style="
                    width: 100%;
                    height: 100%;
                    object-fit: contain;
                    border-radius: 10px;
                  "
                />
              </div>
              <h3>LLM 視覺化完整教學</h3>
              <p>
                從 nano-gpt 的 85,000 個參數開始，逐步深入理解 GPT 的運作原理。
                完整解析 Embedding、Self-Attention、Multi-Head Attention、Layer
                Normalization、 Feed-Forward Network、Softmax 等核心機制。
              </p>
              <div class="card-meta">
                <span>2026 最新</span>
                <span>•</span>
                <span>6 章完整教學</span>
              </div>
            </div>

            <!-- Featured Card: NLP Trilogy -->
            <div
              class="bento-card"
              onclick="location.href='topics/nlp-trilogy.html'"
              style="cursor: pointer"
            >
              <div
                class="card-icon"
                style="
                  background: transparent;
                  width: 100px;
                  height: 100px;
                  margin-bottom: 20px;
                  padding: 0;
                "
              >
                <img
                  src="images/user_generate_image_20260102041052_792e_256.png"
                  alt="NLP Trilogy Icon"
                  style="width: 100%; height: 100%; object-fit: contain"
                />
              </div>
              <h3>現代 NLP 演進三部曲</h3>
              <p>
                探索改變 NLP 世界的關鍵論文：從 Transformer 架構革命， 到 BERT
                的雙向理解，再到 GPT-3 的規模奇蹟， 最後以 InstructGPT
                實現人類對齊。
              </p>
              <div class="card-meta">
                <span>2017-2022</span>
                <span>•</span>
                <span>4 篇核心論文</span>
              </div>
            </div>

            <!-- Featured Card: Prompt Paradigm -->
            <div
              class="bento-card"
              onclick="location.href='topics/prompt-paradigm.html'"
              style="cursor: pointer"
            >
              <div
                class="card-icon"
                style="
                  background: transparent;
                  width: 100px;
                  height: 100px;
                  margin-bottom: 20px;
                  padding: 0;
                "
              >
                <img
                  src="images/user_generate_image_20260102062435_7be9.png"
                  alt="Prompt Paradigm Icon"
                  style="width: 100%; height: 100%; object-fit: contain"
                />
              </div>
              <h3>Prompt 提示詞的範式革命</h3>
              <p>
                從 GPT-3 的驚人發現，到 2021 年「第四範式」的理論正名。 深入理解
                Prompt 如何從一種黑魔法技巧， 演變成現代 AI 的核心開發範式。
              </p>
              <div class="card-meta">
                <span>2020-2021</span>
                <span>•</span>
                <span>核心理論專題</span>
              </div>
            </div>

            <!-- Featured Card: Multimodal Evaluation -->
            <div
              class="bento-card"
              onclick="location.href='topics/multimodal-evaluation.html'"
              style="cursor: pointer"
            >
              <div
                class="card-icon"
                style="
                  background: linear-gradient(
                    135deg,
                    #ef4444 0%,
                    #f97316 50%,
                    #8b5cf6 100%
                  );
                  width: 100px;
                  height: 100px;
                  margin-bottom: 20px;
                  padding: 0;
                  display: flex;
                  align-items: center;
                  justify-content: center;
                  font-size: 3rem;
                  border-radius: 10px;
                "
              >
                🔍
              </div>
              <h3>多模態評測三部曲</h3>
              <p>
                探索三篇震撼研究：MME-RealWorld、MMMU-Pro、RBench-V。 揭示多模態
                AI 在真實場景、嚴格評測、視覺推理中的三大盲點。
              </p>
              <div class="card-meta">
                <span>2024-2025</span>
                <span>•</span>
                <span>3 篇評測論文</span>
              </div>
            </div>
          </div>
        </div>

        <!-- Tab 1: Timeline -->
        <div id="timeline" class="tab-content active">
          <div
            class="bento-card"
            style="
              background: transparent;
              border: none;
              box-shadow: none;
              padding: 0;
            "
          >
            <h3 style="text-align: center; margin-bottom: 10px">
              ⏳ AI 發展黃金年代
            </h3>

            <div class="timeline-container">
              <div class="timeline-line"></div>

              <!-- 2017 Transformer -->
              <div class="timeline-item right">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                    "
                  >
                    <img
                      src="images/user_generate_image_20260102041052_792e_256.png"
                      style="width: 56px; height: 56px; object-fit: contain"
                      alt="Transformer Icon"
                    />
                    <h3 style="margin: 0">2017 Transformer</h3>
                  </div>
                  <strong>Attention Is All You Need</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    Google Brain / Google Research
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    拋棄 RNN，用純注意力機制奠定現代 LLM
                    基礎。現代深度學習的基石，所有 LLM 的架構基礎，引用數超過 10
                    萬次，徹底改變了 NLP 和 AI 領域。
                  </p>
                  <a href="transformer-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2018 BERT -->
              <div class="timeline-item left">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                      justify-content: flex-end;
                    "
                  >
                    <h3 style="margin: 0">2018 BERT</h3>
                    <img
                      src="images/user_generate_image_20260102041123_05b5_256.png"
                      style="width: 56px; height: 56px; object-fit: contain"
                      alt="BERT Icon"
                    />
                  </div>
                  <strong>雙向理解革命</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    Google AI Language
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    Masked LM
                    讓機器真正「讀懂」上下文。雙向預訓練的開創性工作，在 11 項
                    NLP 任務上達到 SOTA，引用數超過 8
                    萬次，成為預訓練模型的標準架構。
                  </p>
                  <a href="bert-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2019 GPT-2 -->
              <div class="timeline-item right">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                    "
                  >
                    <img
                      src="images/gpt2_hero_20260106.png"
                      style="
                        width: 56px;
                        height: 56px;
                        object-fit: cover;
                        border-radius: 8px;
                      "
                      alt="GPT-2 Icon"
                    />
                    <h3 style="margin: 0">2019 GPT-2</h3>
                  </div>
                  <strong>Zero-Shot Learning 的起源</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    OpenAI
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    意外發現：不用微調也能執行多任務。首次證明語言模型可以「零範例」執行任務，發現語言模型的「湧現能力」，證明大規模模型可以零樣本執行多種任務，為後續
                    GPT-3、ChatGPT 奠定基礎。
                  </p>
                  <a href="gpt2-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2020 Scaling Laws -->
              <div class="timeline-item left">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                      justify-content: flex-end;
                    "
                  >
                    <h3 style="margin: 0">2020 Scaling Laws</h3>
                    <img
                      src="scaling-laws-tutorial/images/chapter01_hero.png"
                      style="
                        width: 56px;
                        height: 56px;
                        object-fit: cover;
                        border-radius: 8px;
                      "
                      alt="Scaling Laws Icon"
                    />
                  </div>
                  <strong>大模型時代的科學預言</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    OpenAI
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    發現性能與規模的冪律關係，預言 GPT-3
                    成功。證明模型性能隨參數、數據、計算量呈現可預測的冪律增長，打破「收斂迷思」，建立大模型訓練的科學基礎，成為所有大模型訓練的指導原則。
                  </p>
                  <a href="scaling-laws-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2020 GPT-3 -->
              <div class="timeline-item right">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                    "
                  >
                    <img
                      src="images/user_generate_image_20260102041143_3328_256.png"
                      style="width: 56px; height: 56px; object-fit: contain"
                      alt="GPT-3 Icon"
                    />
                    <h3 style="margin: 0">2020 GPT-3</h3>
                  </div>
                  <strong>Few-Shot Learning 的威力</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    OpenAI
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    承接 GPT-2，證明「給幾個範例」效果驚人。175B
                    參數讓模型從「幾個範例」中學會新任務，效果接近專業模型。開啟大語言模型時代，證明規模化帶來的湧現能力，直接催生
                    ChatGPT 和現代 AI 應用。
                  </p>
                  <a href="gpt3-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2020 ViT -->
              <div class="timeline-item left">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                      justify-content: flex-end;
                    "
                  >
                    <h3 style="margin: 0">2020 ViT</h3>
                    <img
                      src="vit-tutorial/images/generated/index_hero.png"
                      style="
                        width: 56px;
                        height: 56px;
                        object-fit: cover;
                        border-radius: 8px;
                      "
                      alt="ViT Icon"
                    />
                  </div>
                  <strong>圖片就是 16x16 個詞彙</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    Google Research
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    純 Transformer 應用到視覺，開啟後 CNN 時代。把圖片切成 16x16
                    的碎片，用 Transformer
                    處理，證明了「大規模訓練勝過歸納偏置」，在 JFT-300M
                    上超越所有 CNN，成為現代視覺模型的基礎架構。
                  </p>
                  <a href="vit-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2021 Prompt Paradigm -->
              <div class="timeline-item left">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                      justify-content: flex-end;
                    "
                  >
                    <h3 style="margin: 0">2021 Prompt Paradigm</h3>
                    <img
                      src="images/user_generate_image_20260102062435_7be9.png"
                      style="width: 56px; height: 56px; object-fit: contain"
                      alt="Prompt Paradigm Icon"
                    />
                  </div>
                  <strong>理論正名：第四範式</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    Carnegie Mellon University
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    將 Zero/Few-Shot 系統化成 NLP 新範式。為 GPT-2 和 GPT-3
                    的實踐提供理論框架，將「Prompt」從技巧昇華為科學，系統化整理
                    Prompt Engineering 理論，正式宣告 NLP
                    從「微調時代」進入「提示時代」。
                  </p>
                  <a
                    href="prompt-engineering-tutorial/index.html"
                    class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2021 CLIP -->
              <div class="timeline-item right">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                    "
                  >
                    <img
                      src="clip-tutorial/images/chapter01_hero.png"
                      style="
                        width: 56px;
                        height: 56px;
                        object-fit: cover;
                        border-radius: 8px;
                      "
                      alt="CLIP Icon"
                    />
                    <h3 style="margin: 0">2021 CLIP</h3>
                  </div>
                  <strong>連接視覺與語言</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    OpenAI
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    從 400M 圖文對中學習，開啟多模態 AI 時代。零樣本在 ImageNet
                    上達到 76.2% 準確率，匹敵
                    ResNet-50，證明了圖像與文字可以在同一個向量空間中對齊，證明對比學習在視覺-語言對齊中的威力，為
                    DALL-E、GPT-4 Vision 奠定基礎。
                  </p>
                  <a href="clip-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2021 Swin Transformer -->
              <div class="timeline-item right">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                    "
                  >
                    <img
                      src="images/swin_icon_3d.png"
                      style="
                        width: 56px;
                        height: 56px;
                        object-fit: cover;
                        border-radius: 12px;
                        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                      "
                      alt="Swin Transformer Icon"
                    />
                    <h3 style="margin: 0">2021 Swin Transformer</h3>
                  </div>
                  <strong>視覺模型的「通用骨幹」</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    Microsoft Research Asia
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    用「移位視窗」打破 ViT 的解析度魔咒。結合 CNN 的層次感與
                    Transformer 的全局觀，獨創 Shifted Windows
                    機制，同時實現線性複雜度與像素級精確度，視覺 Transformer
                    的里程碑，解決 ViT
                    的局限性，實現通用視覺骨幹網路，成為現代視覺模型的標準架構。
                  </p>
                  <a
                    href="swin-transformer-tutorial/index.html"
                    class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2022 InstructGPT -->
              <div class="timeline-item left">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                      justify-content: flex-end;
                    "
                  >
                    <h3 style="margin: 0">2022 InstructGPT</h3>
                    <img
                      src="images/user_generate_image_20260102041223_7eac_256.png"
                      style="width: 56px; height: 56px; object-fit: contain"
                      alt="InstructGPT Icon"
                    />
                  </div>
                  <strong>對齊人類意圖</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    OpenAI
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    RLHF 讓 AI 終於能聽懂並遵循指令。建立 RLHF 標準流程，讓 AI
                    真正對齊人類意圖，直接催生 ChatGPT 和現代對話
                    AI，成為所有對話模型的訓練範本。
                  </p>
                  <a href="instructgpt-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2023 LLaVA -->
              <div class="timeline-item right">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                    "
                  >
                    <img
                      src="llava-tutorial/images/generated/hero_llava.png"
                      style="
                        width: 56px;
                        height: 56px;
                        object-fit: cover;
                        border-radius: 8px;
                      "
                      alt="LLaVA Icon"
                    />
                    <h3 style="margin: 0">2023 LLaVA</h3>
                  </div>
                  <strong>視覺指令微調</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    University of Wisconsin-Madison, Microsoft Research
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    首個將指令微調延伸到多模態的模型。用 GPT-4 生成 158K
                    視覺指令資料，在 Science QA 上達到 92.53% SOTA。開源多模態
                    AI
                    的里程碑，證明小團隊也能做出高品質視覺語言模型，成為開源視覺
                    AI 的標準架構。
                  </p>
                  <a href="llava-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2023 GPT-4 -->
              <div class="timeline-item right">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                    "
                  >
                    <img
                      src="gpt4-tutorial/images/chapter01_hero.png"
                      style="
                        width: 56px;
                        height: 56px;
                        object-fit: cover;
                        border-radius: 8px;
                      "
                      alt="GPT-4 Icon"
                    />
                    <h3 style="margin: 0">2023 GPT-4</h3>
                  </div>
                  <strong>多模態與人類級性能</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    OpenAI
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    不僅能讀文字，還能看圖片，考試成績超越人類。在律師考試中取得前
                    10%
                    的成績，展現了真正的通用推理能力，引入「可預測擴展」法則，多模態通用
                    AI 的里程碑，定義了現代 AI 的標準和可能性。
                  </p>
                  <a href="gpt4-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2023 Gemini -->
              <div class="timeline-item left">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                      justify-content: flex-end;
                    "
                  >
                    <h3 style="margin: 0">2023 Gemini</h3>
                    <img
                      src="gemini-tutorial/images/original/tech_fig_architecture.png"
                      style="
                        width: 56px;
                        height: 56px;
                        object-fit: cover;
                        border-radius: 8px;
                      "
                      alt="Gemini Icon"
                    />
                  </div>
                  <strong>MMLU 90%+ 的歷史性突破</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    Google DeepMind
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    首個在 MMLU 達到人類專家水準的多模態模型。Gemini Ultra 在 32
                    個基準中的 30 個取得 SOTA，MMLU 得分
                    90.0%，超越人類專家水準（89.8%）。原生多模態架構設計，統一處理文字、圖像、音訊和影片，為負責任
                    AI 部署樹立新標準。
                  </p>
                  <a href="gemini-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>

              <!-- 2025 Nested Learning -->
              <div class="timeline-item right">
                <div class="timeline-dot"></div>
                <div class="timeline-card">
                  <div
                    class="timeline-header"
                    style="
                      display: flex;
                      align-items: center;
                      gap: 15px;
                      margin-bottom: 10px;
                    "
                  >
                    <img
                      src="nested-learning-tutorial/images/generated/hero-01-intro.png"
                      style="
                        width: 56px;
                        height: 56px;
                        object-fit: cover;
                        border-radius: 8px;
                      "
                      alt="Nested Learning Icon"
                    />
                    <h3 style="margin: 0">2025 Nested Learning</h3>
                  </div>
                  <strong>打破「深度」學習的迷思</strong>
                  <p
                    style="
                      font-size: 0.9rem;
                      color: var(--text-secondary);
                      margin: 8px 0;
                    "
                  >
                    Google Research
                  </p>
                  <p
                    style="
                      font-size: 0.85rem;
                      color: var(--text-secondary);
                      margin-top: 8px;
                    "
                  >
                    重新定義學習本質：將模型視為巢狀優化問題。提出聯想記憶視角，統一理解優化器與架構，引入 Continuum Memory System 和 Hope 模型，實現持續學習與長上下文理解。從神經科學啟發，揭示「層級」比「深度」更重要，為下一代 AI 架構提供路線圖。
                  </p>
                  <a href="nested-learning-tutorial/index.html" class="quick-link"
                    >深度解析 →</a
                  >
                </div>
              </div>
            </div>
          </div>
        </div>

        <!-- Tab 3: Articles -->
        <div id="articles" class="tab-content">
          <div style="margin-bottom: 30px; text-align: center">
            <h3
              style="
                font-size: 1.5rem;
                color: var(--text-main);
                margin-bottom: 10px;
              "
            >
              📝 知識筆記
            </h3>
            <p style="color: var(--text-secondary)">深度技術解析與實作筆記</p>
          </div>

          <div class="magazine-grid">
            <!-- Featured Article: Embedding Vector Guide -->
            <a
              href="articles/embedding-matrix-visual-guide.html"
              class="magazine-card featured-article"
            >
              <div class="card-image-wrapper">
                <img
                  src="images/vector_decomposition.png"
                  alt="Embedding Vector Guide Cover"
                />
              </div>
              <div class="card-content">
                <span
                  class="card-tag"
                  style="
                    background: linear-gradient(
                      135deg,
                      #ffd700 0%,
                      #ffed4e 100%
                    );
                    color: #1a1a1a;
                  "
                  >🔥 最新力作</span
                >
                <h3>向量的真相：字母'A'的768個數字</h3>
                <p>
                  從 GPT-2 的 Token Embed 出發，完整解析文字如何變成 768
                  個真實的數字。 揭開 Embedding、Position
                  Encoding、向量相加的秘密， 用真實數據帶你看懂高維向量空間。
                </p>
                <span class="read-more">深度閱讀 →</span>
              </div>
            </a>

            <!-- Article 1: GPT-3 Inference -->
            <a
              href="articles/gpt3-inference-detailed.html"
              class="magazine-card"
            >
              <div class="card-image-wrapper">
                <img
                  src="images/user_generate_image_20260102051640_9a1a.png"
                  alt="GPT-3 Inference Cover"
                />
              </div>
              <div class="card-content">
                <span
                  class="card-tag"
                  style="
                    background: var(--primary-light);
                    color: var(--primary-color);
                  "
                  >推理流程</span
                >
                <h3>GPT-3 推理流程詳解</h3>
                <p>
                  完整拆解 ChatGPT 回答問題時，背後發生的每一步：從 Tokenization
                  到 96 層 Transformer 的自回歸循環。
                </p>
                <span class="read-more">閱讀全文</span>
              </div>
            </a>

            <!-- Article 2: Training vs Inference -->
            <a
              href="articles/gpt3-training-vs-inference.html"
              class="magazine-card"
            >
              <div class="card-image-wrapper">
                <img
                  src="images/user_generate_image_20260102051654_7ade.png"
                  alt="Training vs Inference Cover"
                />
              </div>
              <div class="card-content">
                <span
                  class="card-tag"
                  style="
                    background: var(--secondary-light);
                    color: var(--secondary-color);
                  "
                  >機制解析</span
                >
                <h3>訓練 vs 推理：Tokens 輸入機制</h3>
                <p>
                  深入解析為什麼訓練時可以並行處理 (Teacher
                  Forcing)，而推理時只能逐字生成。
                </p>
                <span class="read-more">閱讀全文</span>
              </div>
            </a>

            <!-- Article 3: 96 Layers -->
            <a
              href="articles/gpt3-96-layers-architecture.html"
              class="magazine-card"
            >
              <div class="card-image-wrapper">
                <img
                  src="images/user_generate_image_20260102051709_5bd1.png"
                  alt="96 Layers Cover"
                />
              </div>
              <div class="card-content">
                <span
                  class="card-tag"
                  style="
                    background: var(--purple-light);
                    color: var(--purple-color);
                  "
                  >架構拆解</span
                >
                <h3>GPT-3 的 96 層架構解析</h3>
                <p>
                  96 層 Decoder Block
                  到底在做什麼？每層如何分工？探索深度學習模型的「摩天大樓」結構。
                </p>
                <span class="read-more">閱讀全文</span>
              </div>
            </a>

            <!-- Article 4: Dimensions -->
            <a
              href="articles/model-dimension-evolution.html"
              class="magazine-card"
            >
              <div class="card-image-wrapper">
                <img
                  src="images/user_generate_image_20260102051723_77e2.png"
                  alt="Dimension Evolution Cover"
                />
              </div>
              <div class="card-content">
                <span
                  class="card-tag"
                  style="
                    background: var(--accent-light);
                    color: var(--accent-color);
                  "
                  >演進史</span
                >
                <h3>模型維度演進史</h3>
                <p>
                  從 Transformer (512維) 到 GPT-3
                  (12,288維)，維度的增長代表了什麼？視覺化向量演變。
                </p>
                <span class="read-more">閱讀全文</span>
              </div>
            </a>

            <!-- Article 5: Tokenizer -->
            <a
              href="articles/tokenizer-embedding-explained.html"
              class="magazine-card"
            >
              <div class="card-image-wrapper">
                <img
                  src="images/user_generate_image_20260102051739_0da6.png"
                  alt="Tokenizer Cover"
                />
              </div>
              <div class="card-content">
                <span
                  class="card-tag"
                  style="
                    background: var(--secondary-light);
                    color: var(--secondary-color);
                  "
                  >基礎原理</span
                >
                <h3>從文字到向量：Tokenizer & Embedding</h3>
                <p>
                  AI 怎麼讀懂文字？拆解 Tokenization 與 Embedding
                  的兩階段轉換，理解向量由來。
                </p>
                <span class="read-more">閱讀全文</span>
              </div>
            </a>

            <!-- Article 6: MME-RealWorld -->
            <a href="articles/mme-realworld.html" class="magazine-card">
              <div class="card-image-wrapper">
                <img
                  src="articles/mme-realworld/images/generated/hero.png"
                  alt="MME-RealWorld Cover"
                  style="object-fit: cover"
                />
              </div>
              <div class="card-content">
                <span
                  class="card-tag"
                  style="background: #fee2e2; color: #ef4444"
                  >多模態評測</span
                >
                <h3>真實世界的考驗：MME-RealWorld</h3>
                <p>
                  2024年8月，使用13,366張高解析度真實圖片測試29個頂尖模型。
                  結果震撼：沒有任何模型超過60%準確率。
                </p>
                <span class="read-more">閱讀全文</span>
              </div>
            </a>

            <!-- Article 7: MMMU-Pro -->
            <a href="articles/mmmu-pro.html" class="magazine-card">
              <div class="card-image-wrapper">
                <img
                  src="articles/mmmu-pro/images/generated/hero.png"
                  alt="MMMU-Pro Cover"
                  style="object-fit: cover"
                />
              </div>
              <div class="card-content">
                <span
                  class="card-tag"
                  style="background: #fff7ed; color: #f97316"
                  >多模態評測</span
                >
                <h3>反作弊的評測：MMMU-Pro</h3>
                <p>
                  2024年9月，堵住「作弊途徑」後，所有模型準確率下降16.8%-26.9%。
                  揭示原版評測的「虛假高分」。
                </p>
                <span class="read-more">閱讀全文</span>
              </div>
            </a>

            <!-- Article 8: RBench-V -->
            <a href="articles/rbench-v.html" class="magazine-card">
              <div class="card-image-wrapper">
                <img
                  src="articles/rbench-v/images/generated/hero.png"
                  alt="RBench-V Cover"
                  style="object-fit: cover"
                />
              </div>
              <div class="card-content">
                <span
                  class="card-tag"
                  style="background: #f3e8ff; color: #8b5cf6"
                  >多模態評測</span
                >
                <h3>視覺推理的盲點：RBench-V</h3>
                <p>
                  2025年5月，要求模型「畫圖思考」的評測。
                  最佳模型僅25.8%，人類專家82.3%，揭示視覺推理的根本性缺陷。
                </p>
                <span class="read-more">閱讀全文</span>
              </div>
            </a>
          </div>
        </div>

        <!-- Tab 4: About & Updates -->
        <div id="about" class="tab-content">
          <div class="bento-grid">
            <!-- Mission -->
            <div class="bento-card featured">
              <div class="card-icon">🎯</div>
              <h3>關於 AI Paper Archaeology</h3>
              <p>
                AI 技術發展神速，但每個突破都源於經典論文。
                本站致力於「考古」這些改變世界的論文，用白話文解析技術細節，
                讓更多人理解 AI 的演進脈絡。
              </p>
              <div class="card-meta" style="flex-wrap: wrap; gap: 15px">
                <span>✅ 原文對照翻譯</span>
                <span>✅ 生活化類比</span>
                <span>✅ 技術深度解析</span>
              </div>
            </div>

            <!-- Author -->
            <div class="bento-card">
              <div class="card-icon">👨‍💻</div>
              <h3>關於作者</h3>
              <p>
                <strong>謝承緯 (Chen Wei Hsieh)</strong><br />熱愛 AI
                技術的研究者與工程師。
              </p>
              <div
                style="
                  margin-top: 20px;
                  display: flex;
                  flex-direction: column;
                  gap: 10px;
                "
              >
                <a
                  href="https://github.com/hsiehchenwei/ai-paper-archaeology"
                  target="_blank"
                  class="card-link"
                  >GitHub</a
                >
                <a href="mailto:pushy.chordal_0s@icloud.com" class="card-link"
                  >Email</a
                >
              </div>
            </div>

            <!-- Updates -->
            <div class="bento-card" style="display: none">
              <!-- Hidden or removed -->
            </div>
          </div>
        </div>
      </div>

      <!-- Footer -->
      <footer class="site-footer">
        <div class="footer-grid" style="justify-content: center; gap: 40px">
          <div class="footer-section">
            <h4>⚖️ 版權資訊</h4>
            <a href="LICENSE">MIT License</a>
            <a href="ATTRIBUTION.md">圖片來源與版權聲明</a>
          </div>
          <div class="footer-section">
            <h4>🌟 支持本專案</h4>
            <a
              href="https://github.com/hsiehchenwei/ai-paper-archaeology"
              target="_blank"
              >Star on GitHub</a
            >
          </div>
        </div>

        <div
          style="
            text-align: center;
            margin-top: 30px;
            padding-top: 20px;
            border-top: 1px solid rgba(0, 0, 0, 0.1);
            color: var(--text-secondary);
            font-size: 0.9rem;
            line-height: 1.6;
          "
        >
          <p>⚠️ 本站內容為 AI 根據論文原文生成的圖文解析。</p>
          <p>圖片來源包含：原始論文截圖 與 AI 生成示意圖。</p>
        </div>

        <div class="footer-bottom">
          <p>© 2026 AI Paper Archaeology | Made with ❤️ for AI Learners</p>
        </div>
      </footer>
    </div>

    <!-- Tab Switching Logic -->
    <script>
      function switchTab(tabId) {
        // 1. Hide all tab contents
        document.querySelectorAll(".tab-content").forEach((content) => {
          content.classList.remove("active");
        });

        // 2. Deactivate all tab buttons
        document.querySelectorAll(".tab-btn").forEach((btn) => {
          btn.classList.remove("active");
        });

        // 3. Show selected content
        document.getElementById(tabId).classList.add("active");

        // 4. Activate selected button
        // Find the button that calls this function with the matching tabId
        const buttons = document.querySelectorAll(".tab-btn");
        buttons.forEach((btn) => {
          if (btn.getAttribute("onclick").includes(tabId)) {
            btn.classList.add("active");
          }
        });
      }
    </script>
  </body>
</html>
