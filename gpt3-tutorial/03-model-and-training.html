<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第 3 頁:模型架構與訓練 - GPT-3 論文深度解析</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <h1>⚙️ 第 3 頁:模型架構與訓練 - GPT-3 如何打造</h1>

        <h2>🏗️ 模型架構:站在巨人肩膀上</h2>

        <div class="text-pair">
            <div class="original-text">
                We use the same model and architecture as GPT-2, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer.
            </div>
            <div class="translation">
                我們使用與 GPT-2 相同的模型和架構,包括其中描述的修改初始化、預標準化和可逆分詞,唯一的例外是我們在 Transformer 的層中使用交替的密集和局部帶狀稀疏注意力模式。
            </div>
        </div>

        <div class="key-concept">
            <h4>🎯 核心技術:Transformer</h4>
            
            <div style="margin: 30px 0; text-align: center;">
                <img src="../images/user_generate_image_20260101095111_3c87.png" 
                     alt="BERT vs GPT-3 架構對比" 
                     style="max-width: 100%; border-radius: var(--radius-lg); box-shadow: var(--shadow-lg);">
                <p class="caption">
                    <strong>🔄 BERT vs GPT-3 架構核心差異</strong><br>
                    左:BERT (雙向 Encoder,理解型) | 右:GPT-3 (單向 Decoder,生成型)<br>
                    BERT 能「看前看後」理解上下文 | GPT-3 只能「向前看」逐字生成<br>
                    不同任務,不同設計！
                </p>
            </div>
            
            <p><strong>GPT-3 的基礎:</strong></p>
            <ul>
                <li><strong>架構</strong>:Transformer Decoder (來自 2017 年的「Attention Is All You Need」論文)</li>
                <li><strong>改進</strong>:基於 GPT-2 的優化版本</li>
                <li><strong>創新</strong>:稀疏注意力機制(Sparse Attention)</li>
            </ul>

            <p><em>💡 如果你想深入了解 Transformer,可以參考同目錄下的 transformer-tutorial!</em></p>
        </div>

        <div class="explanation">
            <h4>🔍 什麼是 Autoregressive(自回歸)?</h4>
            <p><strong>定義:</strong>模型一次生成一個 Token,每個新 Token 都基於前面生成的 Tokens。</p>

            <h5>💡 生活類比:接龍遊戲</h5>
            <p><strong>場景:成語接龍</strong></p>
            <ol>
                <li>給定開頭:「一心」</li>
                <li>模型生成:「一心」→「一意」(看前 2 個字)</li>
                <li>繼續生成:「一心一意」→「孤」(看前 4 個字)</li>
                <li>再繼續:「一心一意孤」→「行」(看前 5 個字)</li>
            </ol>
            <p>每次都是「看前面的內容」→「預測下一個」!</p>
        </div>

        <div class="analogy">
            <h4>🤖 AI 體驗連結:為什麼 ChatGPT 一個字一個字「打」出來?</h4>
            <p><strong>你的觀察:</strong></p>
            <p>ChatGPT 的回答不是「一次全部出現」,而是像打字一樣逐字出現!</p>

            <p><strong>原因:</strong></p>
            <p>這就是 <strong>Autoregressive</strong> 的特性!</p>

            <pre><code>// ChatGPT 內部運作(簡化)
prompt = "請說明什麼是AI"

token1 = model.predict(prompt)              // "AI"
token2 = model.predict(prompt + token1)     // "是"
token3 = model.predict(prompt + token1 + token2) // "人工"
...

// 每個 token 都基於「前面所有的 tokens」!
</code></pre>

            <p><strong>⚠️ Reality Check:</strong></p>
            <ul>
                <li><strong>類比:</strong>模型「一個字一個字思考」</li>
                <li><strong>實際:</strong>模型每次都是獨立的前向推理,沒有「連續思考」的過程</li>
                <li><strong>差異:</strong>模型看不到「未來」,只能基於「過去」預測</li>
            </ul>
        </div>

        <div style="margin: 40px 0; text-align: center;">
            <img src="../images/user_generate_image_20260101094959_c797.png" 
                 alt="Autoregressive Token 生成過程" 
                 style="max-width: 100%; border-radius: var(--radius-lg); box-shadow: var(--shadow-lg);">
            <p class="caption">
                <strong>⚙️ Autoregressive 逐 Token 生成視覺化</strong><br>
                四格展示句子 "The cat sat on..." 的生成過程<br>
                每一格:前面的 tokens → 模型 → 預測下一個 token<br>
                <strong>關鍵:</strong>這就是為什麼 ChatGPT 「像打字機一樣」逐字輸出!
            </p>
        </div>

        <h2>📊 模型規模:8 個不同大小</h2>

        <div class="text-pair">
            <div class="original-text">
                To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.
            </div>
            <div class="translation">
                為了研究機器學習效能對模型大小的依賴性,我們訓練了 8 種不同大小的模型,參數量橫跨三個數量級,從 1.25 億到 1750 億參數,最後一個就是我們稱為 GPT-3 的模型。
            </div>
        </div>

        <div class="key-concept">
            <h4>📏 GPT-3 模型家族</h4>
            <table>
                <thead>
                    <tr>
                        <th>模型名稱</th>
                        <th>參數量</th>
                        <th>層數 \(n_{\text{layers}}\)</th>
                        <th>模型維度 \(d_{\text{model}}\)</th>
                        <th>注意力頭數</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>GPT-3 Small</td>
                        <td>125M</td>
                        <td>12</td>
                        <td>768</td>
                        <td>12</td>
                    </tr>
                    <tr>
                        <td>GPT-3 Medium</td>
                        <td>350M</td>
                        <td>24</td>
                        <td>1024</td>
                        <td>16</td>
                    </tr>
                    <tr>
                        <td>GPT-3 Large</td>
                        <td>760M</td>
                        <td>24</td>
                        <td>1536</td>
                        <td>16</td>
                    </tr>
                    <tr>
                        <td>GPT-3 XL</td>
                        <td>1.3B</td>
                        <td>24</td>
                        <td>2048</td>
                        <td>24</td>
                    </tr>
                    <tr>
                        <td>GPT-3 2.7B</td>
                        <td>2.7B</td>
                        <td>32</td>
                        <td>2560</td>
                        <td>32</td>
                    </tr>
                    <tr>
                        <td>GPT-3 6.7B</td>
                        <td>6.7B</td>
                        <td>32</td>
                        <td>4096</td>
                        <td>32</td>
                    </tr>
                    <tr>
                        <td>GPT-3 13B</td>
                        <td>13B</td>
                        <td>40</td>
                        <td>5140</td>
                        <td>40</td>
                    </tr>
                    <tr style="background: var(--primary-light);">
                        <td><strong>GPT-3 175B</strong></td>
                        <td><strong>175B ⭐</strong></td>
                        <td><strong>96</strong></td>
                        <td><strong>12288</strong></td>
                        <td><strong>96</strong></td>
                    </tr>
                </tbody>
            </table>

            <p><strong>關鍵觀察:</strong></p>
            <ul>
                <li>參數量從 125M 到 175B = <strong>1400 倍</strong>!</li>
                <li>層數從 12 到 96 = <strong>8 倍</strong></li>
                <li>模型維度從 768 到 12288 = <strong>16 倍</strong></li>
            </ul>
        </div>

        <div class="figure">
            <img src="images/training_curves.png" alt="不同模型規模的訓練曲線" style="max-width: 100%; height: auto;">
            <p class="caption">
                <strong>圖 3.1:不同模型規模的訓練曲線</strong><br>
                展示了從小模型到 GPT-3 175B 的訓練損失(training loss)變化。
                可以看到:<br>
                • 所有模型的損失都平滑下降<br>
                • <strong>更大的模型達到更低的損失</strong><br>
                • 沒有出現過擬合或發散問題<br>
                這驗證了 Scaling Law 的預測!
            </p>
        </div>

        <div class="explanation">
            <h4>🔢 參數量如何計算?</h4>
            <p><strong>簡化公式:</strong></p>
            <p>\[ n_{\text{params}} \approx 12 \times n_{\text{layers}} \times d_{\text{model}}^2 \]</p>

            <p><strong>以 GPT-3 175B 為例:</strong></p>
            <pre><code>n_layers = 96
d_model = 12288

n_params ≈ 12 × 96 × (12288)²
         ≈ 12 × 96 × 150,994,944
         ≈ 173,949,444,096
         ≈ 174B 參數

// 實際是 175B,公式只是近似值
</code></pre>
        </div>

        <div class="analogy">
            <h4>💡 生活類比:大腦的「厚度」與「寬度」</h4>
            <p>如果把 Transformer 想像成大腦:</p>
            <ul>
                <li><strong>\(n_{\text{layers}}\) = 96</strong>: 「大腦皮層」有 96 層(深度)</li>
                <li><strong>\(d_{\text{model}}\) = 12288</strong>: 每層有 12,288 個「神經元」(寬度)</li>
                <li><strong>注意力頭數 = 96</strong>: 每層有 96 個「專家」同時觀察(多角度)</li>
            </ul>
            <p>更深 + 更寬 = 更強大的「思考能力」!</p>
        </div>

        <h2>📚 訓練資料:吃遍整個網際網路</h2>

        <div class="text-pair">
            <div class="original-text">
                Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset constituting nearly a trillion words. This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice.
            </div>
            <div class="translation">
                語言模型的資料集快速擴展,最終達到包含近一兆個單詞的 Common Crawl 資料集。這個規模的資料集足以訓練我們最大的模型,而不會對同一個序列更新兩次。
            </div>
        </div>

        <div class="key-concept">
            <h4>🌐 GPT-3 的訓練資料來源</h4>
            <table>
                <thead>
                    <tr>
                        <th>資料來源</th>
                        <th>Tokens 數量</th>
                        <th>比例</th>
                        <th>訓練中採樣次數</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Common Crawl (過濾後)</strong></td>
                        <td>410B</td>
                        <td>60%</td>
                        <td>0.44x</td>
                    </tr>
                    <tr>
                        <td><strong>WebText2</strong></td>
                        <td>19B</td>
                        <td>22%</td>
                        <td>2.9x</td>
                    </tr>
                    <tr>
                        <td><strong>Books1</strong></td>
                        <td>12B</td>
                        <td>8%</td>
                        <td>1.9x</td>
                    </tr>
                    <tr>
                        <td><strong>Books2</strong></td>
                        <td>55B</td>
                        <td>8%</td>
                        <td>0.43x</td>
                    </tr>
                    <tr>
                        <td><strong>Wikipedia</strong></td>
                        <td>3B</td>
                        <td>3%</td>
                        <td>3.4x</td>
                    </tr>
                </tbody>
            </table>

            <p><strong>總計:約 500B (5000 億) Tokens</strong></p>
        </div>

        <div class="explanation">
            <h4>🔍 什麼是 Common Crawl?</h4>
            <p><strong>定義:</strong>一個非營利組織定期爬取整個網際網路的資料集。</p>

            <p><strong>規模:</strong></p>
            <ul>
                <li><strong>原始資料</strong>:45TB 壓縮文本</li>
                <li><strong>過濾後</strong>:570GB 高品質文本</li>
                <li><strong>Token 數</strong>:約 4000 億個 Tokens</li>
            </ul>

            <p><strong>⚠️ 問題:品質參差不齊</strong></p>
            <ul>
                <li>包含低品質網頁、廣告、垃圾內容</li>
                <li>需要<strong>過濾</strong>才能使用!</li>
            </ul>
        </div>

        <div class="analogy">
            <h4>💡 生活類比:圖書館藏書策略</h4>
            <p><strong>情境:打造一個超級圖書館</strong></p>

            <h5>方法 1:全部照收 (未過濾的 Common Crawl)</h5>
            <ul>
                <li>收集全世界所有書籍、雜誌、傳單、廣告</li>
                <li><strong>問題</strong>:品質不一,讀到爛書會「學壞」!</li>
            </ul>

            <h5>方法 2:精選藏書 (OpenAI 的策略)</h5>
            <ul>
                <li>先篩選:只留「類似高品質文章」的內容</li>
                <li>去重複:同樣的書不要重複收</li>
                <li>加精品:額外加入 Wikipedia、經典書籍</li>
            </ul>
        </div>

        <div class="explanation">
            <h4>🎯 採樣策略:不平均採樣</h4>
            <p><strong>關鍵策略:高品質資料多看幾次!</strong></p>

            <p><strong>範例:</strong></p>
            <ul>
                <li><strong>Common Crawl</strong>:只看 0.44 次(不看完)</li>
                <li><strong>Wikipedia</strong>:看 3.4 次(重複學習)</li>
                <li><strong>WebText2</strong>:看 2.9 次</li>
            </ul>

            <p><strong>為什麼?</strong></p>
            <p>寧可在「高品質資料」上稍微過擬合,也不要被「低品質資料」污染!</p>
        </div>

        <div class="problem">
            <h4>⚠️ 資料污染問題 (Data Contamination)</h4>
            <p><strong>問題:</strong></p>
            <p>如果訓練資料中「不小心」包含了測試集的內容怎麼辦?</p>

            <p><strong>舉例:</strong></p>
            <ul>
                <li>Common Crawl 可能包含某個測試題的答案(因為答案在網路上)</li>
                <li>模型可能「記住」答案,而不是真的「學會」解題!</li>
            </ul>

            <p><strong>OpenAI 的處理:</strong></p>
            <ul>
                <li>嘗試移除與測試集重疊的資料</li>
                <li>但因為有 bug,沒完全移除</li>
                <li>論文中分析了這個問題的影響(第 4 節)</li>
            </ul>
        </div>

        <h2>🔧 訓練過程:超級電腦集群</h2>

        <div class="text-pair">
            <div class="original-text">
                To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU's on part of a high-bandwidth cluster provided by Microsoft.
            </div>
            <div class="translation">
                為了在不耗盡記憶體的情況下訓練更大的模型,我們在每次矩陣乘法內和網路層之間都使用模型並行。所有模型都在 Microsoft 提供的高頻寬叢集的一部分 V100 GPU 上訓練。
            </div>
        </div>

        <div class="key-concept">
            <h4>💻 訓練基礎設施</h4>
            <ul>
                <li><strong>硬體</strong>:NVIDIA V100 GPU 叢集(由 Microsoft 提供)</li>
                <li><strong>技術</strong>:模型並行(Model Parallelism)</li>
                <li><strong>Context Window</strong>:所有模型都是 2048 tokens</li>
            </ul>

            <h5>📊 訓練成本估計</h5>
            <ul>
                <li><strong>計算量</strong>:約 3640 petaflop/s-days</li>
                <li><strong>金錢成本</strong>:估計 <strong>460 萬美元</strong>(雲端運算價格)</li>
                <li><strong>訓練時間</strong>:數週到數月</li>
            </ul>
        </div>

        <div class="explanation">
            <h4>🔍 什麼是模型並行 (Model Parallelism)?</h4>
            <p><strong>問題:</strong>175B 參數的模型太大,一張 GPU 放不下!</p>

            <h5>解決方案:切割模型</h5>
            <pre><code>// 簡化概念
GPU 1: 負責 Layer 1-24
GPU 2: 負責 Layer 25-48  
GPU 3: 負責 Layer 49-72
GPU 4: 負責 Layer 73-96

// 資料流動
Input → GPU 1 → GPU 2 → GPU 3 → GPU 4 → Output
</code></pre>

            <p><strong>挑戰:</strong></p>
            <ul>
                <li>GPU 之間需要高速通訊</li>
                <li>需要「高頻寬叢集」來降低延遲</li>
            </ul>
        </div>

        <div class="analogy">
            <h4>💡 生活類比:工廠流水線</h4>
            <p><strong>場景:製造一台超級複雜的機器</strong></p>
            <p>單一工人無法完成 → 使用流水線:</p>
            <ul>
                <li><strong>工人 A</strong>:組裝零件 1-25</li>
                <li><strong>工人 B</strong>:組裝零件 26-50</li>
                <li><strong>工人 C</strong>:組裝零件 51-75</li>
                <li><strong>工人 D</strong>:組裝零件 76-100</li>
            </ul>
            <p>每個工人專注自己的部分,但產品需要「流動」經過所有工人!</p>
        </div>

        <div class="analogy">
            <h4>🤖 AI 體驗連結:為什麼你不能自己訓練 GPT-3?</h4>
            <p><strong>現實檢查:</strong></p>

            <h5>訓練 GPT-3 需要:</h5>
            <ul>
                <li><strong>硬體</strong>:數百張高階 GPU(V100 或 A100)</li>
                <li><strong>時間</strong>:數週到數月</li>
                <li><strong>金錢</strong>:約 460 萬美元</li>
                <li><strong>電力</strong>:相當於 126 個家庭一年的用電</li>
                <li><strong>專業知識</strong>:分散式系統、深度學習優化</li>
            </ul>

            <p><strong>這就是為什麼:</strong></p>
            <ul>
                <li>只有大公司(OpenAI, Google, Meta)能訓練超大模型</li>
                <li>個人開發者只能用「API」或「開源小模型」</li>
                <li>GPT-3 開啟了「模型即服務」(MaaS) 的時代</li>
            </ul>
        </div>

        <div class="key-concept">
            <h4>⚡ 訓練配置細節</h4>
            <h5>批次大小 (Batch Size)</h5>
            <p>模型越大 → 批次越大:</p>
            <ul>
                <li><strong>GPT-3 Small (125M)</strong>: Batch Size ≈ 0.5M tokens</li>
                <li><strong>GPT-3 175B</strong>: Batch Size ≈ 3.2M tokens</li>
            </ul>

            <h5>學習率 (Learning Rate)</h5>
            <p>模型越大 → 學習率越小(更謹慎更新)</p>
        </div>

        <div class="analogy">
            <h4>🔧 工程類比:Batch Size 的權衡</h4>
            <pre><code>// 小 Batch Size (例如 32)
優點: 更新頻繁,學習快
缺點: 不穩定,雜訊多,GPU 利用率低

// 大 Batch Size (例如 3.2M tokens)
優點: 更新穩定,GPU 利用率高,訓練快
缺點: 需要更多記憶體,可能陷入局部最優

// GPT-3 的策略:
// 用「梯度雜訊規模」(Gradient Noise Scale)
// 動態決定最佳 Batch Size
</code></pre>
        </div>

        <h2>🎯 訓練目標:語言建模</h2>

        <div class="key-concept">
            <h4>📚 訓練任務:預測下一個 Token</h4>
            <p><strong>損失函數:</strong></p>
            <p>\[ \mathcal{L} = -\sum_{t=1}^{T} \log P(x_t | x_1, x_2, ..., x_{t-1}) \]</p>

            <p><strong>簡單說:</strong></p>
            <p>給定前面的文字,預測下一個字的機率要越高越好!</p>

            <h5>範例</h5>
            <pre><code>輸入: "The cat sat on the"
目標: 預測 "mat" 的機率要高!

模型學習:
P("mat" | "The cat sat on the") = 0.8  ✓ 好
P("dog" | "The cat sat on the") = 0.01 ✗ 差
</code></pre>
        </div>

        <div class="explanation">
            <h4>🤔 為什麼這麼簡單的任務就能產生智能?</h4>
            <p><strong>關鍵洞察:</strong></p>
            <p>要「完美預測下一個字」,模型必須:</p>
            <ul>
                <li>理解<strong>文法</strong>(什麼時候該用動詞、名詞)</li>
                <li>理解<strong>語意</strong>(貓會坐在墊子上,不是天空)</li>
                <li>理解<strong>常識</strong>(重物往下掉、火會燙)</li>
                <li>理解<strong>邏輯</strong>(如果 A>B 且 B>C,則 A>C)</li>
            </ul>
            <p><strong>所以:</strong>語言建模不只是「記憶」,而是學習「世界知識」!</p>
        </div>

        <div class="analogy">
            <h4>🤖 AI 體驗連結:ChatGPT 的「知識」從哪來?</h4>
            <p><strong>常見誤解:</strong></p>
            <p>❌ ChatGPT 有連網查資料</p>
            <p>❌ ChatGPT 有資料庫儲存事實</p>

            <p><strong>真相:</strong></p>
            <p>✅ ChatGPT 的「知識」全部<strong>壓縮在參數</strong>裡!</p>

            <p><strong>類比:</strong></p>
            <p>就像你讀了 10,000 本書後,書的內容變成了你的「記憶」和「理解」,不需要再翻書!</p>

            <p><strong>限制:</strong></p>
            <ul>
                <li>訓練後發生的事情 → 不知道</li>
                <li>罕見的知識 → 可能不準確</li>
                <li>需要精確數字 → 可能編造(幻覺)</li>
            </ul>
        </div>

        <div class="nav-bar">
            <a href="02-approach-overview.html" class="nav-btn">← 上一頁:方法概覽</a>
            <a href="index.html" class="nav-btn">📑 目錄</a>
            <a href="04-results-highlights.html" class="nav-btn primary">下一頁 → 結果展示</a>
        </div>
    </div>
</body>
</html>

