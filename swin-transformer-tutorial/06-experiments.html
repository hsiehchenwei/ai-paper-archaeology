<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å¯¦é©—çµæœ - Swin Transformer æ·±åº¦è§£æ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <!-- Hero Section -->
        <div class="hero-section" style="background-image: url('images/generated/hero_06_experiments.png');">
            <div class="hero-overlay"></div>
            <div class="hero-content">
                <h1>å…¨é¢è¶…è¶Šï¼šä¸‰å€‹ä»»å‹™çš„ SOTA</h1>
                <p class="hero-subtitle">ImageNetã€COCOã€ADE20K çš„å“è¶Šè¡¨ç¾</p>
                <div class="hero-meta">Swin Transformer æ·±åº¦è§£æ Â· ç¬¬ 6 ç« </div>
            </div>
        </div>

        <!-- Breadcrumb -->
        <nav class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <a href="index.html">Swin Transformer æ·±åº¦è§£æ</a>
            <span>/</span>
            <span class="current">ç¬¬ 6 ç« ï¼šå¯¦é©—çµæœ</span>
        </nav>

        <div class="story-container">
            <!-- Drop Cap é–‹å ´ -->
            <p class="drop-cap">
                Swin Transformer çš„å¯¦é©—çµæœè­‰æ˜äº†å…¶è¨­è¨ˆçš„å„ªè¶Šæ€§ã€‚åœ¨ ImageNet-1K åœ–åƒåˆ†é¡ä¸Šé”åˆ° 87.3% çš„ top-1 æº–ç¢ºç‡ï¼Œåœ¨ COCO ç›®æ¨™æª¢æ¸¬ä¸Šå–å¾— 58.7 box AP å’Œ 51.1 mask APï¼Œåœ¨ ADE20K èªç¾©åˆ†å‰²ä¸Šé”åˆ° 53.5 mIoUã€‚é€™äº›çµæœä¸åƒ…è¶…è¶Šäº† ViT å’Œ ResNetï¼Œæ›´åœ¨å¤šå€‹ä»»å‹™ä¸Šé”åˆ°äº†æ–°çš„ SOTAï¼Œè­‰æ˜äº† Swin Transformer ä½œç‚ºé€šç”¨è¦–è¦ºéª¨å¹¹ç¶²è·¯çš„å¼·å¤§èƒ½åŠ›ã€‚
            </p>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>ImageNet-1K åœ–åƒåˆ†é¡</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Compared to the previous state-of-the-art Transformer-based architecture, i.e. DeiT, Swin Transformers noticeably surpass the counterpart DeiT architectures with similar complexities: +1.5% for Swin-T (81.3%) over DeiT-S (79.8%) using 224Â² input, and +1.5%/1.4% for Swin-B (83.3%/84.5%) over DeiT-B (81.8%/83.1%) using 224Â²/384Â² input, respectively.
                        
                        The larger Swin-L model achieves 87.3% top-1 accuracy, +0.9% better than that of the Swin-B model.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        èˆ‡ä¹‹å‰æœ€å…ˆé€²çš„åŸºæ–¼ Transformer çš„æ¶æ§‹ï¼ˆå³ DeiTï¼‰ç›¸æ¯”ï¼ŒSwin Transformer åœ¨ç›¸ä¼¼è¤‡é›œåº¦ä¸‹æ˜é¡¯è¶…è¶Šäº†å°æ‡‰çš„ DeiT æ¶æ§‹ï¼šä½¿ç”¨ 224Â² è¼¸å…¥æ™‚ï¼ŒSwin-T (81.3%) æ¯” DeiT-S (79.8%) é«˜ +1.5%ï¼Œä½¿ç”¨ 224Â²/384Â² è¼¸å…¥æ™‚ï¼ŒSwin-B (83.3%/84.5%) æ¯” DeiT-B (81.8%/83.1%) åˆ†åˆ¥é«˜ +1.5%/+1.4%ã€‚
                        
                        æ›´å¤§çš„ Swin-L æ¨¡å‹é”åˆ° 87.3% çš„ top-1 æº–ç¢ºç‡ï¼Œæ¯” Swin-B æ¨¡å‹é«˜ +0.9%ã€‚
                    </div>
                </div>
            </div>

            <div class="paradigm-grid">
                <div class="paradigm-card">
                    <h4>Swin-T</h4>
                    <p><strong>81.3%</strong> top-1 (224Â²)</p>
                    <p>29M åƒæ•¸ï¼Œ4.5G FLOPs</p>
                    <p>è¶…è¶Š DeiT-S +1.5%</p>
                </div>
                <div class="paradigm-card">
                    <h4>Swin-S</h4>
                    <p><strong>83.0%</strong> top-1 (224Â²)</p>
                    <p>50M åƒæ•¸ï¼Œ8.7G FLOPs</p>
                </div>
                <div class="paradigm-card">
                    <h4>Swin-B</h4>
                    <p><strong>83.5%</strong> top-1 (224Â²)</p>
                    <p><strong>84.5%</strong> top-1 (384Â²)</p>
                    <p>88M åƒæ•¸ï¼Œè¶…è¶Š DeiT-B</p>
                </div>
                <div class="paradigm-card">
                    <h4>Swin-L</h4>
                    <p><strong>87.3%</strong> top-1 (384Â²)</p>
                    <p>197M åƒæ•¸ï¼Œ103.9G FLOPs</p>
                    <p>ImageNet-22K é è¨“ç·´</p>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>COCO ç›®æ¨™æª¢æ¸¬</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Table 5(a) lists the results of Swin-T and ResNet-50 on the four object detection frameworks. Our Swin-T architecture brings consistent +3.4~4.2 box AP gains over ResNet-50, with slightly larger model size, FLOPs and latency.
                        
                        Our best model achieves 58.7 box AP and 51.1 mask AP on COCO test-dev, surpassing the previous best results by +2.7 box AP (Copy-paste without external data) and +2.6 mask AP (DetectoRS).
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        è¡¨ 5(a) åˆ—å‡ºäº† Swin-T å’Œ ResNet-50 åœ¨å››å€‹ç›®æ¨™æª¢æ¸¬æ¡†æ¶ä¸Šçš„çµæœã€‚æˆ‘å€‘çš„ Swin-T æ¶æ§‹åœ¨ ResNet-50 åŸºç¤ä¸Šå¸¶ä¾†ä¸€è‡´çš„ +3.4~4.2 box AP æå‡ï¼Œæ¨¡å‹å¤§å°ã€FLOPs å’Œå»¶é²ç•¥å¤§ã€‚
                        
                        æˆ‘å€‘çš„æœ€ä½³æ¨¡å‹åœ¨ COCO test-dev ä¸Šé”åˆ° 58.7 box AP å’Œ 51.1 mask APï¼Œè¶…éä¹‹å‰æœ€ä½³çµæœ +2.7 box APï¼ˆCopy-pasteï¼Œç„¡å¤–éƒ¨æ•¸æ“šï¼‰å’Œ +2.6 mask APï¼ˆDetectoRSï¼‰ã€‚
                    </div>
                </div>
            </div>

            <div class="key-concept">
                <h4>ğŸ“Š COCO å¯¦é©—çµæœæ‘˜è¦</h4>
                <ul>
                    <li><strong>Swin-T vs ResNet-50</strong>ï¼šåœ¨å››å€‹æ¡†æ¶ä¸Šéƒ½å¸¶ä¾† +3.4~4.2 box AP æå‡</li>
                    <li><strong>Swin-B</strong>ï¼š51.9 box APï¼Œ45.0 mask APï¼ˆCascade Mask R-CNNï¼‰</li>
                    <li><strong>Swin-L (HTC++)</strong>ï¼š58.7 box APï¼Œ51.1 mask APï¼ˆç³»çµ±ç´šå°æ¯”ï¼‰</li>
                    <li><strong>è¶…è¶Š SOTA</strong>ï¼š+2.7 box APï¼Œ+2.6 mask AP</li>
                </ul>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>ADE20K èªç¾©åˆ†å‰²</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        From the results in Table 6, it can be seen that Swin-S is +5.3 mIoU higher (49.3 vs. 44.0) than DeiT-S with similar computation cost. It is also +4.4 mIoU higher than ResNet-101, and +2.4 mIoU higher than ResNeSt-101. Our Swin-L model with ImageNet-22K pre-training achieves 53.5 mIoU on the val set, surpassing the previous best model by +3.2 mIoU (50.3 mIoU by SETR which has a larger model size).
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        å¾è¡¨ 6 çš„çµæœå¯ä»¥çœ‹å‡ºï¼ŒSwin-S åœ¨ç›¸ä¼¼è¨ˆç®—æˆæœ¬ä¸‹æ¯” DeiT-S é«˜ +5.3 mIoU (49.3 vs. 44.0)ã€‚å®ƒä¹Ÿæ¯” ResNet-101 é«˜ +4.4 mIoUï¼Œæ¯” ResNeSt-101 é«˜ +2.4 mIoUã€‚æˆ‘å€‘ä½¿ç”¨ ImageNet-22K é è¨“ç·´çš„ Swin-L æ¨¡å‹åœ¨ val é›†ä¸Šé”åˆ° 53.5 mIoUï¼Œè¶…éä¹‹å‰æœ€ä½³æ¨¡å‹ +3.2 mIoUï¼ˆSETR çš„ 50.3 mIoUï¼Œæ¨¡å‹å°ºå¯¸æ›´å¤§ï¼‰ã€‚
                    </div>
                </div>
            </div>

            <div class="paradigm-grid">
                <div class="paradigm-card">
                    <h4>Swin-T</h4>
                    <p><strong>46.1</strong> mIoU</p>
                    <p>60M åƒæ•¸ï¼Œ945G FLOPs</p>
                </div>
                <div class="paradigm-card">
                    <h4>Swin-S</h4>
                    <p><strong>49.3</strong> mIoU</p>
                    <p>81M åƒæ•¸ï¼Œ1038G FLOPs</p>
                    <p>è¶…è¶Š DeiT-S +5.3</p>
                </div>
                <div class="paradigm-card">
                    <h4>Swin-B</h4>
                    <p><strong>51.6</strong> mIoU</p>
                    <p>121M åƒæ•¸ï¼ˆImageNet-22K é è¨“ç·´ï¼‰</p>
                </div>
                <div class="paradigm-card">
                    <h4>Swin-L</h4>
                    <p><strong>53.5</strong> mIoU</p>
                    <p>234M åƒæ•¸ï¼ˆImageNet-22K é è¨“ç·´ï¼‰</p>
                    <p>è¶…è¶Š SETR +3.2</p>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>èˆ‡ ResNet å’Œ ViT çš„å°æ¯”</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        It outperforms the ViT / DeiT and ResNe(X)t models significantly with similar latency on the three tasks. The performance of DeiT-S using the Cascade Mask R-CNN framework is shown in Table 5(b). The results of Swin-T are +2.5 box AP and +2.3 mask AP higher than DeiT-S with similar model size (86M vs. 80M) and significantly higher inference speed (15.3 FPS vs. 10.4 FPS).
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        å®ƒåœ¨ä¸‰å€‹ä»»å‹™ä¸Šä»¥ç›¸ä¼¼çš„å»¶é²é¡¯è‘—è¶…è¶Šäº† ViT / DeiT å’Œ ResNe(X)t æ¨¡å‹ã€‚ä½¿ç”¨ Cascade Mask R-CNN æ¡†æ¶çš„ DeiT-S çš„æ€§èƒ½å¦‚è¡¨ 5(b) æ‰€ç¤ºã€‚Swin-T çš„çµæœæ¯” DeiT-S é«˜ +2.5 box AP å’Œ +2.3 mask APï¼Œæ¨¡å‹å¤§å°ç›¸ä¼¼ï¼ˆ86M vs. 80Mï¼‰ï¼Œæ¨ç†é€Ÿåº¦é¡¯è‘—æ›´é«˜ï¼ˆ15.3 FPS vs. 10.4 FPSï¼‰ã€‚
                    </div>
                </div>
            </div>

            <!-- é›™é‡é¡æ¯” -->
            <div class="analogy-section">
                <div class="analogy-card life">
                    <h4>ğŸ  ç”Ÿæ´»é¡æ¯”</h4>
                    <p>
                        å°±åƒç«¶è³½æˆç¸¾ä¸€æ¨£ï¼šSwin Transformer åœ¨ä¸‰å€‹ä¸åŒçš„ã€Œç«¶è³½é …ç›®ã€ï¼ˆåœ–åƒåˆ†é¡ã€ç›®æ¨™æª¢æ¸¬ã€èªç¾©åˆ†å‰²ï¼‰ä¸­éƒ½å–å¾—äº†å„ªç•°æˆç¸¾ã€‚ä¸åƒ…åœ¨å–®ä¸€é …ç›®ä¸Šè¡¨ç¾å‡ºè‰²ï¼Œæ›´é›£èƒ½å¯è²´çš„æ˜¯åœ¨æ‰€æœ‰é …ç›®ä¸Šéƒ½ä¿æŒé ˜å…ˆï¼Œé€™è­‰æ˜äº†å®ƒæ˜¯ä¸€å€‹çœŸæ­£çš„ã€Œå…¨èƒ½é¸æ‰‹ã€ï¼Œè€Œä¸æ˜¯åªæ“…é•·æŸä¸€é …ç›®çš„ã€Œå°ˆé …é‹å‹•å“¡ã€ã€‚
                    </p>
                </div>
                <div class="analogy-card engineering">
                    <h4>âš™ï¸ å·¥ç¨‹é¡æ¯”</h4>
                    <p>
                        åœ¨ç³»çµ±æ¶æ§‹ä¸Šï¼Œé€™é¡ä¼¼æ–¼åŸºæº–æ¸¬è©¦ï¼ˆBenchmarkï¼‰çš„å…¨é¢è©•ä¼°ï¼šä¸€å€‹å¥½çš„ç³»çµ±ä¸åƒ…è¦åœ¨å–®ä¸€åŸºæº–ä¸Šè¡¨ç¾å„ªç§€ï¼Œæ›´è¦åœ¨å¤šå€‹ä¸åŒé¡å‹çš„åŸºæº–ä¸Šéƒ½é”åˆ°æœ€ä½³æ€§èƒ½ã€‚Swin Transformer åœ¨ä¸‰å€‹å®Œå…¨ä¸åŒé¡å‹çš„è¦–è¦ºä»»å‹™ä¸Šéƒ½é”åˆ° SOTAï¼Œè­‰æ˜äº†å…¶æ¶æ§‹è¨­è¨ˆçš„é€šç”¨æ€§å’Œå„ªè¶Šæ€§ã€‚
                    </p>
                </div>
            </div>

            <!-- Quote Block é‡‘å¥ -->
            <div class="quote-block">
                ã€Œä¸‰å€‹ä»»å‹™ï¼Œä¸‰å€‹ SOTAï¼šSwin Transformer è­‰æ˜äº†é€šç”¨è¦–è¦ºéª¨å¹¹ç¶²è·¯çš„å¯èƒ½æ€§ã€
            </div>

            <!-- æœ¬ç« é‡é»å›é¡§ -->
            <div class="chapter-summary">
                <h3>ğŸ’¡ æœ¬ç« é‡é»</h3>
                <ul>
                    <li><strong>ImageNet-1K</strong>ï¼šSwin-L é”åˆ° 87.3% top-1ï¼Œè¶…è¶Š ViT å’Œ ResNet</li>
                    <li><strong>COCO ç›®æ¨™æª¢æ¸¬</strong>ï¼š58.7 box APï¼Œ51.1 mask APï¼Œè¶…è¶Šä¹‹å‰ SOTA +2.7/+2.6</li>
                    <li><strong>ADE20K èªç¾©åˆ†å‰²</strong>ï¼š53.5 mIoUï¼Œè¶…è¶Š SETR +3.2</li>
                    <li><strong>èˆ‡ ResNet å°æ¯”</strong>ï¼šSwin-T åœ¨å››å€‹æª¢æ¸¬æ¡†æ¶ä¸Šéƒ½å¸¶ä¾† +3.4~4.2 box AP æå‡</li>
                    <li><strong>èˆ‡ ViT/DeiT å°æ¯”</strong>ï¼šç›¸ä¼¼æ¨¡å‹å¤§å°ä¸‹æ€§èƒ½æ›´å¥½ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«</li>
                    <li><strong>é€šç”¨æ€§é©—è­‰</strong>ï¼šåœ¨ä¸‰å€‹å®Œå…¨ä¸åŒé¡å‹çš„ä»»å‹™ä¸Šéƒ½é”åˆ° SOTA</li>
                </ul>
            </div>
        </div>

        <!-- Navigation -->
        <div class="chapter-nav">
            <a href="05-efficient-computation.html" class="prev">â† ä¸Šä¸€ç« </a>
            <a href="index.html" class="home">ğŸ“‘ ç›®éŒ„</a>
            <a href="07-conclusion.html" class="next">ä¸‹ä¸€ç« ï¼šçµè«– â†’</a>
        </div>
    </div>
</body>
</html>
