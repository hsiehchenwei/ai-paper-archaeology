<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç§»ä½è¦–çª—æ©Ÿåˆ¶ - Swin Transformer æ·±åº¦è§£æ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <!-- Hero Section -->
        <div class="hero-section" style="background-image: url('images/generated/hero_04_shifted_windows.png');">
            <div class="hero-overlay"></div>
            <div class="hero-content">
                <h1>Shifted Windowsï¼šæ•ˆç‡èˆ‡å»ºæ¨¡èƒ½åŠ›çš„å¹³è¡¡</h1>
                <p class="hero-subtitle">Swin Transformer çš„æ ¸å¿ƒå‰µæ–°</p>
                <div class="hero-meta">Swin Transformer æ·±åº¦è§£æ Â· ç¬¬ 4 ç« </div>
            </div>
        </div>

        <!-- Breadcrumb -->
        <nav class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <a href="index.html">Swin Transformer æ·±åº¦è§£æ</a>
            <span>/</span>
            <span class="current">ç¬¬ 4 ç« ï¼šç§»ä½è¦–çª—æ©Ÿåˆ¶</span>
        </nav>

        <div class="story-container">
            <!-- Drop Cap é–‹å ´ -->
            <p class="drop-cap">
                Shifted Windows æ˜¯ Swin Transformer æœ€æ ¸å¿ƒçš„å‰µæ–°ã€‚å®ƒå·§å¦™åœ°è§£æ±ºäº†ä¸€å€‹çœ‹ä¼¼çŸ›ç›¾çš„å•é¡Œï¼šå¦‚ä½•åœ¨ä¿æŒå±€éƒ¨è¨ˆç®—æ•ˆç‡çš„åŒæ™‚ï¼Œå»ºç«‹è·¨è¦–çª—çš„é€£æ¥ï¼Ÿç­”æ¡ˆæ˜¯ã€Œç§»ä½ã€â€”â€”é€šéåœ¨é€£çºŒå±¤ä¹‹é–“ç§»å‹•è¦–çª—åˆ†å€ï¼Œè®“è³‡è¨Šèƒ½å¤ åœ¨ä¸åŒè¦–çª—ä¹‹é–“æµå‹•ï¼Œæ—¢ä¿æŒäº†ç·šæ€§è¤‡é›œåº¦ï¼Œåˆå¤§å¹…æå‡äº†å»ºæ¨¡èƒ½åŠ›ã€‚
            </p>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>å…¨å±€è‡ªæ³¨æ„åŠ›çš„å•é¡Œ</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        The standard Transformer architecture and its adaptation for image classification both conduct global self-attention, where the relationships between a token and all other tokens are computed. The global computation leads to quadratic complexity with respect to the number of tokens, making it unsuitable for many vision problems requiring an immense set of tokens for dense prediction or to represent a high-resolution image.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        æ¨™æº– Transformer æ¶æ§‹åŠå…¶å°åœ–åƒåˆ†é¡çš„é©é…éƒ½é€²è¡Œå…¨å±€è‡ªæ³¨æ„åŠ›ï¼Œå…¶ä¸­è¨ˆç®— token èˆ‡æ‰€æœ‰å…¶ä»– token ä¹‹é–“çš„é—œä¿‚ã€‚å…¨å±€è¨ˆç®—å°è‡´ç›¸å°æ–¼ token æ•¸é‡çš„äºŒæ¬¡è¤‡é›œåº¦ï¼Œä½¿å…¶ä¸é©åˆè¨±å¤šéœ€è¦å¤§é‡ token é€²è¡Œå¯†é›†é æ¸¬æˆ–è¡¨ç¤ºé«˜è§£æåº¦åœ–åƒçš„è¦–è¦ºå•é¡Œã€‚
                    </div>
                </div>
            </div>

            <div class="key-concept">
                <h4>âš ï¸ è¤‡é›œåº¦å•é¡Œ</h4>
                <p>
                    å°æ–¼ä¸€å€‹ $h \times w$ çš„åœ–åƒï¼Œå…¨å±€è‡ªæ³¨æ„åŠ›çš„è¨ˆç®—è¤‡é›œåº¦æ˜¯ $O((hw)^2)$ã€‚ç•¶åœ–åƒè§£æåº¦ç‚º 224Ã—224 æ™‚ï¼Œéœ€è¦è¨ˆç®— 50,176 å€‹ token ä¹‹é–“çš„é—œä¿‚ï¼Œé€™åœ¨è¨ˆç®—ä¸Šæ˜¯ä¸å¯è¡Œçš„ã€‚
                </p>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>å±€éƒ¨è¦–çª—è‡ªæ³¨æ„åŠ›</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        For efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner. Supposing each window contains $M\times M$ patches, the computational complexity of a global MSA module and a window based one on an image of $h\times w$ patches are:
                        \begin{align}
                        &\Omega (\text{MSA}) = 4hwC^2 + 2 (hw)^2C, \\
                        &\Omega (\text{W-MSA}) = 4hwC^2 + 2 M^2 hwC,
                        \end{align}
                        where the former is quadratic to patch number $hw$, and the latter is linear when $M$ is fixed (set to $7$ by default).
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        ç‚ºäº†é«˜æ•ˆå»ºæ¨¡ï¼Œæˆ‘å€‘æå‡ºåœ¨å±€éƒ¨è¦–çª—å…§è¨ˆç®—è‡ªæ³¨æ„åŠ›ã€‚è¦–çª—ä»¥éé‡ç–Šçš„æ–¹å¼å‡å‹»åŠƒåˆ†åœ–åƒã€‚å‡è¨­æ¯å€‹è¦–çª—åŒ…å« $M\times M$ å€‹ patchï¼Œå…¨å±€ MSA æ¨¡çµ„å’ŒåŸºæ–¼è¦–çª—çš„æ¨¡çµ„åœ¨ $h\times w$ å€‹ patch çš„åœ–åƒä¸Šçš„è¨ˆç®—è¤‡é›œåº¦ç‚ºï¼š
                        \begin{align}
                        &\Omega (\text{MSA}) = 4hwC^2 + 2 (hw)^2C, \\
                        &\Omega (\text{W-MSA}) = 4hwC^2 + 2 M^2 hwC,
                        \end{align}
                        å…¶ä¸­å‰è€…ç›¸å°æ–¼ patch æ•¸é‡ $hw$ æ˜¯äºŒæ¬¡çš„ï¼Œå¾Œè€…åœ¨ $M$ å›ºå®šæ™‚ï¼ˆé è¨­ç‚º $7$ï¼‰æ˜¯ç·šæ€§çš„ã€‚
                    </div>
                </div>
            </div>

            <!-- AI ç”Ÿæˆæ¦‚å¿µåœ– -->
            <div class="figure figure-ai">
                <img src="images/generated/concept_complexity_03.png" alt="è¤‡é›œåº¦å°æ¯”åœ–">
                <div class="caption">
                    ğŸ’¡ <strong>AI åœ–è§£ï¼š</strong>è¨ˆç®—è¤‡é›œåº¦å°æ¯”â€”â€”å¾ O(nÂ²) åˆ° O(n) çš„æ•ˆç‡æå‡
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>Shifted Window çš„é—œéµå‰µæ–°</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        The window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.
                        
                        As illustrated in Figure 2, the first module uses a regular window partitioning strategy which starts from the top-left pixel, and the $8\times 8$ feature map is evenly partitioned into $2\times2$ windows of size $4\times4$ ($M=4$). Then, the next module adopts a windowing configuration that is shifted from that of the preceding layer, by displacing the windows by $ (\lfloor\frac{M}{2} \rfloor, \lfloor\frac{M}{2} \rfloor)$ pixels from the regularly partitioned windows.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        åŸºæ–¼è¦–çª—çš„è‡ªæ³¨æ„åŠ›æ¨¡çµ„ç¼ºä¹è·¨è¦–çª—çš„é€£æ¥ï¼Œé€™é™åˆ¶äº†å…¶å»ºæ¨¡èƒ½åŠ›ã€‚ç‚ºäº†åœ¨ä¿æŒéé‡ç–Šè¦–çª—é«˜æ•ˆè¨ˆç®—çš„åŒæ™‚å¼•å…¥è·¨è¦–çª—é€£æ¥ï¼Œæˆ‘å€‘æå‡ºäº†ä¸€ç¨®ç§»ä½è¦–çª—åˆ†å€æ–¹æ³•ï¼Œåœ¨é€£çºŒçš„ Swin Transformer block ä¹‹é–“äº¤æ›¿ä½¿ç”¨å…©ç¨®åˆ†å€é…ç½®ã€‚
                        
                        å¦‚åœ– 2 æ‰€ç¤ºï¼Œç¬¬ä¸€å€‹æ¨¡çµ„ä½¿ç”¨å¾å·¦ä¸Šè§’åƒç´ é–‹å§‹çš„å¸¸è¦è¦–çª—åˆ†å€ç­–ç•¥ï¼Œ$8\times 8$ çš„ç‰¹å¾µåœ–è¢«å‡å‹»åŠƒåˆ†ç‚º $2\times2$ å€‹å¤§å°ç‚º $4\times4$ï¼ˆ$M=4$ï¼‰çš„è¦–çª—ã€‚ç„¶å¾Œï¼Œä¸‹ä¸€å€‹æ¨¡çµ„æ¡ç”¨å¾å‰ä¸€å±¤ç§»ä½çš„è¦–çª—é…ç½®ï¼Œé€šéå°‡è¦–çª—å¾å¸¸è¦åˆ†å€è¦–çª—ç§»å‹• $ (\lfloor\frac{M}{2} \rfloor, \lfloor\frac{M}{2} \rfloor)$ å€‹åƒç´ ã€‚
                    </div>
                </div>
            </div>

            <!-- åŸå§‹åœ–ç‰‡å±•ç¤º -->
            <div class="figure figure-original">
                <img src="images/original/teaser_v4.png" alt="Shifted Window æ©Ÿåˆ¶ç¤ºæ„åœ–">
                <div class="caption">
                    <strong>Figure 2:</strong> Shifted Window æ©Ÿåˆ¶ç¤ºæ„åœ–ï¼ˆè«–æ–‡åŸåœ–ï¼‰
                </div>
                <div class="explanation">
                    <h4>ğŸ–¼ï¸ åŸæ–‡åœ–è¡¨è§£æ</h4>
                    <p>
                        é€™å¼µåœ–æ¸…æ™°åœ°å±•ç¤ºäº† Shifted Window çš„å·¥ä½œåŸç†ï¼š
                    </p>
                    <ul>
                        <li><strong>å·¦åœ–ï¼ˆLayer lï¼‰</strong>ï¼šå¸¸è¦è¦–çª—åˆ†å€ï¼Œ8Ã—8 ç‰¹å¾µåœ–è¢«åŠƒåˆ†ç‚º 2Ã—2 å€‹ 4Ã—4 è¦–çª—</li>
                        <li><strong>å³åœ–ï¼ˆLayer l+1ï¼‰</strong>ï¼šè¦–çª—è¢«ç§»ä½ï¼ˆç§»å‹• âŒŠM/2âŒ‹ = 2 å€‹åƒç´ ï¼‰ï¼Œæ–°çš„è¦–çª—è·¨è¶Šäº†å‰ä¸€å±¤è¦–çª—çš„é‚Šç•Œ</li>
                    </ul>
                    <p>
                        é€™ç¨®è¨­è¨ˆè®“è³‡è¨Šèƒ½å¤ åœ¨ä¸åŒè¦–çª—ä¹‹é–“æµå‹•ï¼ŒåŒæ™‚ä¿æŒäº†å±€éƒ¨è¨ˆç®—çš„æ•ˆç‡ã€‚
                    </p>
                </div>
            </div>

            <!-- AI ç”Ÿæˆæ¦‚å¿µåœ– -->
            <div class="figure figure-ai">
                <img src="images/generated/concept_shifted_window_02.png" alt="Shifted Window æ©Ÿåˆ¶åœ–è§£">
                <div class="caption">
                    ğŸ’¡ <strong>AI åœ–è§£ï¼š</strong>Shifted Window æ©Ÿåˆ¶è©³è§£â€”â€”è¦–çª—å¦‚ä½•åœ¨å±¤ä¹‹é–“ç§»å‹•ä¸¦å»ºç«‹é€£æ¥
                </div>
            </div>

            <!-- æ¦‚å¿µå‹•ç•« -->
            <div class="video-container">
                <video autoplay loop muted playsinline>
                    <source src="images/videos/shifted_window_mechanism.mp4" type="video/mp4">
                </video>
                <div class="caption">ğŸ¬ Shifted Window æ©Ÿåˆ¶å‹•ç•«ï¼šå±•ç¤ºè¦–çª—å¦‚ä½•åœ¨é€£çºŒå±¤ä¹‹é–“ç§»ä½ä¸¦å»ºç«‹è·¨è¦–çª—é€£æ¥</div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>æ•¸å­¸å…¬å¼ï¼šSwin Transformer Block</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        With the shifted window partitioning approach, consecutive Swin Transformer blocks are computed as
                        \begin{align}
                            &{{\hat{\bf{z}}}^{l}} = \text{W-MSA}\left( {\text{LN}\left( {{{\bf{z}}^{l - 1}}} \right)} \right) + {\bf{z}}^{l - 1},\nonumber\\
                            &{{\bf{z}}^l} = \text{MLP}\left( {\text{LN}\left( {{{\hat{\bf{z}}}^{l}}} \right)} \right) + {{\hat{\bf{z}}}^{l}},\nonumber\\
                            &{{\hat{\bf{z}}}^{l+1}} = \text{SW-MSA}\left( {\text{LN}\left( {{{\bf{z}}^{l}}} \right)} \right) + {\bf{z}}^{l}, \nonumber\\
                            &{{\bf{z}}^{l+1}} = \text{MLP}\left( {\text{LN}\left( {{{\hat{\bf{z}}}^{l+1}}} \right)} \right) + {{\hat{\bf{z}}}^{l+1}}, \label{eq.swin}
                        \end{align}
                        where ${\hat{\bf{z}}}^l$ and ${\bf{z}}^l$ denote the output features of the (S)W-MSA module and the MLP module for block $l$, respectively; $\text{W-MSA}$ and $\text{SW-MSA}$ denote window based multi-head self-attention using regular and shifted window partitioning configurations, respectively.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        ä½¿ç”¨ç§»ä½è¦–çª—åˆ†å€æ–¹æ³•ï¼Œé€£çºŒçš„ Swin Transformer block è¨ˆç®—å¦‚ä¸‹ï¼š
                        \begin{align}
                            &{{\hat{\bf{z}}}^{l}} = \text{W-MSA}\left( {\text{LN}\left( {{{\bf{z}}^{l - 1}}} \right)} \right) + {\bf{z}}^{l - 1},\nonumber\\
                            &{{\bf{z}}^l} = \text{MLP}\left( {\text{LN}\left( {{{\hat{\bf{z}}}^{l}}} \right)} \right) + {{\hat{\bf{z}}}^{l}},\nonumber\\
                            &{{\hat{\bf{z}}}^{l+1}} = \text{SW-MSA}\left( {\text{LN}\left( {{{\bf{z}}^{l}}} \right)} \right) + {\bf{z}}^{l}, \nonumber\\
                            &{{\bf{z}}^{l+1}} = \text{MLP}\left( {\text{LN}\left( {{{\hat{\bf{z}}}^{l+1}}} \right)} \right) + {{\hat{\bf{z}}}^{l+1}}, \label{eq.swin}
                        \end{align}
                        å…¶ä¸­ ${\hat{\bf{z}}}^l$ å’Œ ${\bf{z}}^l$ åˆ†åˆ¥è¡¨ç¤º block $l$ çš„ (S)W-MSA æ¨¡çµ„å’Œ MLP æ¨¡çµ„çš„è¼¸å‡ºç‰¹å¾µï¼›$\text{W-MSA}$ å’Œ $\text{SW-MSA}$ åˆ†åˆ¥è¡¨ç¤ºä½¿ç”¨å¸¸è¦å’Œç§»ä½è¦–çª—åˆ†å€é…ç½®çš„åŸºæ–¼è¦–çª—çš„å¤šé ­è‡ªæ³¨æ„åŠ›ã€‚
                    </div>
                </div>
            </div>

            <div class="key-concept">
                <h4>ğŸ’¡ å…¬å¼è§£æ</h4>
                <p>
                    é€™å€‹å…¬å¼å±•ç¤ºäº† Swin Transformer Block çš„å®Œæ•´è¨ˆç®—æµç¨‹ï¼š
                </p>
                <ul>
                    <li><strong>Block l</strong>ï¼šä½¿ç”¨ W-MSAï¼ˆå¸¸è¦è¦–çª—ï¼‰+ MLP</li>
                    <li><strong>Block l+1</strong>ï¼šä½¿ç”¨ SW-MSAï¼ˆç§»ä½è¦–çª—ï¼‰+ MLP</li>
                </ul>
                <p>
                    å…©å€‹ block æˆå°å‡ºç¾ï¼Œäº¤æ›¿ä½¿ç”¨å¸¸è¦å’Œç§»ä½è¦–çª—ï¼Œæ—¢ä¿æŒäº†æ•ˆç‡ï¼Œåˆå»ºç«‹äº†è·¨è¦–çª—é€£æ¥ã€‚
                </p>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>ç‚ºä»€éº¼ Shifted Windows å¦‚æ­¤æœ‰æ•ˆï¼Ÿ</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        The shifted window partitioning approach introduces connections between neighboring non-overlapping windows in the previous layer and is found to be effective in image classification, object detection, and semantic segmentation, as shown in Table 4. This strategy is also efficient in regards to real-world latency: all <em>query</em> patches within a window share the same <em>key</em> set, which facilitates memory access in hardware.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        ç§»ä½è¦–çª—åˆ†å€æ–¹æ³•åœ¨å‰ä¸€å±¤çš„ç›¸é„°éé‡ç–Šè¦–çª—ä¹‹é–“å¼•å…¥é€£æ¥ï¼Œä¸¦è¢«ç™¼ç¾åœ¨åœ–åƒåˆ†é¡ã€ç›®æ¨™æª¢æ¸¬å’Œèªç¾©åˆ†å‰²ä¸­æœ‰æ•ˆï¼Œå¦‚è¡¨ 4 æ‰€ç¤ºã€‚é€™å€‹ç­–ç•¥åœ¨å¯¦éš›å»¶é²æ–¹é¢ä¹Ÿå¾ˆé«˜æ•ˆï¼šè¦–çª—å…§çš„æ‰€æœ‰ <em>query</em> patch å…±äº«ç›¸åŒçš„ <em>key</em> é›†åˆï¼Œé€™æœ‰åˆ©æ–¼ç¡¬é«”ä¸­çš„è¨˜æ†¶é«”è¨ªå•ã€‚
                    </div>
                </div>
            </div>

            <!-- é›™é‡é¡æ¯” -->
            <div class="analogy-section">
                <div class="analogy-card life">
                    <h4>ğŸ  ç”Ÿæ´»é¡æ¯”</h4>
                    <p>
                        å°±åƒæ»‘å‹•æ‹¼åœ–ä¸€æ¨£ï¼šç¬¬ä¸€å±¤å°‡æ‹¼åœ–åˆ†æˆå›ºå®šçš„ 4Ã—4 å€å¡Šï¼Œæ¯å€‹å€å¡Šå…§éƒ¨å¯ä»¥è‡ªç”±ç§»å‹•ï¼ˆå±€éƒ¨æ³¨æ„åŠ›ï¼‰ã€‚ç¬¬äºŒå±¤å°‡æ‹¼åœ–ç¨å¾®æ»‘å‹•ä¸€ä¸‹ï¼Œæ–°çš„å€å¡Šè·¨è¶Šäº†åŸä¾†å€å¡Šçš„é‚Šç•Œï¼Œé€™æ¨£ä¸åŒå€å¡Šä¹‹é–“å°±èƒ½ã€Œçœ‹åˆ°ã€å½¼æ­¤äº†ã€‚é€šéé€™ç¨®æ–¹å¼ï¼Œæ—¢ä¿æŒäº†æ¯å€‹å€å¡Šå…§éƒ¨çš„æ•ˆç‡ï¼Œåˆå»ºç«‹äº†å€å¡Šä¹‹é–“çš„é€£æ¥ã€‚
                    </p>
                </div>
                <div class="analogy-card engineering">
                    <h4>âš™ï¸ å·¥ç¨‹é¡æ¯”</h4>
                    <p>
                        åœ¨ç³»çµ±æ¶æ§‹ä¸Šï¼Œé€™é¡ä¼¼æ–¼åˆ†ç‰‡è³‡æ–™åº«ï¼ˆSharded Databaseï¼‰çš„è¨­è¨ˆï¼šæ¯å€‹è¦–çª—å°±åƒä¸€å€‹è³‡æ–™åº«åˆ†ç‰‡ï¼Œå…§éƒ¨æŸ¥è©¢éå¸¸é«˜æ•ˆï¼ˆå±€éƒ¨è¨ˆç®—ï¼‰ã€‚Shifted Windows å°±åƒåœ¨ä¸åŒåˆ†ç‰‡ä¹‹é–“å»ºç«‹ã€Œè¤‡è£½ã€æˆ–ã€ŒåŒæ­¥ã€æ©Ÿåˆ¶ï¼Œè®“è³‡è¨Šèƒ½å¤ åœ¨ä¸åŒåˆ†ç‰‡ä¹‹é–“æµå‹•ï¼Œæ—¢ä¿æŒäº†åˆ†ç‰‡çš„æ•ˆç‡å„ªå‹¢ï¼Œåˆå¯¦ç¾äº†å…¨å±€çš„ä¸€è‡´æ€§ã€‚
                    </p>
                </div>
            </div>

            <!-- Quote Block é‡‘å¥ -->
            <div class="quote-block">
                ã€Œç§»ä½è¦–çª—ï¼šåœ¨æ•ˆç‡èˆ‡å»ºæ¨¡èƒ½åŠ›ä¹‹é–“æ‰¾åˆ°å®Œç¾å¹³è¡¡çš„è¨­è¨ˆæ™ºæ…§ã€
            </div>

            <!-- æœ¬ç« é‡é»å›é¡§ -->
            <div class="chapter-summary">
                <h3>ğŸ’¡ æœ¬ç« é‡é»</h3>
                <ul>
                    <li><strong>å…¨å±€è‡ªæ³¨æ„åŠ›çš„å•é¡Œ</strong>ï¼šO(nÂ²) è¤‡é›œåº¦ï¼Œä¸é©åˆé«˜è§£æåº¦åœ–åƒ</li>
                    <li><strong>å±€éƒ¨è¦–çª—è§£æ±ºæ–¹æ¡ˆ</strong>ï¼šå°‡è¨ˆç®—é™åˆ¶åœ¨ MÃ—M è¦–çª—å…§ï¼Œå¯¦ç¾ O(n) è¤‡é›œåº¦</li>
                    <li><strong>Shifted Windows å‰µæ–°</strong>ï¼šåœ¨é€£çºŒå±¤ä¹‹é–“ç§»ä½è¦–çª—ï¼Œå»ºç«‹è·¨è¦–çª—é€£æ¥</li>
                    <li><strong>W-MSA å’Œ SW-MSA</strong>ï¼šäº¤æ›¿ä½¿ç”¨å¸¸è¦å’Œç§»ä½è¦–çª—åˆ†å€</li>
                    <li><strong>ç¡¬é«”æ•ˆç‡</strong>ï¼šåŒä¸€è¦–çª—å…§çš„ query å…±äº« key é›†åˆï¼Œå„ªåŒ–è¨˜æ†¶é«”è¨ªå•</li>
                    <li><strong>å¯¦é©—é©—è­‰</strong>ï¼šåœ¨ä¸‰å€‹ä»»å‹™ä¸Šéƒ½å¸¶ä¾†é¡¯è‘—æ€§èƒ½æå‡</li>
                </ul>
            </div>
        </div>

        <!-- Navigation -->
        <div class="chapter-nav">
            <a href="03-architecture.html" class="prev">â† ä¸Šä¸€ç« </a>
            <a href="index.html" class="home">ğŸ“‘ ç›®éŒ„</a>
            <a href="05-efficient-computation.html" class="next">ä¸‹ä¸€ç« ï¼šé«˜æ•ˆè¨ˆç®— â†’</a>
        </div>
    </div>
</body>
</html>
