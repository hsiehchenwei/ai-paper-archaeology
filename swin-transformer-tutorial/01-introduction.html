<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>引言與摘要 - Swin Transformer 深度解析</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <!-- Hero Section -->
        <div class="hero-section" style="background-image: url('images/generated/hero_01_introduction.png');">
            <div class="hero-overlay"></div>
            <div class="hero-content">
                <h1>層次化視覺 Transformer 的誕生</h1>
                <p class="hero-subtitle">從 ViT 的局限到 Swin 的突破</p>
                <div class="hero-meta">Swin Transformer 深度解析 · 第 1 章</div>
            </div>
        </div>

        <!-- Breadcrumb -->
        <nav class="breadcrumb">
            <a href="../index.html">🏠 首頁</a>
            <span>/</span>
            <a href="index.html">Swin Transformer 深度解析</a>
            <span>/</span>
            <span class="current">第 1 章：引言與摘要</span>
        </nav>

        <div class="story-container">
            <!-- Drop Cap 開場 -->
            <p class="drop-cap">
                2021 年 3 月，Microsoft Research Asia 的研究團隊提出了一個革命性的想法：如果我們能讓 Transformer 在視覺領域像在 NLP 領域一樣強大，會發生什麼？這個想法催生了 Swin Transformer——一個能夠作為通用視覺骨幹網路的層次化 Transformer。與 ViT 不同，Swin Transformer 不僅解決了單一解析度的問題，更實現了線性計算複雜度，讓 Transformer 真正成為視覺任務的通用解決方案。
            </p>

            <div class="section-divider"><span>✦</span></div>

            <!-- 論文摘要 -->
            <div class="original-quote">
                <div class="icon">📄</div>
                <div class="content">
                    <div class="en">
                        <strong>論文原文（Abstract）</strong><br><br>
                        This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with <strong>S</strong>hifted <strong>win</strong>dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size.
                    </div>
                    <div class="zh">
                        <strong>中文翻譯</strong><br><br>
                        本論文提出了一個新的視覺 Transformer，稱為 Swin Transformer，能夠作為電腦視覺的通用骨幹網路。將 Transformer 從語言領域適配到視覺領域的挑戰源於兩個領域之間的差異，例如視覺實體尺度的巨大變化，以及圖像中像素相較於文字中詞彙的高解析度。為了解決這些差異，我們提出了一個層次化 Transformer，其表示是通過<strong>移位視窗</strong>（Shifted Windows）計算的。移位視窗方案通過將自注意力計算限制在非重疊的局部視窗內，同時允許跨視窗連接，帶來了更高的效率。這種層次化架構具有在不同尺度建模的靈活性，並且相對於圖像大小具有線性計算複雜度。
                    </div>
                </div>
            </div>

            <!-- AI 生成概念圖 -->
            <div class="figure figure-ai">
                <img src="images/generated/concept_hierarchical_01.jpg" alt="層次化架構概念圖">
                <div class="caption">
                    💡 <strong>AI 圖解：</strong>層次化特徵圖構建——從小的 patch 逐步合併成大的特徵圖，就像俄羅斯套娃一樣
                </div>
            </div>

            <h2>ViT 的局限性</h2>

            <p>
                在 Swin Transformer 出現之前，Vision Transformer (ViT) 已經證明了 Transformer 可以在視覺任務上取得優秀的表現。然而，ViT 有兩個關鍵的局限性：
            </p>

            <div class="key-concept">
                <h4>⚠️ ViT 的兩個核心問題</h4>
                <ul>
                    <li><strong>單一解析度</strong>：ViT 只產生單一低解析度的特徵圖，無法像 CNN 那樣構建多尺度特徵金字塔</li>
                    <li><strong>二次複雜度</strong>：全局自注意力的計算複雜度是 O(n²)，對於高解析度圖像來說計算成本過高</li>
                </ul>
            </div>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>📄 論文原文</strong><br><br>
                        In existing Transformer-based models, tokens are all of a fixed scale, a property unsuitable for these vision applications. Another difference is the much higher resolution of pixels in images compared to words in passages of text. There exist many vision tasks such as semantic segmentation that require dense prediction at the pixel level, and this would be intractable for Transformer on high-resolution images, as the computational complexity of its self-attention is quadratic to image size.
                    </div>
                    <div class="zh">
                        <strong>翻譯</strong><br><br>
                        在現有的基於 Transformer 的模型中，token 都是固定尺度的，這個特性不適合這些視覺應用。另一個差異是圖像中像素的解析度遠高於文字段落中的詞彙。存在許多視覺任務，如語義分割，需要在像素級別進行密集預測，這對於 Transformer 在高解析度圖像上來說是難以處理的，因為其自注意力的計算複雜度相對於圖像大小是二次的。
                    </div>
                </div>
            </div>

            <!-- 三個核心概念的詳細解釋 -->
            <div class="key-concept" style="background: linear-gradient(135deg, #fef3c720 0%, #fde68a20 100%); border-left: 5px solid #f59e0b; margin: 30px 0; padding: 25px;">
                <h4>🔍 三個核心概念的深度解析</h4>
                
                <!-- AI 生成概念圖 -->
                <div class="figure figure-ai" style="margin: 25px 0;">
                    <img src="images/generated/swin_fixed_scale_tokens_explanation.jpg" alt="三個核心概念視覺化圖解">
                    <div class="caption">
                        💡 <strong>AI 圖解：</strong> 三個核心概念的視覺化說明——固定尺度 tokens、高解析度像素、語義分割密集預測
                    </div>
                </div>
                
                <div style="margin: 25px 0;">
                    <h5>1️⃣ Token 都是固定尺度的（Fixed Scale Tokens）</h5>
                    
                    <p><strong>📝 文字解釋：</strong></p>
                    <p>
                        在 NLP 的 Transformer 中，每個 token（詞彙）代表一個「原子單位」，例如「我」、「愛」、「AI」都是同樣大小的 token。
                        但在視覺任務中，物體的大小差異極大：一隻螞蟻可能只佔圖片的 0.01%，而一棟建築物可能佔據 50% 的畫面。
                    </p>
                    
                    <p><strong>🏠 生活類比：</strong></p>
                    <p>
                        想像你在閱讀一本書，每個字的大小都一樣（固定尺度的 token）。但當你看一張照片時，照片中的物體大小完全不同：
                        遠處的山可能很小，近處的花朵可能很大。如果我們用「固定大小」的方式來處理這些物體，就像用同樣大小的框框來框住螞蟻和大象一樣不合理。
                    </p>
                    
                    <p><strong>⚙️ 工程上的例子：</strong></p>
                    <p>
                        在 ViT 中，一張 224×224 的圖片被切成 16×16 的 patches，每個 patch 都是同樣大小（16×16 像素）。
                        這意味著：
                    </p>
                    <ul>
                        <li>一個佔據 100×100 像素的大物體會被切成多個 patches，模型需要「拼湊」這些 patches 才能理解這是一個完整的物體</li>
                        <li>一個只佔 5×5 像素的小物體可能只佔一個 patch 的一小部分，資訊可能被稀釋</li>
                        <li>模型無法像 CNN 那樣自然地處理「多尺度」的物體（CNN 透過不同大小的卷積核來處理不同尺度的特徵）</li>
                    </ul>
                </div>

                <div style="margin: 25px 0; padding-top: 25px; border-top: 2px dashed #e5e7eb;">
                    <h5>2️⃣ 圖像中像素的解析度遠高於文字段落中的詞彙（High Resolution Pixels）</h5>
                    
                    <p><strong>📝 文字解釋：</strong></p>
                    <p>
                        一段文字可能只有幾十到幾百個詞彙（tokens），但一張高解析度圖片可能有數百萬個像素。
                        例如：一張 1920×1080 的圖片有 2,073,600 個像素，而一段 500 字的文章可能只有 500-1000 個 tokens。
                    </p>
                    
                    <p><strong>🏠 生活類比：</strong></p>
                    <p>
                        想像你在閱讀一本小說，整本書可能有 10 萬個字（詞彙），但一張 4K 照片可能有 800 萬個像素點。
                        如果用處理文字的方式（每個 token 都要和其他所有 token 計算注意力），處理這張照片就像要讓每個像素都「認識」其他 800 萬個像素，這在計算上幾乎不可能。
                    </p>
                    
                    <p><strong>⚙️ 工程上的例子：</strong></p>
                    <p>
                        <strong>文字處理（BERT/GPT）：</strong>
                    </p>
                    <ul>
                        <li>一段文字：500 個 tokens</li>
                        <li>Self-Attention 計算：500 × 500 = 250,000 次計算</li>
                        <li>這是可以接受的計算量</li>
                    </ul>
                    
                    <p><strong>圖像處理（ViT）：</strong></p>
                    <ul>
                        <li>一張 224×224 圖片 → 196 個 patches（16×16 切分）</li>
                        <li>Self-Attention 計算：196 × 196 = 38,416 次計算（還可以接受）</li>
                        <li>但如果是 1024×1024 的高解析度圖片 → 4,096 個 patches</li>
                        <li>Self-Attention 計算：4,096 × 4,096 = <strong>16,777,216 次計算</strong>（計算量暴增！）</li>
                    </ul>
                    
                    <p>
                        這就是為什麼 ViT 在處理高解析度圖片時會遇到計算瓶頸。Swin Transformer 透過「局部視窗」機制，將計算限制在每個視窗內，讓計算複雜度從 O(n²) 降為 O(n)。
                    </p>
                </div>

                <div style="margin: 25px 0; padding-top: 25px; border-top: 2px dashed #e5e7eb;">
                    <h5>3️⃣ 語義分割需要在像素級別進行密集預測（Dense Prediction at Pixel Level）</h5>
                    
                    <p><strong>📝 文字解釋：</strong></p>
                    <p>
                        語義分割（Semantic Segmentation）是一個「密集預測」任務，需要對圖片的<strong>每一個像素</strong>都進行分類，判斷這個像素屬於哪個類別（例如：天空、建築、道路、車輛等）。
                        這與圖像分類不同：分類只需要判斷「整張圖片是什麼」，而分割需要判斷「每個像素是什麼」。
                    </p>
                    
                    <p><strong>🏠 生活類比：</strong></p>
                    <p>
                        <strong>圖像分類</strong>就像回答「這張照片裡有什麼？」（答案：一隻貓）
                        <br>
                        <strong>語義分割</strong>就像用不同顏色的筆，把照片中的每個區域都標記出來：
                    </p>
                    <ul>
                        <li>藍色區域 = 天空</li>
                        <li>綠色區域 = 草地</li>
                        <li>灰色區域 = 道路</li>
                        <li>紅色區域 = 車輛</li>
                    </ul>
                    <p>
                        這就像是在一張地圖上，把每個區域都標上不同的顏色和名稱，需要「逐像素」地進行標記。
                    </p>
                    
                    <p><strong>⚙️ 工程上的例子：</strong></p>
                    <p>
                        <strong>圖像分類任務（ImageNet）：</strong>
                    </p>
                    <ul>
                        <li>輸入：224×224 圖片</li>
                        <li>輸出：1 個類別標籤（例如：「貓」）</li>
                        <li>預測數量：1 個</li>
                    </ul>
                    
                    <p><strong>語義分割任務（Cityscapes）：</strong></p>
                    <ul>
                        <li>輸入：1024×512 圖片</li>
                        <li>輸出：524,288 個像素標籤（1024 × 512 = 524,288）</li>
                        <li>預測數量：524,288 個（每個像素都要預測）</li>
                    </ul>
                    
                    <p>
                        <strong>為什麼 ViT 難以處理？</strong>
                    </p>
                    <ul>
                        <li>ViT 只產生<strong>單一低解析度的特徵圖</strong>（例如：14×14 = 196 個特徵點）</li>
                        <li>但語義分割需要<strong>高解析度的預測</strong>（例如：1024×512 = 524,288 個預測點）</li>
                        <li>ViT 缺乏像 CNN 那樣的「層次化特徵金字塔」，無法自然地從低解析度特徵「上採樣」到高解析度預測</li>
                        <li>如果強行用 ViT 處理高解析度圖片，計算複雜度會是 O(n²)，對於 1024×1024 的圖片來說，這幾乎不可行</li>
                    </ul>
                    
                    <p>
                        <strong>Swin Transformer 的解決方案：</strong>
                    </p>
                    <ul>
                        <li>構建<strong>層次化特徵圖</strong>：從 56×56 → 28×28 → 14×14 → 7×7，就像 CNN 的特徵金字塔</li>
                        <li>使用<strong>局部視窗</strong>計算注意力，將複雜度從 O(n²) 降為 O(n)</li>
                        <li>可以自然地與 FPN（Feature Pyramid Network）或 U-Net 結合，進行密集預測任務</li>
                    </ul>
                </div>
            </div>

            <!-- 原始圖片展示 -->
            <div class="figure figure-original">
                <img src="images/original/teaser11.png" alt="Swin vs ViT 對比圖">
                <div class="caption">
                    <strong>Figure 1:</strong> Swin Transformer vs ViT 架構對比（論文原圖）
                </div>
                <div class="explanation">
                    <h4>🖼️ 原文圖表解析</h4>
                    <p>
                        這張圖清晰地展示了 Swin Transformer 與 ViT 的關鍵差異：
                    </p>
                    <ul>
                        <li><strong>左圖 (Swin)</strong>：構建層次化特徵圖，從小的 patch（灰色）逐步合併，在每個局部視窗（紅色）內計算自注意力，實現線性複雜度</li>
                        <li><strong>右圖 (ViT)</strong>：只產生單一低解析度的特徵圖，全局自注意力導致二次複雜度</li>
                    </ul>
                    <p>
                        Swin Transformer 的設計讓它能夠作為通用骨幹網路，同時適用於圖像分類和密集識別任務（如目標檢測和語義分割）。
                    </p>
                </div>
            </div>

            <div class="section-divider"><span>✦</span></div>

            <h2>Swin Transformer 的解決方案</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>📄 論文原文</strong><br><br>
                        To overcome these issues, we propose a general-purpose Transformer backbone, called Swin Transformer, which constructs hierarchical feature maps and has linear computational complexity to image size. As illustrated in Figure 1(a), Swin Transformer constructs a hierarchical representation by starting from small-sized patches (outlined in gray) and gradually merging neighboring patches in deeper Transformer layers. With these hierarchical feature maps, the Swin Transformer model can conveniently leverage advanced techniques for dense prediction such as feature pyramid networks (FPN) or U-Net. The linear computational complexity is achieved by computing self-attention locally within non-overlapping windows that partition an image (outlined in red). The number of patches in each window is fixed, and thus the complexity becomes linear to image size.
                    </div>
                    <div class="zh">
                        <strong>翻譯</strong><br><br>
                        為了解決這些問題，我們提出了一個通用 Transformer 骨幹網路，稱為 Swin Transformer，它構建層次化特徵圖，並且相對於圖像大小具有線性計算複雜度。如圖 1(a) 所示，Swin Transformer 通過從小的 patch（灰色輪廓）開始，在更深的 Transformer 層中逐漸合併相鄰的 patch 來構建層次化表示。有了這些層次化特徵圖，Swin Transformer 模型可以方便地利用先進的密集預測技術，如特徵金字塔網路（FPN）或 U-Net。線性計算複雜度是通過在劃分圖像的非重疊視窗（紅色輪廓）內局部計算自注意力來實現的。每個視窗中的 patch 數量是固定的，因此複雜度變為相對於圖像大小的線性。
                    </div>
                </div>
            </div>

            <!-- 雙重類比 -->
            <div class="analogy-section">
                <div class="analogy-card life">
                    <h4>🏠 生活類比</h4>
                    <p>
                        就像俄羅斯套娃一樣，Swin Transformer 從最小的「娃娃」（4×4 的 patch）開始，一層一層地合併，最終形成一個完整的「大娃娃」（整個圖像的特徵表示）。每一層都能看到不同尺度的資訊，就像從近距離看細節，到遠距離看整體一樣。
                    </p>
                </div>
                <div class="analogy-card engineering">
                    <h4>⚙️ 工程類比</h4>
                    <p>
                        在系統架構上，Swin Transformer 類似於多解析度的金字塔結構。就像地圖應用程式會提供不同縮放級別的地圖（街道級、城市級、國家級），Swin Transformer 也構建了多個解析度的特徵圖，讓模型能夠同時處理細節和全局資訊。而移位視窗機制就像是在不同層級之間建立「橋樑」，讓資訊能夠在不同尺度之間流動。
                    </p>
                </div>
            </div>

            <div class="section-divider"><span>✦</span></div>

            <h2>核心創新：Shifted Windows</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>📄 論文原文</strong><br><br>
                        A key design element of Swin Transformer is its <em>shift</em> of the window partition between consecutive self-attention layers, as illustrated in Figure 2. The shifted windows bridge the windows of the preceding layer, providing connections among them that significantly enhance modeling power. This strategy is also efficient in regards to real-world latency: all <em>query</em> patches within a window share the same <em>key</em> set, which facilitates memory access in hardware.
                    </div>
                    <div class="zh">
                        <strong>翻譯</strong><br><br>
                        Swin Transformer 的一個關鍵設計元素是在連續的自注意力層之間對視窗分區進行<em>移位</em>，如圖 2 所示。移位的視窗橋接了前一層的視窗，在它們之間提供連接，顯著增強了建模能力。這個策略在實際延遲方面也很高效：視窗內的所有 <em>query</em> patch 共享相同的 <em>key</em> 集合，這有利於硬體中的記憶體訪問。
                    </div>
                </div>
            </div>

            <!-- 原始圖片展示 -->
            <div class="figure figure-original">
                <img src="images/original/teaser_v4.png" alt="Shifted Window 機制示意圖">
                <div class="caption">
                    <strong>Figure 2:</strong> Shifted Window 機制示意圖（論文原圖）
                </div>
                <div class="explanation">
                    <h4>🖼️ 原文圖表解析</h4>
                    <p>
                        這張圖展示了 Swin Transformer 的核心創新——移位視窗機制：
                    </p>
                    <ul>
                        <li><strong>左圖（Layer l）</strong>：使用常規視窗分區，將 8×8 的特徵圖均勻劃分為 2×2 個 4×4 的視窗</li>
                        <li><strong>右圖（Layer l+1）</strong>：視窗分區被移位，新的視窗跨越了前一層視窗的邊界，建立了跨視窗連接</li>
                    </ul>
                    <p>
                        這種設計既保持了局部計算的效率，又通過移位實現了跨視窗的資訊交流，是 Swin Transformer 成功的關鍵。
                    </p>
                </div>
            </div>

            <div class="section-divider"><span>✦</span></div>

            <h2>卓越的實驗結果</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>📄 論文原文</strong><br><br>
                        The proposed Swin Transformer achieves strong performance on the recognition tasks of image classification, object detection and semantic segmentation. It outperforms the ViT / DeiT and ResNe(X)t models significantly with similar latency on the three tasks. Its 58.7 box AP and 51.1 mask AP on the COCO test-dev set surpass the previous state-of-the-art results by +2.7 box AP (Copy-paste without external data) and +2.6 mask AP (DetectoRS). On ADE20K semantic segmentation, it obtains 53.5 mIoU on the val set, an improvement of +3.2 mIoU over the previous state-of-the-art (SETR). It also achieves a top-1 accuracy of 87.3% on ImageNet-1K image classification.
                    </div>
                    <div class="zh">
                        <strong>翻譯</strong><br><br>
                        提出的 Swin Transformer 在圖像分類、目標檢測和語義分割的識別任務上取得了強勁的表現。它在三個任務上以相似的延遲顯著超越了 ViT / DeiT 和 ResNe(X)t 模型。它在 COCO test-dev 集上取得了 58.7 box AP 和 51.1 mask AP，超過了之前的最佳結果 +2.7 box AP（Copy-paste，無外部數據）和 +2.6 mask AP（DetectoRS）。在 ADE20K 語義分割上，它在 val 集上取得了 53.5 mIoU，比之前的最佳結果（SETR）提高了 +3.2 mIoU。它還在 ImageNet-1K 圖像分類上取得了 87.3% 的 top-1 準確率。
                    </div>
                </div>
            </div>

            <div class="paradigm-grid">
                <div class="paradigm-card">
                    <h4>📊 ImageNet-1K</h4>
                    <p><strong>87.3%</strong> top-1 準確率</p>
                    <p>超越 ViT 和 ResNet 的表現</p>
                </div>
                <div class="paradigm-card">
                    <h4>🎯 COCO 目標檢測</h4>
                    <p><strong>58.7</strong> box AP</p>
                    <p><strong>51.1</strong> mask AP</p>
                    <p>超越之前 SOTA +2.7/+2.6</p>
                </div>
                <div class="paradigm-card">
                    <h4>🖼️ ADE20K 語義分割</h4>
                    <p><strong>53.5</strong> mIoU</p>
                    <p>超越之前 SOTA +3.2</p>
                </div>
            </div>

            <div class="section-divider"><span>✦</span></div>

            <h2>統一架構的願景</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>📄 論文原文</strong><br><br>
                        It is our belief that a unified architecture across computer vision and natural language processing could benefit both fields, since it would facilitate joint modeling of visual and textual signals and the modeling knowledge from both domains can be more deeply shared. We hope that Swin Transformer's strong performance on various vision problems can drive this belief deeper in the community and encourage unified modeling of vision and language signals.
                    </div>
                    <div class="zh">
                        <strong>翻譯</strong><br><br>
                        我們相信，電腦視覺和自然語言處理之間的統一架構可以讓兩個領域都受益，因為它將促進視覺和文字信號的聯合建模，並且兩個領域的建模知識可以更深入地共享。我們希望 Swin Transformer 在各種視覺問題上的強勁表現能夠在社群中深化這一信念，並鼓勵視覺和語言信號的統一建模。
                    </div>
                </div>
            </div>

            <!-- Quote Block 金句 -->
            <div class="quote-block">
                「層次化架構 + 移位視窗 = 線性複雜度的通用視覺骨幹網路」
            </div>

            <!-- 本章重點回顧 -->
            <div class="chapter-summary">
                <h3>💡 本章重點</h3>
                <ul>
                    <li><strong>核心問題</strong>：ViT 的局限性（單一解析度、二次複雜度）阻礙了 Transformer 成為通用視覺骨幹網路</li>
                    <li><strong>Swin 的解決方案</strong>：層次化架構 + 移位視窗機制，實現線性複雜度</li>
                    <li><strong>關鍵創新</strong>：Shifted Windows 在保持效率的同時建立跨視窗連接</li>
                    <li><strong>卓越表現</strong>：在 ImageNet、COCO、ADE20K 三個任務上都達到 SOTA</li>
                    <li><strong>願景</strong>：統一視覺和語言的架構，促進跨領域知識共享</li>
                </ul>
            </div>
        </div>

        <!-- Navigation -->
        <div class="chapter-nav">
            <a href="#" class="prev disabled">← 上一章</a>
            <a href="index.html" class="home">📑 目錄</a>
            <a href="02-related-work.html" class="next">下一章：相關工作 →</a>
        </div>
    </div>
</body>
</html>
