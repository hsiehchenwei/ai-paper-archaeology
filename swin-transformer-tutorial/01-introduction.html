<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å¼•è¨€èˆ‡æ‘˜è¦ - Swin Transformer æ·±åº¦è§£æ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <!-- Hero Section -->
        <div class="hero-section" style="background-image: url('images/generated/hero_01_introduction.png');">
            <div class="hero-overlay"></div>
            <div class="hero-content">
                <h1>å±¤æ¬¡åŒ–è¦–è¦º Transformer çš„èª•ç”Ÿ</h1>
                <p class="hero-subtitle">å¾ ViT çš„å±€é™åˆ° Swin çš„çªç ´</p>
                <div class="hero-meta">Swin Transformer æ·±åº¦è§£æ Â· ç¬¬ 1 ç« </div>
            </div>
        </div>

        <!-- Breadcrumb -->
        <nav class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <a href="index.html">Swin Transformer æ·±åº¦è§£æ</a>
            <span>/</span>
            <span class="current">ç¬¬ 1 ç« ï¼šå¼•è¨€èˆ‡æ‘˜è¦</span>
        </nav>

        <div class="story-container">
            <!-- Drop Cap é–‹å ´ -->
            <p class="drop-cap">
                2021 å¹´ 3 æœˆï¼ŒMicrosoft Research Asia çš„ç ”ç©¶åœ˜éšŠæå‡ºäº†ä¸€å€‹é©å‘½æ€§çš„æƒ³æ³•ï¼šå¦‚æœæˆ‘å€‘èƒ½è®“ Transformer åœ¨è¦–è¦ºé ˜åŸŸåƒåœ¨ NLP é ˜åŸŸä¸€æ¨£å¼·å¤§ï¼Œæœƒç™¼ç”Ÿä»€éº¼ï¼Ÿé€™å€‹æƒ³æ³•å‚¬ç”Ÿäº† Swin Transformerâ€”â€”ä¸€å€‹èƒ½å¤ ä½œç‚ºé€šç”¨è¦–è¦ºéª¨å¹¹ç¶²è·¯çš„å±¤æ¬¡åŒ– Transformerã€‚èˆ‡ ViT ä¸åŒï¼ŒSwin Transformer ä¸åƒ…è§£æ±ºäº†å–®ä¸€è§£æåº¦çš„å•é¡Œï¼Œæ›´å¯¦ç¾äº†ç·šæ€§è¨ˆç®—è¤‡é›œåº¦ï¼Œè®“ Transformer çœŸæ­£æˆç‚ºè¦–è¦ºä»»å‹™çš„é€šç”¨è§£æ±ºæ–¹æ¡ˆã€‚
            </p>

            <div class="section-divider"><span>âœ¦</span></div>

            <!-- è«–æ–‡æ‘˜è¦ -->
            <div class="original-quote">
                <div class="icon">ğŸ“„</div>
                <div class="content">
                    <div class="en">
                        <strong>è«–æ–‡åŸæ–‡ï¼ˆAbstractï¼‰</strong><br><br>
                        This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with <strong>S</strong>hifted <strong>win</strong>dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size.
                    </div>
                    <div class="zh">
                        <strong>ä¸­æ–‡ç¿»è­¯</strong><br><br>
                        æœ¬è«–æ–‡æå‡ºäº†ä¸€å€‹æ–°çš„è¦–è¦º Transformerï¼Œç¨±ç‚º Swin Transformerï¼Œèƒ½å¤ ä½œç‚ºé›»è…¦è¦–è¦ºçš„é€šç”¨éª¨å¹¹ç¶²è·¯ã€‚å°‡ Transformer å¾èªè¨€é ˜åŸŸé©é…åˆ°è¦–è¦ºé ˜åŸŸçš„æŒ‘æˆ°æºæ–¼å…©å€‹é ˜åŸŸä¹‹é–“çš„å·®ç•°ï¼Œä¾‹å¦‚è¦–è¦ºå¯¦é«”å°ºåº¦çš„å·¨å¤§è®ŠåŒ–ï¼Œä»¥åŠåœ–åƒä¸­åƒç´ ç›¸è¼ƒæ–¼æ–‡å­—ä¸­è©å½™çš„é«˜è§£æåº¦ã€‚ç‚ºäº†è§£æ±ºé€™äº›å·®ç•°ï¼Œæˆ‘å€‘æå‡ºäº†ä¸€å€‹å±¤æ¬¡åŒ– Transformerï¼Œå…¶è¡¨ç¤ºæ˜¯é€šé<strong>ç§»ä½è¦–çª—</strong>ï¼ˆShifted Windowsï¼‰è¨ˆç®—çš„ã€‚ç§»ä½è¦–çª—æ–¹æ¡ˆé€šéå°‡è‡ªæ³¨æ„åŠ›è¨ˆç®—é™åˆ¶åœ¨éé‡ç–Šçš„å±€éƒ¨è¦–çª—å…§ï¼ŒåŒæ™‚å…è¨±è·¨è¦–çª—é€£æ¥ï¼Œå¸¶ä¾†äº†æ›´é«˜çš„æ•ˆç‡ã€‚é€™ç¨®å±¤æ¬¡åŒ–æ¶æ§‹å…·æœ‰åœ¨ä¸åŒå°ºåº¦å»ºæ¨¡çš„éˆæ´»æ€§ï¼Œä¸¦ä¸”ç›¸å°æ–¼åœ–åƒå¤§å°å…·æœ‰ç·šæ€§è¨ˆç®—è¤‡é›œåº¦ã€‚
                    </div>
                </div>
            </div>

            <!-- AI ç”Ÿæˆæ¦‚å¿µåœ– -->
            <div class="figure figure-ai">
                <img src="images/generated/concept_hierarchical_01.png" alt="å±¤æ¬¡åŒ–æ¶æ§‹æ¦‚å¿µåœ–">
                <div class="caption">
                    ğŸ’¡ <strong>AI åœ–è§£ï¼š</strong>å±¤æ¬¡åŒ–ç‰¹å¾µåœ–æ§‹å»ºâ€”â€”å¾å°çš„ patch é€æ­¥åˆä½µæˆå¤§çš„ç‰¹å¾µåœ–ï¼Œå°±åƒä¿„ç¾…æ–¯å¥—å¨ƒä¸€æ¨£
                </div>
            </div>

            <h2>ViT çš„å±€é™æ€§</h2>

            <p>
                åœ¨ Swin Transformer å‡ºç¾ä¹‹å‰ï¼ŒVision Transformer (ViT) å·²ç¶“è­‰æ˜äº† Transformer å¯ä»¥åœ¨è¦–è¦ºä»»å‹™ä¸Šå–å¾—å„ªç§€çš„è¡¨ç¾ã€‚ç„¶è€Œï¼ŒViT æœ‰å…©å€‹é—œéµçš„å±€é™æ€§ï¼š
            </p>

            <div class="key-concept">
                <h4>âš ï¸ ViT çš„å…©å€‹æ ¸å¿ƒå•é¡Œ</h4>
                <ul>
                    <li><strong>å–®ä¸€è§£æåº¦</strong>ï¼šViT åªç”¢ç”Ÿå–®ä¸€ä½è§£æåº¦çš„ç‰¹å¾µåœ–ï¼Œç„¡æ³•åƒ CNN é‚£æ¨£æ§‹å»ºå¤šå°ºåº¦ç‰¹å¾µé‡‘å­—å¡”</li>
                    <li><strong>äºŒæ¬¡è¤‡é›œåº¦</strong>ï¼šå…¨å±€è‡ªæ³¨æ„åŠ›çš„è¨ˆç®—è¤‡é›œåº¦æ˜¯ O(nÂ²)ï¼Œå°æ–¼é«˜è§£æåº¦åœ–åƒä¾†èªªè¨ˆç®—æˆæœ¬éé«˜</li>
                </ul>
            </div>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        In existing Transformer-based models, tokens are all of a fixed scale, a property unsuitable for these vision applications. Another difference is the much higher resolution of pixels in images compared to words in passages of text. There exist many vision tasks such as semantic segmentation that require dense prediction at the pixel level, and this would be intractable for Transformer on high-resolution images, as the computational complexity of its self-attention is quadratic to image size.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        åœ¨ç¾æœ‰çš„åŸºæ–¼ Transformer çš„æ¨¡å‹ä¸­ï¼Œtoken éƒ½æ˜¯å›ºå®šå°ºåº¦çš„ï¼Œé€™å€‹ç‰¹æ€§ä¸é©åˆé€™äº›è¦–è¦ºæ‡‰ç”¨ã€‚å¦ä¸€å€‹å·®ç•°æ˜¯åœ–åƒä¸­åƒç´ çš„è§£æåº¦é é«˜æ–¼æ–‡å­—æ®µè½ä¸­çš„è©å½™ã€‚å­˜åœ¨è¨±å¤šè¦–è¦ºä»»å‹™ï¼Œå¦‚èªç¾©åˆ†å‰²ï¼Œéœ€è¦åœ¨åƒç´ ç´šåˆ¥é€²è¡Œå¯†é›†é æ¸¬ï¼Œé€™å°æ–¼ Transformer åœ¨é«˜è§£æåº¦åœ–åƒä¸Šä¾†èªªæ˜¯é›£ä»¥è™•ç†çš„ï¼Œå› ç‚ºå…¶è‡ªæ³¨æ„åŠ›çš„è¨ˆç®—è¤‡é›œåº¦ç›¸å°æ–¼åœ–åƒå¤§å°æ˜¯äºŒæ¬¡çš„ã€‚
                    </div>
                </div>
            </div>

            <!-- åŸå§‹åœ–ç‰‡å±•ç¤º -->
            <div class="figure figure-original">
                <img src="images/original/teaser11.png" alt="Swin vs ViT å°æ¯”åœ–">
                <div class="caption">
                    <strong>Figure 1:</strong> Swin Transformer vs ViT æ¶æ§‹å°æ¯”ï¼ˆè«–æ–‡åŸåœ–ï¼‰
                </div>
                <div class="explanation">
                    <h4>ğŸ–¼ï¸ åŸæ–‡åœ–è¡¨è§£æ</h4>
                    <p>
                        é€™å¼µåœ–æ¸…æ™°åœ°å±•ç¤ºäº† Swin Transformer èˆ‡ ViT çš„é—œéµå·®ç•°ï¼š
                    </p>
                    <ul>
                        <li><strong>å·¦åœ– (Swin)</strong>ï¼šæ§‹å»ºå±¤æ¬¡åŒ–ç‰¹å¾µåœ–ï¼Œå¾å°çš„ patchï¼ˆç°è‰²ï¼‰é€æ­¥åˆä½µï¼Œåœ¨æ¯å€‹å±€éƒ¨è¦–çª—ï¼ˆç´…è‰²ï¼‰å…§è¨ˆç®—è‡ªæ³¨æ„åŠ›ï¼Œå¯¦ç¾ç·šæ€§è¤‡é›œåº¦</li>
                        <li><strong>å³åœ– (ViT)</strong>ï¼šåªç”¢ç”Ÿå–®ä¸€ä½è§£æåº¦çš„ç‰¹å¾µåœ–ï¼Œå…¨å±€è‡ªæ³¨æ„åŠ›å°è‡´äºŒæ¬¡è¤‡é›œåº¦</li>
                    </ul>
                    <p>
                        Swin Transformer çš„è¨­è¨ˆè®“å®ƒèƒ½å¤ ä½œç‚ºé€šç”¨éª¨å¹¹ç¶²è·¯ï¼ŒåŒæ™‚é©ç”¨æ–¼åœ–åƒåˆ†é¡å’Œå¯†é›†è­˜åˆ¥ä»»å‹™ï¼ˆå¦‚ç›®æ¨™æª¢æ¸¬å’Œèªç¾©åˆ†å‰²ï¼‰ã€‚
                    </p>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>Swin Transformer çš„è§£æ±ºæ–¹æ¡ˆ</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        To overcome these issues, we propose a general-purpose Transformer backbone, called Swin Transformer, which constructs hierarchical feature maps and has linear computational complexity to image size. As illustrated in Figure 1(a), Swin Transformer constructs a hierarchical representation by starting from small-sized patches (outlined in gray) and gradually merging neighboring patches in deeper Transformer layers. With these hierarchical feature maps, the Swin Transformer model can conveniently leverage advanced techniques for dense prediction such as feature pyramid networks (FPN) or U-Net. The linear computational complexity is achieved by computing self-attention locally within non-overlapping windows that partition an image (outlined in red). The number of patches in each window is fixed, and thus the complexity becomes linear to image size.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        ç‚ºäº†è§£æ±ºé€™äº›å•é¡Œï¼Œæˆ‘å€‘æå‡ºäº†ä¸€å€‹é€šç”¨ Transformer éª¨å¹¹ç¶²è·¯ï¼Œç¨±ç‚º Swin Transformerï¼Œå®ƒæ§‹å»ºå±¤æ¬¡åŒ–ç‰¹å¾µåœ–ï¼Œä¸¦ä¸”ç›¸å°æ–¼åœ–åƒå¤§å°å…·æœ‰ç·šæ€§è¨ˆç®—è¤‡é›œåº¦ã€‚å¦‚åœ– 1(a) æ‰€ç¤ºï¼ŒSwin Transformer é€šéå¾å°çš„ patchï¼ˆç°è‰²è¼ªå»“ï¼‰é–‹å§‹ï¼Œåœ¨æ›´æ·±çš„ Transformer å±¤ä¸­é€æ¼¸åˆä½µç›¸é„°çš„ patch ä¾†æ§‹å»ºå±¤æ¬¡åŒ–è¡¨ç¤ºã€‚æœ‰äº†é€™äº›å±¤æ¬¡åŒ–ç‰¹å¾µåœ–ï¼ŒSwin Transformer æ¨¡å‹å¯ä»¥æ–¹ä¾¿åœ°åˆ©ç”¨å…ˆé€²çš„å¯†é›†é æ¸¬æŠ€è¡“ï¼Œå¦‚ç‰¹å¾µé‡‘å­—å¡”ç¶²è·¯ï¼ˆFPNï¼‰æˆ– U-Netã€‚ç·šæ€§è¨ˆç®—è¤‡é›œåº¦æ˜¯é€šéåœ¨åŠƒåˆ†åœ–åƒçš„éé‡ç–Šè¦–çª—ï¼ˆç´…è‰²è¼ªå»“ï¼‰å…§å±€éƒ¨è¨ˆç®—è‡ªæ³¨æ„åŠ›ä¾†å¯¦ç¾çš„ã€‚æ¯å€‹è¦–çª—ä¸­çš„ patch æ•¸é‡æ˜¯å›ºå®šçš„ï¼Œå› æ­¤è¤‡é›œåº¦è®Šç‚ºç›¸å°æ–¼åœ–åƒå¤§å°çš„ç·šæ€§ã€‚
                    </div>
                </div>
            </div>

            <!-- é›™é‡é¡æ¯” -->
            <div class="analogy-section">
                <div class="analogy-card life">
                    <h4>ğŸ  ç”Ÿæ´»é¡æ¯”</h4>
                    <p>
                        å°±åƒä¿„ç¾…æ–¯å¥—å¨ƒä¸€æ¨£ï¼ŒSwin Transformer å¾æœ€å°çš„ã€Œå¨ƒå¨ƒã€ï¼ˆ4Ã—4 çš„ patchï¼‰é–‹å§‹ï¼Œä¸€å±¤ä¸€å±¤åœ°åˆä½µï¼Œæœ€çµ‚å½¢æˆä¸€å€‹å®Œæ•´çš„ã€Œå¤§å¨ƒå¨ƒã€ï¼ˆæ•´å€‹åœ–åƒçš„ç‰¹å¾µè¡¨ç¤ºï¼‰ã€‚æ¯ä¸€å±¤éƒ½èƒ½çœ‹åˆ°ä¸åŒå°ºåº¦çš„è³‡è¨Šï¼Œå°±åƒå¾è¿‘è·é›¢çœ‹ç´°ç¯€ï¼Œåˆ°é è·é›¢çœ‹æ•´é«”ä¸€æ¨£ã€‚
                    </p>
                </div>
                <div class="analogy-card engineering">
                    <h4>âš™ï¸ å·¥ç¨‹é¡æ¯”</h4>
                    <p>
                        åœ¨ç³»çµ±æ¶æ§‹ä¸Šï¼ŒSwin Transformer é¡ä¼¼æ–¼å¤šè§£æåº¦çš„é‡‘å­—å¡”çµæ§‹ã€‚å°±åƒåœ°åœ–æ‡‰ç”¨ç¨‹å¼æœƒæä¾›ä¸åŒç¸®æ”¾ç´šåˆ¥çš„åœ°åœ–ï¼ˆè¡—é“ç´šã€åŸå¸‚ç´šã€åœ‹å®¶ç´šï¼‰ï¼ŒSwin Transformer ä¹Ÿæ§‹å»ºäº†å¤šå€‹è§£æåº¦çš„ç‰¹å¾µåœ–ï¼Œè®“æ¨¡å‹èƒ½å¤ åŒæ™‚è™•ç†ç´°ç¯€å’Œå…¨å±€è³‡è¨Šã€‚è€Œç§»ä½è¦–çª—æ©Ÿåˆ¶å°±åƒæ˜¯åœ¨ä¸åŒå±¤ç´šä¹‹é–“å»ºç«‹ã€Œæ©‹æ¨‘ã€ï¼Œè®“è³‡è¨Šèƒ½å¤ åœ¨ä¸åŒå°ºåº¦ä¹‹é–“æµå‹•ã€‚
                    </p>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>æ ¸å¿ƒå‰µæ–°ï¼šShifted Windows</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        A key design element of Swin Transformer is its <em>shift</em> of the window partition between consecutive self-attention layers, as illustrated in Figure 2. The shifted windows bridge the windows of the preceding layer, providing connections among them that significantly enhance modeling power. This strategy is also efficient in regards to real-world latency: all <em>query</em> patches within a window share the same <em>key</em> set, which facilitates memory access in hardware.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        Swin Transformer çš„ä¸€å€‹é—œéµè¨­è¨ˆå…ƒç´ æ˜¯åœ¨é€£çºŒçš„è‡ªæ³¨æ„åŠ›å±¤ä¹‹é–“å°è¦–çª—åˆ†å€é€²è¡Œ<em>ç§»ä½</em>ï¼Œå¦‚åœ– 2 æ‰€ç¤ºã€‚ç§»ä½çš„è¦–çª—æ©‹æ¥äº†å‰ä¸€å±¤çš„è¦–çª—ï¼Œåœ¨å®ƒå€‘ä¹‹é–“æä¾›é€£æ¥ï¼Œé¡¯è‘—å¢å¼·äº†å»ºæ¨¡èƒ½åŠ›ã€‚é€™å€‹ç­–ç•¥åœ¨å¯¦éš›å»¶é²æ–¹é¢ä¹Ÿå¾ˆé«˜æ•ˆï¼šè¦–çª—å…§çš„æ‰€æœ‰ <em>query</em> patch å…±äº«ç›¸åŒçš„ <em>key</em> é›†åˆï¼Œé€™æœ‰åˆ©æ–¼ç¡¬é«”ä¸­çš„è¨˜æ†¶é«”è¨ªå•ã€‚
                    </div>
                </div>
            </div>

            <!-- åŸå§‹åœ–ç‰‡å±•ç¤º -->
            <div class="figure figure-original">
                <img src="images/original/teaser_v4.png" alt="Shifted Window æ©Ÿåˆ¶ç¤ºæ„åœ–">
                <div class="caption">
                    <strong>Figure 2:</strong> Shifted Window æ©Ÿåˆ¶ç¤ºæ„åœ–ï¼ˆè«–æ–‡åŸåœ–ï¼‰
                </div>
                <div class="explanation">
                    <h4>ğŸ–¼ï¸ åŸæ–‡åœ–è¡¨è§£æ</h4>
                    <p>
                        é€™å¼µåœ–å±•ç¤ºäº† Swin Transformer çš„æ ¸å¿ƒå‰µæ–°â€”â€”ç§»ä½è¦–çª—æ©Ÿåˆ¶ï¼š
                    </p>
                    <ul>
                        <li><strong>å·¦åœ–ï¼ˆLayer lï¼‰</strong>ï¼šä½¿ç”¨å¸¸è¦è¦–çª—åˆ†å€ï¼Œå°‡ 8Ã—8 çš„ç‰¹å¾µåœ–å‡å‹»åŠƒåˆ†ç‚º 2Ã—2 å€‹ 4Ã—4 çš„è¦–çª—</li>
                        <li><strong>å³åœ–ï¼ˆLayer l+1ï¼‰</strong>ï¼šè¦–çª—åˆ†å€è¢«ç§»ä½ï¼Œæ–°çš„è¦–çª—è·¨è¶Šäº†å‰ä¸€å±¤è¦–çª—çš„é‚Šç•Œï¼Œå»ºç«‹äº†è·¨è¦–çª—é€£æ¥</li>
                    </ul>
                    <p>
                        é€™ç¨®è¨­è¨ˆæ—¢ä¿æŒäº†å±€éƒ¨è¨ˆç®—çš„æ•ˆç‡ï¼Œåˆé€šéç§»ä½å¯¦ç¾äº†è·¨è¦–çª—çš„è³‡è¨Šäº¤æµï¼Œæ˜¯ Swin Transformer æˆåŠŸçš„é—œéµã€‚
                    </p>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>å“è¶Šçš„å¯¦é©—çµæœ</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        The proposed Swin Transformer achieves strong performance on the recognition tasks of image classification, object detection and semantic segmentation. It outperforms the ViT / DeiT and ResNe(X)t models significantly with similar latency on the three tasks. Its 58.7 box AP and 51.1 mask AP on the COCO test-dev set surpass the previous state-of-the-art results by +2.7 box AP (Copy-paste without external data) and +2.6 mask AP (DetectoRS). On ADE20K semantic segmentation, it obtains 53.5 mIoU on the val set, an improvement of +3.2 mIoU over the previous state-of-the-art (SETR). It also achieves a top-1 accuracy of 87.3% on ImageNet-1K image classification.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        æå‡ºçš„ Swin Transformer åœ¨åœ–åƒåˆ†é¡ã€ç›®æ¨™æª¢æ¸¬å’Œèªç¾©åˆ†å‰²çš„è­˜åˆ¥ä»»å‹™ä¸Šå–å¾—äº†å¼·å‹çš„è¡¨ç¾ã€‚å®ƒåœ¨ä¸‰å€‹ä»»å‹™ä¸Šä»¥ç›¸ä¼¼çš„å»¶é²é¡¯è‘—è¶…è¶Šäº† ViT / DeiT å’Œ ResNe(X)t æ¨¡å‹ã€‚å®ƒåœ¨ COCO test-dev é›†ä¸Šå–å¾—äº† 58.7 box AP å’Œ 51.1 mask APï¼Œè¶…éäº†ä¹‹å‰çš„æœ€ä½³çµæœ +2.7 box APï¼ˆCopy-pasteï¼Œç„¡å¤–éƒ¨æ•¸æ“šï¼‰å’Œ +2.6 mask APï¼ˆDetectoRSï¼‰ã€‚åœ¨ ADE20K èªç¾©åˆ†å‰²ä¸Šï¼Œå®ƒåœ¨ val é›†ä¸Šå–å¾—äº† 53.5 mIoUï¼Œæ¯”ä¹‹å‰çš„æœ€ä½³çµæœï¼ˆSETRï¼‰æé«˜äº† +3.2 mIoUã€‚å®ƒé‚„åœ¨ ImageNet-1K åœ–åƒåˆ†é¡ä¸Šå–å¾—äº† 87.3% çš„ top-1 æº–ç¢ºç‡ã€‚
                    </div>
                </div>
            </div>

            <div class="paradigm-grid">
                <div class="paradigm-card">
                    <h4>ğŸ“Š ImageNet-1K</h4>
                    <p><strong>87.3%</strong> top-1 æº–ç¢ºç‡</p>
                    <p>è¶…è¶Š ViT å’Œ ResNet çš„è¡¨ç¾</p>
                </div>
                <div class="paradigm-card">
                    <h4>ğŸ¯ COCO ç›®æ¨™æª¢æ¸¬</h4>
                    <p><strong>58.7</strong> box AP</p>
                    <p><strong>51.1</strong> mask AP</p>
                    <p>è¶…è¶Šä¹‹å‰ SOTA +2.7/+2.6</p>
                </div>
                <div class="paradigm-card">
                    <h4>ğŸ–¼ï¸ ADE20K èªç¾©åˆ†å‰²</h4>
                    <p><strong>53.5</strong> mIoU</p>
                    <p>è¶…è¶Šä¹‹å‰ SOTA +3.2</p>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>çµ±ä¸€æ¶æ§‹çš„é¡˜æ™¯</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        It is our belief that a unified architecture across computer vision and natural language processing could benefit both fields, since it would facilitate joint modeling of visual and textual signals and the modeling knowledge from both domains can be more deeply shared. We hope that Swin Transformer's strong performance on various vision problems can drive this belief deeper in the community and encourage unified modeling of vision and language signals.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        æˆ‘å€‘ç›¸ä¿¡ï¼Œé›»è…¦è¦–è¦ºå’Œè‡ªç„¶èªè¨€è™•ç†ä¹‹é–“çš„çµ±ä¸€æ¶æ§‹å¯ä»¥è®“å…©å€‹é ˜åŸŸéƒ½å—ç›Šï¼Œå› ç‚ºå®ƒå°‡ä¿ƒé€²è¦–è¦ºå’Œæ–‡å­—ä¿¡è™Ÿçš„è¯åˆå»ºæ¨¡ï¼Œä¸¦ä¸”å…©å€‹é ˜åŸŸçš„å»ºæ¨¡çŸ¥è­˜å¯ä»¥æ›´æ·±å…¥åœ°å…±äº«ã€‚æˆ‘å€‘å¸Œæœ› Swin Transformer åœ¨å„ç¨®è¦–è¦ºå•é¡Œä¸Šçš„å¼·å‹è¡¨ç¾èƒ½å¤ åœ¨ç¤¾ç¾¤ä¸­æ·±åŒ–é€™ä¸€ä¿¡å¿µï¼Œä¸¦é¼“å‹µè¦–è¦ºå’Œèªè¨€ä¿¡è™Ÿçš„çµ±ä¸€å»ºæ¨¡ã€‚
                    </div>
                </div>
            </div>

            <!-- Quote Block é‡‘å¥ -->
            <div class="quote-block">
                ã€Œå±¤æ¬¡åŒ–æ¶æ§‹ + ç§»ä½è¦–çª— = ç·šæ€§è¤‡é›œåº¦çš„é€šç”¨è¦–è¦ºéª¨å¹¹ç¶²è·¯ã€
            </div>

            <!-- æœ¬ç« é‡é»å›é¡§ -->
            <div class="chapter-summary">
                <h3>ğŸ’¡ æœ¬ç« é‡é»</h3>
                <ul>
                    <li><strong>æ ¸å¿ƒå•é¡Œ</strong>ï¼šViT çš„å±€é™æ€§ï¼ˆå–®ä¸€è§£æåº¦ã€äºŒæ¬¡è¤‡é›œåº¦ï¼‰é˜»ç¤™äº† Transformer æˆç‚ºé€šç”¨è¦–è¦ºéª¨å¹¹ç¶²è·¯</li>
                    <li><strong>Swin çš„è§£æ±ºæ–¹æ¡ˆ</strong>ï¼šå±¤æ¬¡åŒ–æ¶æ§‹ + ç§»ä½è¦–çª—æ©Ÿåˆ¶ï¼Œå¯¦ç¾ç·šæ€§è¤‡é›œåº¦</li>
                    <li><strong>é—œéµå‰µæ–°</strong>ï¼šShifted Windows åœ¨ä¿æŒæ•ˆç‡çš„åŒæ™‚å»ºç«‹è·¨è¦–çª—é€£æ¥</li>
                    <li><strong>å“è¶Šè¡¨ç¾</strong>ï¼šåœ¨ ImageNetã€COCOã€ADE20K ä¸‰å€‹ä»»å‹™ä¸Šéƒ½é”åˆ° SOTA</li>
                    <li><strong>é¡˜æ™¯</strong>ï¼šçµ±ä¸€è¦–è¦ºå’Œèªè¨€çš„æ¶æ§‹ï¼Œä¿ƒé€²è·¨é ˜åŸŸçŸ¥è­˜å…±äº«</li>
                </ul>
            </div>
        </div>

        <!-- Navigation -->
        <div class="chapter-nav">
            <a href="#" class="prev disabled">â† ä¸Šä¸€ç« </a>
            <a href="index.html" class="home">ğŸ“‘ ç›®éŒ„</a>
            <a href="02-related-work.html" class="next">ä¸‹ä¸€ç« ï¼šç›¸é—œå·¥ä½œ â†’</a>
        </div>
    </div>
</body>
</html>
