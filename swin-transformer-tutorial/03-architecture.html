<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ•´é«”æ¶æ§‹ - Swin Transformer æ·±åº¦è§£æ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <!-- Hero Section -->
        <div class="hero-section" style="background-image: url('images/generated/hero_03_architecture.png');">
            <div class="hero-overlay"></div>
            <div class="hero-content">
                <h1>å±¤æ¬¡åŒ–æ¶æ§‹çš„è¨­è¨ˆæ™ºæ…§</h1>
                <p class="hero-subtitle">å¾ Patch åˆ°ç‰¹å¾µé‡‘å­—å¡”çš„æ§‹å»º</p>
                <div class="hero-meta">Swin Transformer æ·±åº¦è§£æ Â· ç¬¬ 3 ç« </div>
            </div>
        </div>

        <!-- Breadcrumb -->
        <nav class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <a href="index.html">Swin Transformer æ·±åº¦è§£æ</a>
            <span>/</span>
            <span class="current">ç¬¬ 3 ç« ï¼šæ•´é«”æ¶æ§‹</span>
        </nav>

        <div class="story-container">
            <!-- Drop Cap é–‹å ´ -->
            <p class="drop-cap">
                Swin Transformer çš„æ¶æ§‹è¨­è¨ˆé«”ç¾äº†å°è¦–è¦ºä»»å‹™æœ¬è³ªçš„æ·±åˆ»ç†è§£ã€‚å®ƒä¸åƒ ViT é‚£æ¨£å°‡åœ–åƒã€Œå£“å¹³ã€æˆå–®ä¸€åºåˆ—ï¼Œè€Œæ˜¯åƒ CNN ä¸€æ¨£æ§‹å»ºå¤šå°ºåº¦çš„ç‰¹å¾µé‡‘å­—å¡”ã€‚å¾ 4Ã—4 çš„ patch é–‹å§‹ï¼Œé€šéå››å€‹ Stage çš„å±¤æ¬¡åŒ–è™•ç†ï¼Œé€æ­¥æ§‹å»ºå‡ºå¾ç´°ç¯€åˆ°å…¨å±€çš„å®Œæ•´ç‰¹å¾µè¡¨ç¤ºã€‚é€™ç¨®è¨­è¨ˆè®“ Swin Transformer èƒ½å¤ åŒæ™‚è™•ç†åœ–åƒåˆ†é¡å’Œå¯†é›†é æ¸¬ä»»å‹™ï¼ŒçœŸæ­£æˆç‚ºé€šç”¨çš„è¦–è¦ºéª¨å¹¹ç¶²è·¯ã€‚
            </p>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>æ•´é«”æ¶æ§‹æ¦‚è¦½</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        An overview of the Swin Transformer architecture is presented in Figure 3, which illustrates the tiny version (Swin-T). It first splits an input RGB image into non-overlapping patches by a patch splitting module, like ViT. Each patch is treated as a ``token'' and its feature is set as a concatenation of the raw pixel RGB values. In our implementation, we use a patch size of $4\times 4$ and thus the feature dimension of each patch is $4\times4\times 3=48$. A linear embedding layer is applied on this raw-valued feature to project it to an arbitrary dimension (denoted as $C$).
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        Swin Transformer æ¶æ§‹çš„æ¦‚è¦½å¦‚åœ– 3 æ‰€ç¤ºï¼Œå±•ç¤ºäº† tiny ç‰ˆæœ¬ï¼ˆSwin-Tï¼‰ã€‚å®ƒé¦–å…ˆé€šé patch splitting æ¨¡çµ„å°‡è¼¸å…¥çš„ RGB åœ–åƒåˆ†å‰²æˆéé‡ç–Šçš„ patchï¼Œå°±åƒ ViT ä¸€æ¨£ã€‚æ¯å€‹ patch è¢«è¦–ç‚ºä¸€å€‹ã€Œtokenã€ï¼Œå…¶ç‰¹å¾µè¢«è¨­ç½®ç‚ºåŸå§‹åƒç´  RGB å€¼çš„ä¸²è¯ã€‚åœ¨æˆ‘å€‘çš„å¯¦ç¾ä¸­ï¼Œæˆ‘å€‘ä½¿ç”¨ $4\times 4$ çš„ patch å¤§å°ï¼Œå› æ­¤æ¯å€‹ patch çš„ç‰¹å¾µç¶­åº¦æ˜¯ $4\times4\times 3=48$ã€‚ä¸€å€‹ç·šæ€§åµŒå…¥å±¤æ‡‰ç”¨æ–¼é€™å€‹åŸå§‹å€¼ç‰¹å¾µï¼Œå°‡å…¶æŠ•å½±åˆ°ä»»æ„ç¶­åº¦ï¼ˆè¨˜ç‚º $C$ï¼‰ã€‚
                    </div>
                </div>
            </div>

            <!-- åŸå§‹åœ–ç‰‡å±•ç¤º -->
            <div class="figure figure-original">
                <img src="images/original/HiT-arch-v2.png" alt="Swin Transformer å®Œæ•´æ¶æ§‹åœ–">
                <div class="caption">
                    <strong>Figure 3:</strong> Swin Transformer æ¶æ§‹åœ–ï¼ˆè«–æ–‡åŸåœ–ï¼‰
                </div>
                <div class="explanation">
                    <h4>ğŸ–¼ï¸ åŸæ–‡åœ–è¡¨è§£æ</h4>
                    <p>
                        é€™å¼µæ¶æ§‹åœ–å±•ç¤ºäº† Swin Transformer çš„å®Œæ•´è¨­è¨ˆï¼š
                    </p>
                    <ul>
                        <li><strong>åœ– (a)</strong>ï¼šæ•´é«”æ¶æ§‹ï¼Œå±•ç¤ºäº†å¾è¼¸å…¥åœ–åƒåˆ°å››å€‹ Stage çš„å±¤æ¬¡åŒ–è™•ç†æµç¨‹</li>
                        <li><strong>åœ– (b)</strong>ï¼šå…©å€‹é€£çºŒçš„ Swin Transformer Blockï¼Œå±•ç¤ºäº† W-MSA å’Œ SW-MSA çš„äº¤æ›¿ä½¿ç”¨</li>
                    </ul>
                    <p>
                        æ¶æ§‹çš„æ ¸å¿ƒæ˜¯é€šé Patch Merging é€æ­¥é™ä½è§£æåº¦ï¼ŒåŒæ™‚å¢åŠ ç‰¹å¾µç¶­åº¦ï¼Œæ§‹å»ºå‡ºé¡ä¼¼ CNN çš„å¤šå°ºåº¦ç‰¹å¾µé‡‘å­—å¡”ã€‚
                    </p>
                </div>
            </div>

            <!-- AI ç”Ÿæˆæ¦‚å¿µåœ– -->
            <div class="figure figure-ai">
                <img src="images/generated/concept_hierarchical_01.png" alt="å±¤æ¬¡åŒ–æ¶æ§‹æ¦‚å¿µåœ–">
                <div class="caption">
                    ğŸ’¡ <strong>AI åœ–è§£ï¼š</strong>å±¤æ¬¡åŒ–ç‰¹å¾µåœ–æ§‹å»ºâ€”â€”å¾ Stage 1 åˆ° Stage 4ï¼Œè§£æåº¦é€æ­¥é™ä½ï¼Œç‰¹å¾µé€æ­¥æŠ½è±¡
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>å››å€‹ Stage çš„å±¤æ¬¡åŒ–è¨­è¨ˆ</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Several Transformer blocks with modified self-attention computation (Swin Transformer blocks) are applied on these patch tokens. The Transformer blocks maintain the number of tokens ($\frac{H}{4} \times \frac{W}{4}$), and together with the linear embedding are referred to as ``Stage 1''.
                        
                        To produce a hierarchical representation, the number of tokens is reduced by patch merging layers as the network gets deeper. The first patch merging layer concatenates the features of each group of $2\times 2$ neighboring patches, and applies a linear layer on the $4C$-dimensional concatenated features. This reduces the number of tokens by a multiple of $2\times2=4$ ($2\times$ downsampling of resolution), and the output dimension is set to $2C$. Swin Transformer blocks are applied afterwards for feature transformation, with the resolution kept at $\frac{H}{8} \times \frac{W}{8}$. This first block of patch merging and feature transformation is denoted as ``Stage 2''. The procedure is repeated twice, as ``Stage 3'' and ``Stage 4'', with output resolutions of $\frac{H}{16} \times \frac{W}{16}$ and $\frac{H}{32} \times \frac{W}{32}$, respectively.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        å¹¾å€‹å…·æœ‰ä¿®æ”¹å¾Œè‡ªæ³¨æ„åŠ›è¨ˆç®—çš„ Transformer blockï¼ˆSwin Transformer blocksï¼‰æ‡‰ç”¨æ–¼é€™äº› patch tokenã€‚Transformer blocks ä¿æŒ token æ•¸é‡ï¼ˆ$\frac{H}{4} \times \frac{W}{4}$ï¼‰ï¼Œèˆ‡ç·šæ€§åµŒå…¥ä¸€èµ·ç¨±ç‚ºã€ŒStage 1ã€ã€‚
                        
                        ç‚ºäº†ç”¢ç”Ÿå±¤æ¬¡åŒ–è¡¨ç¤ºï¼Œéš¨è‘—ç¶²è·¯è®Šæ·±ï¼Œtoken æ•¸é‡é€šé patch merging å±¤æ¸›å°‘ã€‚ç¬¬ä¸€å€‹ patch merging å±¤ä¸²è¯æ¯çµ„ $2\times 2$ ç›¸é„° patch çš„ç‰¹å¾µï¼Œä¸¦åœ¨ $4C$ ç¶­ä¸²è¯ç‰¹å¾µä¸Šæ‡‰ç”¨ç·šæ€§å±¤ã€‚é€™å°‡ token æ•¸é‡æ¸›å°‘ $2\times2=4$ å€ï¼ˆè§£æåº¦çš„ $2\times$ ä¸‹æ¡æ¨£ï¼‰ï¼Œè¼¸å‡ºç¶­åº¦è¨­ç½®ç‚º $2C$ã€‚ä¹‹å¾Œæ‡‰ç”¨ Swin Transformer blocks é€²è¡Œç‰¹å¾µè½‰æ›ï¼Œè§£æåº¦ä¿æŒåœ¨ $\frac{H}{8} \times \frac{W}{8}$ã€‚é€™å€‹ patch merging å’Œç‰¹å¾µè½‰æ›çš„ç¬¬ä¸€å€‹ block ç¨±ç‚ºã€ŒStage 2ã€ã€‚è©²éç¨‹é‡è¤‡å…©æ¬¡ï¼Œä½œç‚ºã€ŒStage 3ã€å’Œã€ŒStage 4ã€ï¼Œè¼¸å‡ºè§£æåº¦åˆ†åˆ¥ç‚º $\frac{H}{16} \times \frac{W}{16}$ å’Œ $\frac{H}{32} \times \frac{W}{32}$ã€‚
                    </div>
                </div>
            </div>

            <div class="paradigm-grid">
                <div class="paradigm-card">
                    <h4>Stage 1</h4>
                    <p><strong>è§£æåº¦ï¼š</strong> $\frac{H}{4} \times \frac{W}{4}$</p>
                    <p><strong>æ“ä½œï¼š</strong> Patch Splitting + Linear Embedding + Swin Blocks</p>
                    <p>å°‡åœ–åƒåˆ†å‰²æˆ 4Ã—4 çš„ patchï¼Œæ¯å€‹ patch æˆç‚ºä¸€å€‹ token</p>
                </div>
                <div class="paradigm-card">
                    <h4>Stage 2</h4>
                    <p><strong>è§£æåº¦ï¼š</strong> $\frac{H}{8} \times \frac{W}{8}$</p>
                    <p><strong>æ“ä½œï¼š</strong> Patch Merging (2Ã—2) + Swin Blocks</p>
                    <p>å°‡ 2Ã—2 çš„ patch åˆä½µï¼Œè§£æåº¦é™ä½ 2 å€ï¼Œç¶­åº¦å¢åŠ  2 å€</p>
                </div>
                <div class="paradigm-card">
                    <h4>Stage 3</h4>
                    <p><strong>è§£æåº¦ï¼š</strong> $\frac{H}{16} \times \frac{W}{16}$</p>
                    <p><strong>æ“ä½œï¼š</strong> Patch Merging (2Ã—2) + Swin Blocks</p>
                    <p>ç¹¼çºŒåˆä½µï¼Œæ§‹å»ºä¸­ç­‰å°ºåº¦çš„ç‰¹å¾µè¡¨ç¤º</p>
                </div>
                <div class="paradigm-card">
                    <h4>Stage 4</h4>
                    <p><strong>è§£æåº¦ï¼š</strong> $\frac{H}{32} \times \frac{W}{32}$</p>
                    <p><strong>æ“ä½œï¼š</strong> Patch Merging (2Ã—2) + Swin Blocks</p>
                    <p>æœ€çµ‚çš„é«˜å±¤èªç¾©ç‰¹å¾µï¼Œé¡ä¼¼æ–¼ CNN çš„æœ€å¾Œä¸€å±¤</p>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>Swin Transformer Block</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Swin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows (described in Section 4), with other layers kept the same. As illustrated in Figure 3(b), a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU non-linearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        Swin Transformer æ˜¯é€šéå°‡ Transformer block ä¸­çš„æ¨™æº–å¤šé ­è‡ªæ³¨æ„åŠ›ï¼ˆMSAï¼‰æ¨¡çµ„æ›¿æ›ç‚ºåŸºæ–¼ç§»ä½è¦–çª—çš„æ¨¡çµ„ï¼ˆåœ¨ç¬¬ 4 ç¯€ä¸­æè¿°ï¼‰è€Œæ§‹å»ºçš„ï¼Œå…¶ä»–å±¤ä¿æŒä¸è®Šã€‚å¦‚åœ– 3(b) æ‰€ç¤ºï¼ŒSwin Transformer block ç”±ä¸€å€‹åŸºæ–¼ç§»ä½è¦–çª—çš„ MSA æ¨¡çµ„çµ„æˆï¼Œå¾Œé¢æ˜¯ä¸€å€‹ 2 å±¤ MLPï¼Œä¸­é–“æœ‰ GELU éç·šæ€§ã€‚åœ¨æ¯å€‹ MSA æ¨¡çµ„å’Œæ¯å€‹ MLP ä¹‹å‰æ‡‰ç”¨ LayerNorm (LN) å±¤ï¼Œåœ¨æ¯å€‹æ¨¡çµ„ä¹‹å¾Œæ‡‰ç”¨æ®˜å·®é€£æ¥ã€‚
                    </div>
                </div>
            </div>

            <!-- æ¦‚å¿µå‹•ç•« -->
            <div class="video-container">
                <video autoplay loop muted playsinline>
                    <source src="images/videos/hierarchical_architecture.mp4" type="video/mp4">
                </video>
                <div class="caption">ğŸ¬ å±¤æ¬¡åŒ–æ¶æ§‹æ§‹å»ºå‹•ç•«ï¼šå±•ç¤ºå¾ Stage 1 åˆ° Stage 4 çš„ç‰¹å¾µåœ–æ¼”è®Šéç¨‹</div>
            </div>

            <!-- é›™é‡é¡æ¯” -->
            <div class="analogy-section">
                <div class="analogy-card life">
                    <h4>ğŸ  ç”Ÿæ´»é¡æ¯”</h4>
                    <p>
                        å°±åƒåŸå¸‚è¦åŠƒä¸€æ¨£ï¼šStage 1 æ˜¯è¡—é“ç´šåˆ¥ï¼Œå¯ä»¥çœ‹åˆ°æ¯å€‹å»ºç¯‰ç‰©çš„ç´°ç¯€ï¼ˆ4Ã—4 patchï¼‰ï¼›Stage 2 æ˜¯ç¤¾å€ç´šåˆ¥ï¼Œå¯ä»¥çœ‹åˆ°å¹¾å€‹å»ºç¯‰ç‰©çµ„æˆçš„è¡—å€ï¼›Stage 3 æ˜¯å€åŸŸç´šåˆ¥ï¼Œå¯ä»¥çœ‹åˆ°æ•´å€‹ç¤¾å€çš„ä½ˆå±€ï¼›Stage 4 æ˜¯åŸå¸‚ç´šåˆ¥ï¼Œå¯ä»¥çœ‹åˆ°æ•´å€‹åŸå¸‚çš„å®è§€çµæ§‹ã€‚æ¯ä¸€å±¤éƒ½æä¾›äº†ä¸åŒå°ºåº¦çš„è¦–è§’ï¼Œè®“æ¨¡å‹èƒ½å¤ åŒæ™‚ç†è§£ç´°ç¯€å’Œå…¨å±€ã€‚
                    </p>
                </div>
                <div class="analogy-card engineering">
                    <h4>âš™ï¸ å·¥ç¨‹é¡æ¯”</h4>
                    <p>
                        åœ¨ç³»çµ±æ¶æ§‹ä¸Šï¼ŒSwin Transformer é¡ä¼¼æ–¼å¾®æœå‹™æ¶æ§‹ä¸­çš„æœå‹™å±¤ç´šï¼šStage 1 æ˜¯åŸºç¤æœå‹™å±¤ï¼Œè™•ç†åŸå§‹æ•¸æ“šï¼›Stage 2-4 æ˜¯æ¥­å‹™é‚è¼¯å±¤ï¼Œé€æ­¥æŠ½è±¡å’Œèšåˆè³‡è¨Šã€‚æ¯ä¸€å±¤éƒ½å¯ä»¥ç¨ç«‹å„ªåŒ–å’Œæ“´å±•ï¼ŒåŒæ™‚é€šéæ¨™æº–åŒ–çš„æ¥å£ï¼ˆpatch mergingï¼‰é€²è¡Œé€šä¿¡ã€‚é€™ç¨®è¨­è¨ˆæ—¢ä¿æŒäº†æ¨¡çµ„åŒ–ï¼Œåˆå¯¦ç¾äº†é«˜æ•ˆçš„è³‡è¨Šæµå‹•ã€‚
                    </p>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>èˆ‡ CNN çš„å°æ‡‰é—œä¿‚</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        These stages jointly produce a hierarchical representation, with the same feature map resolutions as those of typical convolutional networks, e.g., VGG and ResNet. As a result, the proposed architecture can conveniently replace the backbone networks in existing methods for various vision tasks.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        é€™äº› stage å…±åŒç”¢ç”Ÿå±¤æ¬¡åŒ–è¡¨ç¤ºï¼Œå…·æœ‰èˆ‡å…¸å‹å·ç©ç¶²è·¯ï¼ˆä¾‹å¦‚ VGG å’Œ ResNetï¼‰ç›¸åŒçš„ç‰¹å¾µåœ–è§£æåº¦ã€‚å› æ­¤ï¼Œæå‡ºçš„æ¶æ§‹å¯ä»¥æ–¹ä¾¿åœ°æ›¿æ›ç¾æœ‰æ–¹æ³•ä¸­å„ç¨®è¦–è¦ºä»»å‹™çš„éª¨å¹¹ç¶²è·¯ã€‚
                    </div>
                </div>
            </div>

            <div class="key-concept">
                <h4>ğŸ’¡ ç‚ºä»€éº¼å±¤æ¬¡åŒ–è¨­è¨ˆå¦‚æ­¤é‡è¦ï¼Ÿ</h4>
                <p>
                    å±¤æ¬¡åŒ–ç‰¹å¾µåœ–æ˜¯è¦–è¦ºä»»å‹™çš„åŸºç¤ã€‚ç„¡è«–æ˜¯ç›®æ¨™æª¢æ¸¬ä¸­çš„ FPNï¼ˆFeature Pyramid Networkï¼‰ï¼Œé‚„æ˜¯èªç¾©åˆ†å‰²ä¸­çš„ U-Netï¼Œéƒ½éœ€è¦å¤šå°ºåº¦çš„ç‰¹å¾µè¡¨ç¤ºã€‚Swin Transformer çš„å±¤æ¬¡åŒ–è¨­è¨ˆè®“å®ƒèƒ½å¤ ç„¡ç¸«æ›¿æ› CNN éª¨å¹¹ç¶²è·¯ï¼ŒåŒæ™‚åˆ©ç”¨ Transformer çš„å¼·å¤§å»ºæ¨¡èƒ½åŠ›ã€‚
                </p>
            </div>

            <!-- Quote Block é‡‘å¥ -->
            <div class="quote-block">
                ã€Œå±¤æ¬¡åŒ–æ¶æ§‹ + ç§»ä½è¦–çª— = CNN çš„æ­¸ç´åç½® + Transformer çš„å»ºæ¨¡èƒ½åŠ›ã€
            </div>

            <!-- æœ¬ç« é‡é»å›é¡§ -->
            <div class="chapter-summary">
                <h3>ğŸ’¡ æœ¬ç« é‡é»</h3>
                <ul>
                    <li><strong>Patch Splitting</strong>ï¼šå°‡åœ–åƒåˆ†å‰²æˆ 4Ã—4 çš„ patchï¼Œæ¯å€‹ patch æˆç‚ºä¸€å€‹ token</li>
                    <li><strong>å››å€‹ Stage</strong>ï¼šé€šé Patch Merging é€æ­¥é™ä½è§£æåº¦ï¼ˆH/4 â†’ H/8 â†’ H/16 â†’ H/32ï¼‰</li>
                    <li><strong>å±¤æ¬¡åŒ–ç‰¹å¾µ</strong>ï¼šèˆ‡ CNNï¼ˆVGGã€ResNetï¼‰ç›¸åŒçš„ç‰¹å¾µåœ–è§£æåº¦ï¼Œå¯ç„¡ç¸«æ›¿æ›</li>
                    <li><strong>Swin Block</strong>ï¼šW-MSA å’Œ SW-MSA äº¤æ›¿ä½¿ç”¨ï¼Œä¿æŒæ•ˆç‡çš„åŒæ™‚å»ºç«‹è·¨è¦–çª—é€£æ¥</li>
                    <li><strong>é€šç”¨éª¨å¹¹</strong>ï¼šé©ç”¨æ–¼åœ–åƒåˆ†é¡ã€ç›®æ¨™æª¢æ¸¬ã€èªç¾©åˆ†å‰²ç­‰å¤šç¨®è¦–è¦ºä»»å‹™</li>
                </ul>
            </div>
        </div>

        <!-- Navigation -->
        <div class="chapter-nav">
            <a href="02-related-work.html" class="prev">â† ä¸Šä¸€ç« </a>
            <a href="index.html" class="home">ğŸ“‘ ç›®éŒ„</a>
            <a href="04-shifted-windows.html" class="next">ä¸‹ä¸€ç« ï¼šç§»ä½è¦–çª—æ©Ÿåˆ¶ â†’</a>
        </div>
    </div>
</body>
</html>
