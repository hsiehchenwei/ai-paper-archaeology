<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç›¸é—œå·¥ä½œ - Swin Transformer æ·±åº¦è§£æ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <!-- Hero Section -->
        <div class="hero-section" style="background-image: url('images/generated/hero_02_related_work.png');">
            <div class="hero-overlay"></div>
            <div class="hero-content">
                <h1>å¾ CNN åˆ° Transformer çš„æ¼”é€²</h1>
                <p class="hero-subtitle">è¦–è¦º AI æ¶æ§‹çš„æ­·å²è„ˆçµ¡</p>
                <div class="hero-meta">Swin Transformer æ·±åº¦è§£æ Â· ç¬¬ 2 ç« </div>
            </div>
        </div>

        <!-- Breadcrumb -->
        <nav class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <a href="index.html">Swin Transformer æ·±åº¦è§£æ</a>
            <span>/</span>
            <span class="current">ç¬¬ 2 ç« ï¼šç›¸é—œå·¥ä½œ</span>
        </nav>

        <div class="story-container">
            <!-- Drop Cap é–‹å ´ -->
            <p class="drop-cap">
                è¦ç†è§£ Swin Transformer çš„å‰µæ–°ï¼Œæˆ‘å€‘å¿…é ˆå…ˆå›é¡§è¦–è¦º AI çš„ç™¼å±•æ­·ç¨‹ã€‚å¾ 2012 å¹´ AlexNet é–‹å•Ÿæ·±åº¦å­¸ç¿’è¦–è¦ºæ™‚ä»£ï¼Œåˆ° 2020 å¹´ ViT è­‰æ˜ Transformer åœ¨è¦–è¦ºé ˜åŸŸçš„æ½›åŠ›ï¼Œå†åˆ° 2021 å¹´ Swin Transformer å¯¦ç¾çœŸæ­£çš„é€šç”¨è¦–è¦ºéª¨å¹¹ç¶²è·¯ï¼Œé€™æ˜¯ä¸€æ®µå¾å·ç©åˆ°æ³¨æ„åŠ›çš„æ¼”é€²å²ã€‚
            </p>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>CNN æ™‚ä»£çš„å·”å³°</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Beginning with AlexNet and its revolutionary performance on the ImageNet image classification challenge, CNN architectures have evolved to become increasingly powerful through greater scale, more extensive connections, and more sophisticated forms of convolution. With CNNs serving as backbone networks for a variety of vision tasks, these architectural advances have led to performance improvements that have broadly lifted the entire field.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        é›»è…¦è¦–è¦ºä¸­çš„å»ºæ¨¡é•·æœŸä»¥ä¾†ä¸€ç›´ç”±å·ç©ç¥ç¶“ç¶²è·¯ï¼ˆCNNï¼‰ä¸»å°ã€‚å¾ AlexNet åŠå…¶åœ¨ ImageNet åœ–åƒåˆ†é¡æŒ‘æˆ°ä¸­çš„é©å‘½æ€§è¡¨ç¾é–‹å§‹ï¼ŒCNN æ¶æ§‹é€šéæ›´å¤§çš„è¦æ¨¡ã€æ›´å»£æ³›çš„é€£æ¥å’Œæ›´è¤‡é›œçš„å·ç©å½¢å¼ï¼Œæ¼”é€²å¾—è¶Šä¾†è¶Šå¼·å¤§ã€‚éš¨è‘— CNN ä½œç‚ºå„ç¨®è¦–è¦ºä»»å‹™çš„éª¨å¹¹ç¶²è·¯ï¼Œé€™äº›æ¶æ§‹é€²æ­¥å¸¶ä¾†äº†æ€§èƒ½æå‡ï¼Œå»£æ³›æ¨å‹•äº†æ•´å€‹é ˜åŸŸçš„ç™¼å±•ã€‚
                    </div>
                </div>
            </div>

            <div class="key-concept">
                <h4>ğŸ“š CNN æ¼”é€²å²</h4>
                <ul>
                    <li><strong>AlexNet (2012)</strong>ï¼šé–‹å•Ÿæ·±åº¦å­¸ç¿’è¦–è¦ºæ™‚ä»£</li>
                    <li><strong>VGG (2014)</strong>ï¼šæ·±åº¦ç¶²è·¯è¨­è¨ˆåŸå‰‡</li>
                    <li><strong>ResNet (2015)</strong>ï¼šæ®˜å·®å­¸ç¿’ï¼Œè§£æ±ºæ·±åº¦ç¶²è·¯è¨“ç·´å•é¡Œ</li>
                    <li><strong>DenseNet (2017)</strong>ï¼šå¯†é›†é€£æ¥ï¼Œç‰¹å¾µé‡ç”¨</li>
                    <li><strong>EfficientNet (2019)</strong>ï¼šè¤‡åˆç¸®æ”¾ç­–ç•¥</li>
                </ul>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>Transformer åœ¨ NLP çš„æˆåŠŸ</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        On the other hand, the evolution of network architectures in natural language processing (NLP) has taken a different path, where the prevalent architecture today is instead the Transformer. Designed for sequence modeling and transduction tasks, the Transformer is notable for its use of attention to model long-range dependencies in the data. Its tremendous success in the language domain has led researchers to investigate its adaptation to computer vision, where it has recently demonstrated promising results on certain tasks, specifically image classification and joint vision-language modeling.
                    </div>
                    <div class="zh">
                        <strong>ğŸ“ ç™½è©±ç¿»è­¯</strong><br><br>
                        ä½† NLP é ˜åŸŸçš„ç™¼å±•è·¯ç·šå®Œå…¨ä¸åŒã€‚ç¾åœ¨ NLP çš„ä¸»æµæ¶æ§‹æ˜¯ <strong>Transformer</strong>ï¼Œè€Œä¸æ˜¯ CNNã€‚
                        <br><br>
                        <strong>Transformer çš„è¨­è¨ˆç†å¿µï¼š</strong>å®ƒåŸæœ¬æ˜¯ç‚ºäº†è™•ç†ã€Œåºåˆ—è³‡æ–™ã€ï¼ˆä¾‹å¦‚ï¼šä¸€æ®µæ–‡å­—ã€ä¸€å€‹å¥å­ï¼‰è€Œè¨­è¨ˆçš„ã€‚å®ƒçš„æ ¸å¿ƒç‰¹è‰²æ˜¯ä½¿ç”¨ã€Œæ³¨æ„åŠ›æ©Ÿåˆ¶ã€ä¾†æ•æ‰è³‡æ–™ä¸­ã€Œè·é›¢å¾ˆé ä½†ç›¸é—œã€çš„è³‡è¨Šï¼ˆé•·ç¨‹ä¾è³´é—œä¿‚ï¼‰ã€‚
                        <br><br>
                        <strong>ğŸ’¡ ä»€éº¼æ˜¯é•·ç¨‹ä¾è³´é—œä¿‚ï¼Ÿ</strong>ä¾‹å¦‚åœ¨å¥å­ã€Œæˆ‘æ˜¨å¤©åœ¨å’–å•¡å»³é‡åˆ°çš„é‚£å€‹æœ‹å‹ï¼Œä»–ä»Šå¤©æ‰“é›»è©±çµ¦æˆ‘ã€ä¸­ï¼Œã€Œæœ‹å‹ã€å’Œã€Œä»–ã€é›–ç„¶è·é›¢å¾ˆé ï¼Œä½†æŒ‡çš„æ˜¯åŒä¸€å€‹äººã€‚Transformer çš„æ³¨æ„åŠ›æ©Ÿåˆ¶å¯ä»¥æ•æ‰é€™ç¨®é è·é›¢çš„é—œè¯ã€‚
                        <br><br>
                        <strong>å¾ NLP åˆ°è¦–è¦ºï¼š</strong>Transformer åœ¨èªè¨€é ˜åŸŸçš„å·¨å¤§æˆåŠŸï¼ˆä¾‹å¦‚ï¼šBERTã€GPT éƒ½åŸºæ–¼ Transformerï¼‰ï¼Œè®“ç ”ç©¶äººå“¡é–‹å§‹æ€è€ƒï¼šèƒ½ä¸èƒ½æŠŠ Transformer ä¹Ÿæ‡‰ç”¨åˆ°è¦–è¦ºé ˜åŸŸï¼Ÿæœ€è¿‘çš„ç ”ç©¶é¡¯ç¤ºï¼ŒTransformer åœ¨è¦–è¦ºä»»å‹™ä¸Šç¢ºå¯¦æœ‰æ½›åŠ›ï¼Œç‰¹åˆ¥æ˜¯åœ¨ã€Œåœ–åƒåˆ†é¡ã€ï¼ˆåˆ¤æ–·åœ–ç‰‡æ˜¯ä»€éº¼ï¼‰å’Œã€Œè¦–è¦º-èªè¨€è¯åˆå»ºæ¨¡ã€ï¼ˆåŒæ™‚ç†è§£åœ–ç‰‡å’Œæ–‡å­—ï¼Œä¾‹å¦‚ï¼šçœ‹åœ–èªªæ•…äº‹ï¼‰é€™å…©å€‹ä»»å‹™ä¸Šã€‚
                    </div>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>ViT çš„é–‹å‰µèˆ‡å±€é™</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Most related to our work is the Vision Transformer (ViT) and its follow-ups. The pioneering work of ViT directly applies a Transformer architecture on non-overlapping medium-sized image patches for image classification. It achieves an impressive speed-accuracy trade-off on image classification compared to convolutional networks. While ViT requires large-scale training datasets (i.e., JFT-300M) to perform well, DeiT introduces several training strategies that allow ViT to also be effective using the smaller ImageNet-1K dataset. The results of ViT on image classification are encouraging, but its architecture is unsuitable for use as a general-purpose backbone network on dense vision tasks or when the input image resolution is high, due to its low-resolution feature maps and the quadratic increase in complexity with image size.
                    </div>
                    <div class="zh">
                        <strong>ğŸ“ ç™½è©±ç¿»è­¯</strong><br><br>
                        <strong>èˆ‡ Swin Transformer æœ€ç›¸é—œçš„å·¥ä½œï¼šViT</strong>
                        <br><br>
                        <strong>ViT çš„æ ¸å¿ƒåšæ³•ï¼š</strong>ViT æ˜¯ç¬¬ä¸€å€‹ã€Œç›´æ¥æŠŠ Transformer ç”¨åœ¨åœ–ç‰‡ä¸Šã€çš„é–‹å‰µæ€§å·¥ä½œã€‚å®ƒçš„åšæ³•å¾ˆç°¡å–®ï¼šæŠŠåœ–ç‰‡åˆ‡æˆä¸€å€‹å€‹ä¸é‡ç–Šçš„ã€Œä¸­ç­‰å¤§å°ç¢ç‰‡ã€ï¼ˆpatchesï¼Œä¾‹å¦‚ 16Ã—16 åƒç´ ï¼‰ï¼Œç„¶å¾ŒæŠŠé€™äº›ç¢ç‰‡ç•¶æˆã€Œè©å½™ã€ä¸€æ¨£ï¼Œé€å…¥ Transformer é€²è¡Œåœ–åƒåˆ†é¡ã€‚
                        <br><br>
                        <strong>ViT çš„å„ªå‹¢ï¼š</strong>èˆ‡å‚³çµ±çš„ CNNï¼ˆå·ç©ç¥ç¶“ç¶²è·¯ï¼‰ç›¸æ¯”ï¼ŒViT åœ¨ã€Œé€Ÿåº¦ã€å’Œã€Œæº–ç¢ºç‡ã€ä¹‹é–“å–å¾—äº†å¾ˆå¥½çš„å¹³è¡¡â€”â€”æ—¢å¿«åˆæº–ã€‚
                        <br><br>
                        <strong>ViT çš„è³‡æ–™éœ€æ±‚ï¼š</strong>
                        <ul style="margin: 10px 0; padding-left: 25px;">
                            <li><strong>åŸå§‹ ViT</strong>ï¼šéœ€è¦è¶…å¤§è¦æ¨¡çš„è³‡æ–™é›†ï¼ˆJFT-300Mï¼Œ3 å„„å¼µåœ–ç‰‡ï¼‰æ‰èƒ½è¡¨ç¾å¥½</li>
                            <li><strong>DeiT çš„æ”¹é€²</strong>ï¼šé€éæ›´å¥½çš„è¨“ç·´ç­–ç•¥ï¼Œè®“ ViT ä¹Ÿèƒ½åœ¨è¼ƒå°çš„è³‡æ–™é›†ï¼ˆImageNet-1Kï¼Œ130 è¬å¼µåœ–ç‰‡ï¼‰ä¸Šè¡¨ç¾ä¸éŒ¯</li>
                        </ul>
                        <br><br>
                        <strong>ViT çš„å±€é™æ€§ï¼ˆé€™å°±æ˜¯ Swin Transformer è¦è§£æ±ºçš„å•é¡Œï¼‰ï¼š</strong>
                        <br><br>
                        ViT åœ¨ã€Œåœ–åƒåˆ†é¡ã€ä»»å‹™ä¸Šè¡¨ç¾å¾ˆå¥½ï¼Œä½†<strong>ç„¡æ³•ä½œç‚ºé€šç”¨éª¨å¹¹ç¶²è·¯</strong>ï¼Œç‰¹åˆ¥æ˜¯åœ¨ä»¥ä¸‹å…©ç¨®æƒ…æ³ï¼š
                        <br><br>
                        <strong>1ï¸âƒ£ å¯†é›†è¦–è¦ºä»»å‹™ï¼ˆDense Vision Tasksï¼‰</strong>
                        <ul style="margin: 10px 0; padding-left: 25px;">
                            <li>ä¾‹å¦‚ï¼šèªç¾©åˆ†å‰²ï¼ˆéœ€è¦å°æ¯å€‹åƒç´ éƒ½é€²è¡Œé æ¸¬ï¼‰ã€ç›®æ¨™æª¢æ¸¬ï¼ˆéœ€è¦æ‰¾å‡ºåœ–ç‰‡ä¸­æ‰€æœ‰ç‰©é«”çš„ä½ç½®ï¼‰</li>
                            <li>å•é¡Œï¼šViT åªç”¢ç”Ÿ<strong>ä½è§£æåº¦çš„ç‰¹å¾µåœ–</strong>ï¼ˆä¾‹å¦‚ï¼š14Ã—14ï¼‰ï¼Œç„¡æ³•æä¾›é«˜è§£æåº¦çš„é æ¸¬</li>
                        </ul>
                        <br><br>
                        <strong>2ï¸âƒ£ é«˜è§£æåº¦åœ–ç‰‡</strong>
                        <ul style="margin: 10px 0; padding-left: 25px;">
                            <li>å•é¡Œï¼šViT çš„è¨ˆç®—è¤‡é›œåº¦<strong>éš¨åœ–ç‰‡å¤§å°å‘ˆäºŒæ¬¡å¢é•·</strong>ï¼ˆO(nÂ²)ï¼‰</li>
                            <li>ä¾‹å­ï¼šè™•ç† 1024Ã—1024 çš„åœ–ç‰‡æ™‚ï¼Œè¨ˆç®—é‡æœƒæš´å¢åˆ°å¹¾ä¹ç„¡æ³•è™•ç†çš„ç¨‹åº¦</li>
                        </ul>
                        <br><br>
                        <strong>ğŸ’¡ ç¸½çµï¼š</strong>ViT è­‰æ˜äº† Transformer å¯ä»¥ç”¨åœ¨è¦–è¦ºä»»å‹™ä¸Šï¼Œä½†å®ƒåªé©åˆã€Œåœ–åƒåˆ†é¡ã€é€™ç¨®å–®ä¸€ä»»å‹™ã€‚Swin Transformer çš„ç›®æ¨™å°±æ˜¯è§£æ±ºé€™äº›å•é¡Œï¼Œè®“ Transformer æˆç‚ºçœŸæ­£çš„ã€Œé€šç”¨è¦–è¦ºéª¨å¹¹ç¶²è·¯ã€ã€‚
                    </div>
                </div>
            </div>

            <!-- ç”Ÿæ´»é¡æ¯” -->
            <div class="analogy-section">
                <div class="analogy-card life">
                    <h4>ğŸ  ç”Ÿæ´»é¡æ¯”</h4>
                    <p>
                        å°±åƒå»ºç¯‰é¢¨æ ¼çš„æ¼”é€²ï¼šCNN æ™‚ä»£åƒæ˜¯å¤å…¸å»ºç¯‰ï¼Œæ¯ä¸€å±¤éƒ½æœ‰æ˜ç¢ºçš„çµæ§‹å’ŒåŠŸèƒ½ï¼ˆå·ç©å±¤ã€æ± åŒ–å±¤ï¼‰ã€‚Transformer æ™‚ä»£åƒæ˜¯ç¾ä»£å»ºç¯‰ï¼Œæ›´åŠ éˆæ´»å’Œé–‹æ”¾ï¼ˆæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰ã€‚ViT æ˜¯ç¬¬ä¸€åº§ã€Œç¾ä»£é¢¨æ ¼ã€çš„è¦–è¦ºå»ºç¯‰ï¼Œè­‰æ˜äº†å¯è¡Œæ€§ï¼Œä½†é‚„ä¸å¤ å¯¦ç”¨ã€‚Swin Transformer å‰‡æ˜¯çœŸæ­£å¯¦ç”¨çš„ã€Œç¾ä»£å»ºç¯‰ã€ï¼Œæ—¢ä¿æŒäº†ç¾ä»£é¢¨æ ¼çš„ç¾æ„Ÿï¼Œåˆè§£æ±ºäº†å¯¦éš›ä½¿ç”¨çš„å•é¡Œã€‚
                    </p>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>è‡ªæ³¨æ„åŠ›åœ¨è¦–è¦ºä¸­çš„æ—©æœŸå˜—è©¦</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Also inspired by the success of self-attention layers and Transformer architectures in the NLP field, some works employ self-attention layers to replace some or all of the spatial convolution layers in the popular ResNet. In these works, the self-attention is computed within a local window of each pixel to expedite optimization, and they achieve slightly better accuracy/FLOPs trade-offs than the counterpart ResNet architecture. However, their costly memory access causes their actual latency to be significantly larger than that of the convolutional networks. Instead of using sliding windows, we propose to <em>shift</em> windows between consecutive layers, which allows for a more efficient implementation in general hardware.
                    </div>
                    <div class="zh">
                        <strong>ğŸ“ ç™½è©±ç¿»è­¯</strong><br><br>
                        <strong>æ—©æœŸçš„å˜—è©¦ï¼šæŠŠè‡ªæ³¨æ„åŠ›ã€Œå«æ¥ã€åˆ° CNN ä¸Š</strong>
                        <br><br>
                        å—åˆ° Transformer åœ¨ NLP é ˜åŸŸæˆåŠŸçš„å•Ÿç™¼ï¼Œä¸€äº›ç ”ç©¶å˜—è©¦ç”¨ã€Œè‡ªæ³¨æ„åŠ›å±¤ã€ä¾†æ›¿æ› ResNet ä¸­çš„ã€Œå·ç©å±¤ã€ï¼ˆéƒ¨åˆ†æˆ–å…¨éƒ¨ï¼‰ã€‚
                        <br><br>
                        <strong>ğŸ’¡ ä»€éº¼æ˜¯ç©ºé–“å·ç©å±¤ï¼Ÿ</strong>å°±æ˜¯ ResNet ä¸­è™•ç†åœ–ç‰‡ç©ºé–“è³‡è¨Šçš„å·ç©å±¤ï¼Œä¾‹å¦‚ 3Ã—3 çš„å·ç©æ ¸ã€‚
                        <br><br>
                        <strong>é€™äº›å·¥ä½œçš„åšæ³•ï¼š</strong>
                        <ul style="margin: 10px 0; padding-left: 25px;">
                            <li>åœ¨æ¯å€‹åƒç´ çš„<strong>å±€éƒ¨è¦–çª—</strong>å…§è¨ˆç®—è‡ªæ³¨æ„åŠ›ï¼ˆä¾‹å¦‚ï¼šæ¯å€‹åƒç´ åªçœ‹å‘¨åœ 7Ã—7 çš„å€åŸŸï¼‰</li>
                            <li>é€™æ¨£åšæ˜¯ç‚ºäº†<strong>åŠ å¿«å„ªåŒ–</strong>ï¼ˆå¦‚æœçœ‹å…¨å±€ï¼Œè¨ˆç®—é‡å¤ªå¤§ï¼‰</li>
                        </ul>
                        <br><br>
                        <strong>é€™äº›å·¥ä½œçš„çµæœï¼š</strong>
                        <ul style="margin: 10px 0; padding-left: 25px;">
                            <li><strong>æº–ç¢ºç‡/FLOPs æ¬Šè¡¡ç¨å¥½</strong>ï¼šåœ¨ç›¸åŒçš„è¨ˆç®—é‡ï¼ˆFLOPsï¼‰ä¸‹ï¼Œæº–ç¢ºç‡æ¯” ResNet ç¨é«˜ä¸€é»</li>
                            <li><strong>ä½†å¯¦éš›å»¶é²æ›´å¤§</strong>ï¼šé›–ç„¶è¨ˆç®—é‡å·®ä¸å¤šï¼Œä½†å¯¦éš›é‹è¡Œé€Ÿåº¦åè€Œæ›´æ…¢</li>
                        </ul>
                        <br><br>
                        <strong>ç‚ºä»€éº¼æœƒé€™æ¨£ï¼Ÿ</strong>
                        <br><br>
                        å•é¡Œå‡ºåœ¨<strong>è¨˜æ†¶é«”è¨ªå•</strong>ã€‚é€™äº›æ–¹æ³•ä½¿ç”¨ã€Œæ»‘å‹•è¦–çª—ã€æ©Ÿåˆ¶ï¼Œéœ€è¦é »ç¹åœ°è®€å¯«è¨˜æ†¶é«”ï¼Œå°è‡´ï¼š
                        <ul style="margin: 10px 0; padding-left: 25px;">
                            <li>è¨˜æ†¶é«”è¨ªå•æ¨¡å¼ä¸è¦å‰‡ï¼Œç„¡æ³•å……åˆ†åˆ©ç”¨ç¡¬é«”çš„ä¸¦è¡Œèƒ½åŠ›</li>
                            <li>è¨˜æ†¶é«”å¸¶å¯¬æˆç‚ºç“¶é ¸ï¼Œå³ä½¿è¨ˆç®—é‡ä¸å¤§ï¼Œå¯¦éš›é€Ÿåº¦ä¹Ÿå¾ˆæ…¢</li>
                        </ul>
                        <br><br>
                        <strong>Swin Transformer çš„å‰µæ–°ï¼šç§»ä½è¦–çª—ï¼ˆShifted Windowsï¼‰</strong>
                        <br><br>
                        æˆ‘å€‘<strong>ä¸ä½¿ç”¨æ»‘å‹•è¦–çª—</strong>ï¼Œè€Œæ˜¯æå‡º<strong>åœ¨é€£çºŒå±¤ä¹‹é–“ç§»ä½è¦–çª—</strong>ï¼š
                        <ul style="margin: 10px 0; padding-left: 25px;">
                            <li><strong>æ»‘å‹•è¦–çª—</strong>ï¼šæ¯å€‹åƒç´ éƒ½è¦è¨ˆç®—ä¸€å€‹æ–°çš„è¦–çª—ï¼Œè¨˜æ†¶é«”è¨ªå•ä¸è¦å‰‡</li>
                            <li><strong>ç§»ä½è¦–çª—</strong>ï¼šè¦–çª—ä½ç½®åœ¨æ¯ä¸€å±¤ä¹‹é–“ã€Œç§»ä½ã€ï¼Œä½†æ¯å€‹è¦–çª—å…§çš„è¨ˆç®—æ˜¯è¦å‰‡çš„ï¼Œè¨˜æ†¶é«”è¨ªå•æ›´é«˜æ•ˆ</li>
                        </ul>
                        <br><br>
                        <strong>ğŸ’¡ é¡æ¯”ï¼š</strong>
                        <ul style="margin: 10px 0; padding-left: 25px;">
                            <li><strong>æ»‘å‹•è¦–çª—</strong>ï¼šåƒæ˜¯ä¸€å€‹äººæ‹¿è‘—æ”¾å¤§é¡ï¼Œåœ¨åœ–ç‰‡ä¸Šæ…¢æ…¢æ»‘å‹•ï¼Œæ¯æ¬¡éƒ½è¦é‡æ–°å°ç„¦</li>
                            <li><strong>ç§»ä½è¦–çª—</strong>ï¼šåƒæ˜¯æŠŠåœ–ç‰‡åˆ†æˆå›ºå®šçš„æ ¼å­ï¼Œæ¯æ¬¡çœ‹ä¸åŒçš„æ ¼å­ï¼Œä½†æ¯å€‹æ ¼å­å…§çš„è§€å¯Ÿæ˜¯è¦å‰‡çš„</li>
                        </ul>
                        <br><br>
                        é€™ç¨®è¨­è¨ˆè®“ Swin Transformer åœ¨é€šç”¨ç¡¬é«”ï¼ˆGPUã€TPUï¼‰ä¸Šèƒ½å¤ å¯¦ç¾æ›´é«˜æ•ˆçš„å¯¦ä½œï¼Œæ—¢ä¿æŒäº†è‡ªæ³¨æ„åŠ›çš„å„ªå‹¢ï¼Œåˆé¿å…äº†è¨˜æ†¶é«”è¨ªå•çš„ç“¶é ¸ã€‚
                    </div>
                </div>
            </div>

            <!-- Quote Block é‡‘å¥ -->
            <div class="quote-block">
                ã€Œå¾ CNN çš„æ­¸ç´åç½®åˆ° Transformer çš„éˆæ´»æ€§ï¼ŒSwin æ‰¾åˆ°äº†æœ€ä½³å¹³è¡¡é»ã€
            </div>

            <!-- æœ¬ç« é‡é»å›é¡§ -->
            <div class="chapter-summary">
                <h3>ğŸ’¡ æœ¬ç« é‡é»</h3>
                <ul>
                    <li><strong>CNN æ™‚ä»£</strong>ï¼šå¾ AlexNet åˆ° EfficientNetï¼Œé€šéè¦æ¨¡å’Œæ¶æ§‹å‰µæ–°ä¸æ–·æå‡æ€§èƒ½</li>
                    <li><strong>Transformer åœ¨ NLP</strong>ï¼šæ³¨æ„åŠ›æ©Ÿåˆ¶åœ¨èªè¨€é ˜åŸŸå–å¾—å·¨å¤§æˆåŠŸ</li>
                    <li><strong>ViT çš„é–‹å‰µ</strong>ï¼šé¦–æ¬¡å°‡ Transformer æ‡‰ç”¨åˆ°è¦–è¦ºï¼Œè­‰æ˜äº†å¯è¡Œæ€§</li>
                    <li><strong>ViT çš„å±€é™</strong>ï¼šå–®ä¸€è§£æåº¦ã€äºŒæ¬¡è¤‡é›œåº¦ï¼Œä¸é©åˆä½œç‚ºé€šç”¨éª¨å¹¹ç¶²è·¯</li>
                    <li><strong>Swin çš„å®šä½</strong>ï¼šåœ¨ ViT çš„åŸºç¤ä¸Šï¼Œè§£æ±ºé€šç”¨æ€§å’Œæ•ˆç‡å•é¡Œ</li>
                </ul>
            </div>
        </div>

        <!-- Navigation -->
        <div class="chapter-nav">
            <a href="01-introduction.html" class="prev">â† ä¸Šä¸€ç« </a>
            <a href="index.html" class="home">ğŸ“‘ ç›®éŒ„</a>
            <a href="03-architecture.html" class="next">ä¸‹ä¸€ç« ï¼šæ•´é«”æ¶æ§‹ â†’</a>
        </div>
    </div>
</body>
</html>
