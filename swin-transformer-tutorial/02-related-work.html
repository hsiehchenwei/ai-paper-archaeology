<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç›¸é—œå·¥ä½œ - Swin Transformer æ·±åº¦è§£æ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <!-- Hero Section -->
        <div class="hero-section" style="background-image: url('images/generated/hero_02_related_work.png');">
            <div class="hero-overlay"></div>
            <div class="hero-content">
                <h1>å¾ CNN åˆ° Transformer çš„æ¼”é€²</h1>
                <p class="hero-subtitle">è¦–è¦º AI æ¶æ§‹çš„æ­·å²è„ˆçµ¡</p>
                <div class="hero-meta">Swin Transformer æ·±åº¦è§£æ Â· ç¬¬ 2 ç« </div>
            </div>
        </div>

        <!-- Breadcrumb -->
        <nav class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <a href="index.html">Swin Transformer æ·±åº¦è§£æ</a>
            <span>/</span>
            <span class="current">ç¬¬ 2 ç« ï¼šç›¸é—œå·¥ä½œ</span>
        </nav>

        <div class="story-container">
            <!-- Drop Cap é–‹å ´ -->
            <p class="drop-cap">
                è¦ç†è§£ Swin Transformer çš„å‰µæ–°ï¼Œæˆ‘å€‘å¿…é ˆå…ˆå›é¡§è¦–è¦º AI çš„ç™¼å±•æ­·ç¨‹ã€‚å¾ 2012 å¹´ AlexNet é–‹å•Ÿæ·±åº¦å­¸ç¿’è¦–è¦ºæ™‚ä»£ï¼Œåˆ° 2020 å¹´ ViT è­‰æ˜ Transformer åœ¨è¦–è¦ºé ˜åŸŸçš„æ½›åŠ›ï¼Œå†åˆ° 2021 å¹´ Swin Transformer å¯¦ç¾çœŸæ­£çš„é€šç”¨è¦–è¦ºéª¨å¹¹ç¶²è·¯ï¼Œé€™æ˜¯ä¸€æ®µå¾å·ç©åˆ°æ³¨æ„åŠ›çš„æ¼”é€²å²ã€‚
            </p>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>CNN æ™‚ä»£çš„å·”å³°</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Beginning with AlexNet and its revolutionary performance on the ImageNet image classification challenge, CNN architectures have evolved to become increasingly powerful through greater scale, more extensive connections, and more sophisticated forms of convolution. With CNNs serving as backbone networks for a variety of vision tasks, these architectural advances have led to performance improvements that have broadly lifted the entire field.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        é›»è…¦è¦–è¦ºä¸­çš„å»ºæ¨¡é•·æœŸä»¥ä¾†ä¸€ç›´ç”±å·ç©ç¥ç¶“ç¶²è·¯ï¼ˆCNNï¼‰ä¸»å°ã€‚å¾ AlexNet åŠå…¶åœ¨ ImageNet åœ–åƒåˆ†é¡æŒ‘æˆ°ä¸­çš„é©å‘½æ€§è¡¨ç¾é–‹å§‹ï¼ŒCNN æ¶æ§‹é€šéæ›´å¤§çš„è¦æ¨¡ã€æ›´å»£æ³›çš„é€£æ¥å’Œæ›´è¤‡é›œçš„å·ç©å½¢å¼ï¼Œæ¼”é€²å¾—è¶Šä¾†è¶Šå¼·å¤§ã€‚éš¨è‘— CNN ä½œç‚ºå„ç¨®è¦–è¦ºä»»å‹™çš„éª¨å¹¹ç¶²è·¯ï¼Œé€™äº›æ¶æ§‹é€²æ­¥å¸¶ä¾†äº†æ€§èƒ½æå‡ï¼Œå»£æ³›æ¨å‹•äº†æ•´å€‹é ˜åŸŸçš„ç™¼å±•ã€‚
                    </div>
                </div>
            </div>

            <div class="key-concept">
                <h4>ğŸ“š CNN æ¼”é€²å²</h4>
                <ul>
                    <li><strong>AlexNet (2012)</strong>ï¼šé–‹å•Ÿæ·±åº¦å­¸ç¿’è¦–è¦ºæ™‚ä»£</li>
                    <li><strong>VGG (2014)</strong>ï¼šæ·±åº¦ç¶²è·¯è¨­è¨ˆåŸå‰‡</li>
                    <li><strong>ResNet (2015)</strong>ï¼šæ®˜å·®å­¸ç¿’ï¼Œè§£æ±ºæ·±åº¦ç¶²è·¯è¨“ç·´å•é¡Œ</li>
                    <li><strong>DenseNet (2017)</strong>ï¼šå¯†é›†é€£æ¥ï¼Œç‰¹å¾µé‡ç”¨</li>
                    <li><strong>EfficientNet (2019)</strong>ï¼šè¤‡åˆç¸®æ”¾ç­–ç•¥</li>
                </ul>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>Transformer åœ¨ NLP çš„æˆåŠŸ</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        On the other hand, the evolution of network architectures in natural language processing (NLP) has taken a different path, where the prevalent architecture today is instead the Transformer. Designed for sequence modeling and transduction tasks, the Transformer is notable for its use of attention to model long-range dependencies in the data. Its tremendous success in the language domain has led researchers to investigate its adaptation to computer vision, where it has recently demonstrated promising results on certain tasks, specifically image classification and joint vision-language modeling.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        å¦ä¸€æ–¹é¢ï¼Œè‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰ä¸­ç¶²è·¯æ¶æ§‹çš„æ¼”é€²èµ°äº†ä¸€æ¢ä¸åŒçš„é“è·¯ï¼Œä»Šå¤©çš„ä¸»æµæ¶æ§‹æ˜¯ Transformerã€‚Transformer å°ˆç‚ºåºåˆ—å»ºæ¨¡å’Œè½‰å°ä»»å‹™è€Œè¨­è¨ˆï¼Œä»¥å…¶ä½¿ç”¨æ³¨æ„åŠ›ä¾†å»ºæ¨¡æ•¸æ“šä¸­çš„é•·ç¨‹ä¾è³´é—œä¿‚è€Œè‘—ç¨±ã€‚å®ƒåœ¨èªè¨€é ˜åŸŸçš„å·¨å¤§æˆåŠŸä¿ƒä½¿ç ”ç©¶äººå“¡ç ”ç©¶å…¶åœ¨é›»è…¦è¦–è¦ºä¸­çš„é©é…ï¼Œæœ€è¿‘åœ¨ç‰¹å®šä»»å‹™ä¸Šå±•ç¾äº†æœ‰å¸Œæœ›çš„çµæœï¼Œç‰¹åˆ¥æ˜¯åœ–åƒåˆ†é¡å’Œè¯åˆè¦–è¦º-èªè¨€å»ºæ¨¡ã€‚
                    </div>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>ViT çš„é–‹å‰µèˆ‡å±€é™</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Most related to our work is the Vision Transformer (ViT) and its follow-ups. The pioneering work of ViT directly applies a Transformer architecture on non-overlapping medium-sized image patches for image classification. It achieves an impressive speed-accuracy trade-off on image classification compared to convolutional networks. While ViT requires large-scale training datasets (i.e., JFT-300M) to perform well, DeiT introduces several training strategies that allow ViT to also be effective using the smaller ImageNet-1K dataset. The results of ViT on image classification are encouraging, but its architecture is unsuitable for use as a general-purpose backbone network on dense vision tasks or when the input image resolution is high, due to its low-resolution feature maps and the quadratic increase in complexity with image size.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        èˆ‡æˆ‘å€‘çš„å·¥ä½œæœ€ç›¸é—œçš„æ˜¯ Vision Transformer (ViT) åŠå…¶å¾ŒçºŒå·¥ä½œã€‚ViT çš„é–‹å‰µæ€§å·¥ä½œç›´æ¥å°‡ Transformer æ¶æ§‹æ‡‰ç”¨æ–¼éé‡ç–Šçš„ä¸­ç­‰å¤§å°åœ–åƒ patch é€²è¡Œåœ–åƒåˆ†é¡ã€‚èˆ‡å·ç©ç¶²è·¯ç›¸æ¯”ï¼Œå®ƒåœ¨åœ–åƒåˆ†é¡ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„é€Ÿåº¦-æº–ç¢ºç‡æ¬Šè¡¡ã€‚é›–ç„¶ ViT éœ€è¦å¤§è¦æ¨¡è¨“ç·´æ•¸æ“šé›†ï¼ˆå³ JFT-300Mï¼‰æ‰èƒ½è¡¨ç¾è‰¯å¥½ï¼Œä½† DeiT å¼•å…¥äº†å¹¾ç¨®è¨“ç·´ç­–ç•¥ï¼Œä½¿ ViT ä¹Ÿèƒ½åœ¨ä½¿ç”¨è¼ƒå°çš„ ImageNet-1K æ•¸æ“šé›†æ™‚æœ‰æ•ˆã€‚ViT åœ¨åœ–åƒåˆ†é¡ä¸Šçš„çµæœä»¤äººé¼“èˆï¼Œä½†å…¶æ¶æ§‹ä¸é©åˆä½œç‚ºå¯†é›†è¦–è¦ºä»»å‹™æˆ–è¼¸å…¥åœ–åƒè§£æåº¦è¼ƒé«˜æ™‚çš„é€šç”¨éª¨å¹¹ç¶²è·¯ï¼Œé€™æ˜¯ç”±æ–¼å…¶ä½è§£æåº¦ç‰¹å¾µåœ–å’Œè¤‡é›œåº¦éš¨åœ–åƒå¤§å°å‘ˆäºŒæ¬¡å¢é•·ã€‚
                    </div>
                </div>
            </div>

            <!-- é›™é‡é¡æ¯” -->
            <div class="analogy-section">
                <div class="analogy-card life">
                    <h4>ğŸ  ç”Ÿæ´»é¡æ¯”</h4>
                    <p>
                        å°±åƒå»ºç¯‰é¢¨æ ¼çš„æ¼”é€²ï¼šCNN æ™‚ä»£åƒæ˜¯å¤å…¸å»ºç¯‰ï¼Œæ¯ä¸€å±¤éƒ½æœ‰æ˜ç¢ºçš„çµæ§‹å’ŒåŠŸèƒ½ï¼ˆå·ç©å±¤ã€æ± åŒ–å±¤ï¼‰ã€‚Transformer æ™‚ä»£åƒæ˜¯ç¾ä»£å»ºç¯‰ï¼Œæ›´åŠ éˆæ´»å’Œé–‹æ”¾ï¼ˆæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰ã€‚ViT æ˜¯ç¬¬ä¸€åº§ã€Œç¾ä»£é¢¨æ ¼ã€çš„è¦–è¦ºå»ºç¯‰ï¼Œè­‰æ˜äº†å¯è¡Œæ€§ï¼Œä½†é‚„ä¸å¤ å¯¦ç”¨ã€‚Swin Transformer å‰‡æ˜¯çœŸæ­£å¯¦ç”¨çš„ã€Œç¾ä»£å»ºç¯‰ã€ï¼Œæ—¢ä¿æŒäº†ç¾ä»£é¢¨æ ¼çš„ç¾æ„Ÿï¼Œåˆè§£æ±ºäº†å¯¦éš›ä½¿ç”¨çš„å•é¡Œã€‚
                    </p>
                </div>
                <div class="analogy-card engineering">
                    <h4>âš™ï¸ å·¥ç¨‹é¡æ¯”</h4>
                    <p>
                        åœ¨è»Ÿé«”æ¶æ§‹ä¸Šï¼Œé€™é¡ä¼¼æ–¼å¾å–®é«”æ¶æ§‹ï¼ˆCNNï¼‰åˆ°å¾®æœå‹™æ¶æ§‹ï¼ˆTransformerï¼‰çš„æ¼”é€²ã€‚ViT å°±åƒæ˜¯ç¬¬ä¸€æ¬¡å˜—è©¦å¾®æœå‹™æ¶æ§‹ï¼Œè­‰æ˜äº†æ¦‚å¿µå¯è¡Œï¼Œä½†é‚„æœ‰å¾ˆå¤šå•é¡Œï¼ˆå–®ä¸€è§£æåº¦ã€é«˜è¤‡é›œåº¦ï¼‰ã€‚Swin Transformer å‰‡æ˜¯æˆç†Ÿçš„å¾®æœå‹™æ¶æ§‹ï¼Œé€šéå±¤æ¬¡åŒ–è¨­è¨ˆå’Œå±€éƒ¨è¨ˆç®—ï¼Œæ—¢ä¿æŒäº†å¾®æœå‹™çš„éˆæ´»æ€§ï¼Œåˆè§£æ±ºäº†æ€§èƒ½å’Œè¤‡é›œåº¦çš„å•é¡Œã€‚
                    </p>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <h2>è‡ªæ³¨æ„åŠ›åœ¨è¦–è¦ºä¸­çš„æ—©æœŸå˜—è©¦</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Also inspired by the success of self-attention layers and Transformer architectures in the NLP field, some works employ self-attention layers to replace some or all of the spatial convolution layers in the popular ResNet. In these works, the self-attention is computed within a local window of each pixel to expedite optimization, and they achieve slightly better accuracy/FLOPs trade-offs than the counterpart ResNet architecture. However, their costly memory access causes their actual latency to be significantly larger than that of the convolutional networks. Instead of using sliding windows, we propose to <em>shift</em> windows between consecutive layers, which allows for a more efficient implementation in general hardware.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        åŒæ¨£å—åˆ°è‡ªæ³¨æ„åŠ›å±¤å’Œ Transformer æ¶æ§‹åœ¨ NLP é ˜åŸŸæˆåŠŸçš„å•Ÿç™¼ï¼Œä¸€äº›å·¥ä½œä½¿ç”¨è‡ªæ³¨æ„åŠ›å±¤ä¾†æ›¿æ›æµè¡Œ ResNet ä¸­çš„éƒ¨åˆ†æˆ–å…¨éƒ¨ç©ºé–“å·ç©å±¤ã€‚åœ¨é€™äº›å·¥ä½œä¸­ï¼Œè‡ªæ³¨æ„åŠ›åœ¨æ¯å€‹åƒç´ çš„å±€éƒ¨è¦–çª—å…§è¨ˆç®—ä»¥åŠ å¿«å„ªåŒ–ï¼Œå®ƒå€‘å–å¾—äº†æ¯”å°æ‡‰ ResNet æ¶æ§‹ç¨å¥½çš„æº–ç¢ºç‡/FLOPs æ¬Šè¡¡ã€‚ç„¶è€Œï¼Œå®ƒå€‘æ˜‚è²´çš„è¨˜æ†¶é«”è¨ªå•å°è‡´å…¶å¯¦éš›å»¶é²æ˜é¡¯å¤§æ–¼å·ç©ç¶²è·¯ã€‚æˆ‘å€‘ä¸æ˜¯ä½¿ç”¨æ»‘å‹•è¦–çª—ï¼Œè€Œæ˜¯æå‡ºåœ¨é€£çºŒå±¤ä¹‹é–“<em>ç§»ä½</em>è¦–çª—ï¼Œé€™å…è¨±åœ¨é€šç”¨ç¡¬é«”ä¸Šå¯¦ç¾æ›´é«˜æ•ˆçš„å¯¦ç¾ã€‚
                    </div>
                </div>
            </div>

            <!-- Quote Block é‡‘å¥ -->
            <div class="quote-block">
                ã€Œå¾ CNN çš„æ­¸ç´åç½®åˆ° Transformer çš„éˆæ´»æ€§ï¼ŒSwin æ‰¾åˆ°äº†æœ€ä½³å¹³è¡¡é»ã€
            </div>

            <!-- æœ¬ç« é‡é»å›é¡§ -->
            <div class="chapter-summary">
                <h3>ğŸ’¡ æœ¬ç« é‡é»</h3>
                <ul>
                    <li><strong>CNN æ™‚ä»£</strong>ï¼šå¾ AlexNet åˆ° EfficientNetï¼Œé€šéè¦æ¨¡å’Œæ¶æ§‹å‰µæ–°ä¸æ–·æå‡æ€§èƒ½</li>
                    <li><strong>Transformer åœ¨ NLP</strong>ï¼šæ³¨æ„åŠ›æ©Ÿåˆ¶åœ¨èªè¨€é ˜åŸŸå–å¾—å·¨å¤§æˆåŠŸ</li>
                    <li><strong>ViT çš„é–‹å‰µ</strong>ï¼šé¦–æ¬¡å°‡ Transformer æ‡‰ç”¨åˆ°è¦–è¦ºï¼Œè­‰æ˜äº†å¯è¡Œæ€§</li>
                    <li><strong>ViT çš„å±€é™</strong>ï¼šå–®ä¸€è§£æåº¦ã€äºŒæ¬¡è¤‡é›œåº¦ï¼Œä¸é©åˆä½œç‚ºé€šç”¨éª¨å¹¹ç¶²è·¯</li>
                    <li><strong>Swin çš„å®šä½</strong>ï¼šåœ¨ ViT çš„åŸºç¤ä¸Šï¼Œè§£æ±ºé€šç”¨æ€§å’Œæ•ˆç‡å•é¡Œ</li>
                </ul>
            </div>
        </div>

        <!-- Navigation -->
        <div class="chapter-nav">
            <a href="01-introduction.html" class="prev">â† ä¸Šä¸€ç« </a>
            <a href="index.html" class="home">ğŸ“‘ ç›®éŒ„</a>
            <a href="03-architecture.html" class="next">ä¸‹ä¸€ç« ï¼šæ•´é«”æ¶æ§‹ â†’</a>
        </div>
    </div>
</body>
</html>
