<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <!-- Primary Meta Tags -->
    <title>Chapter 02: 預訓練模型的選擇 | Prompt Engineering 論文導讀</title>
    <meta name="description" content="深入解析三種主流預訓練模型架構對 Prompt 的影響：Left-to-Right (GPT), Masked (BERT), 和 Encoder-Decoder (T5)。了解為什麼 GPT 適合生成，BERT 適合分類，T5 適合翻譯。" />
    <meta name="keywords" content="預訓練模型, GPT vs BERT, T5, Language Models, Left-to-Right LM, Masked LM, Encoder-Decoder, Prompt 兼容性" />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Chapter 02: 預訓練模型的選擇 | Prompt Engineering 論文導讀" />
    <meta property="og:description" content="不是每個模型都適合玩 Prompt。深入比較 GPT (鸚鵡)、BERT (偵探) 與 T5 (翻譯官) 的性格差異與適用場景。" />
    <meta property="og:image" content="https://hsiehchenwei.github.io/ai-paper-archaeology/prompt-engineering-tutorial/images/model_types_robots.png" />

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image" />
    <meta property="twitter:title" content="Chapter 02: 預訓練模型的選擇 | Prompt Engineering 論文導讀" />
    <meta property="twitter:description" content="不是每個模型都適合玩 Prompt。深入比較 GPT (鸚鵡)、BERT (偵探) 與 T5 (翻譯官) 的性格差異與適用場景。" />
    <meta property="twitter:image" content="https://hsiehchenwei.github.io/ai-paper-archaeology/prompt-engineering-tutorial/images/model_types_robots.png" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&family=Noto+Serif+TC:wght@400;700&family=JetBrains+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="../styles/global.css" />
    <link rel="stylesheet" href="../styles/prompt-eng.css" />
    
  </head>
  <body class="prompt-eng">

    <header class="chapter-header">
      <div class="header-bg"></div>
      <div class="header-overlay"></div>
      <div class="header-content">
        <span class="chapter-num">Chapter 02</span>
        <h1 class="chapter-title">預訓練模型的選擇</h1>
        <p style="font-size: 1.2rem; color: rgba(255,255,255,0.9); max-width: 600px;">
          不是每個模型都適合玩 Prompt：GPT, BERT 與 T5 的性格分析
        </p>
      </div>
    </header>

    <article class="content-container">
      
      <p style="font-size: 1.2rem; color: #666; margin-bottom: 60px;">
        在確定了「Prompt」這個方向後，下一個問題是：我們該用誰？這篇論文系統性地比較了當時最強的三類模型：Left-to-Right (GPT), Masked (BERT), 和 Encoder-Decoder (T5)。
      </p>

      <figure style="margin: 40px 0;">
        <img src="images/comic_models.png" alt="AI Models Comic: GPT, BERT, T5" style="width: 100%; border-radius: 8px; box-shadow: 0 10px 30px rgba(0,0,0,0.1);">
        <figcaption style="text-align: center; font-size: 0.9rem; color: #666; margin-top: 10px;">
          AI 模型的三種人格：話癆鸚鵡 (GPT)、克漏字偵探 (BERT)、精準翻譯官 (T5)
        </figcaption>
      </figure>

      <h2>1. 天選之子：Left-to-Right LM (GPT 系列)</h2>

      <div class="deep-dive-block">
        <div class="original-text">
          "Left-to-right LMs (L2R LMs), a variety of auto-regressive LM, predict the upcoming words or assign a probability P(x) to a sequence of words..."<br><br>
          "The probability is commonly broken down using the chain rule in a left-to-right fashion: P(x) = P(x1) × ... P(xn|x1...xn-1)."
        </div>
        <div class="analysis-text">
          <p>
            <strong>單向生成模型 (GPT)</strong>
          </p>
          <p>
            這是最標準的語言模型，像一隻只會往下說話的鸚鵡。它的原理很簡單：根據上文，猜測下一個字是什麼。
          </p>
          <p>
            雖然在 BERT 出現時，這種單向結構被認為「不夠聰明」（因為看不到下文），但這篇論文指出，正是這種「接龍」特性，讓它成為 Prompt 的最佳載體。因為 Prompt 的本質就是「給個開頭，請你接下去」。
          </p>
          <div class="chatgpt-insight">
            <span class="chatgpt-badge">現代使用對照</span>
            <p><strong>這就是為什麼它是 "Chat" GPT。</strong></p>
            <p>當你在使用 ChatGPT 時，你會發現它是逐字吐出來的（Streaming）。這正是 Left-to-Right 架構的特徵。它永遠在做同一件事：根據你給的 Prompt (上文)，預測下一個字。這種架構最適合「生成式」任務，比如寫文章、寫代碼。</p>
          </div>
        </div>
      </div>

      <h2>2. 閱讀高手：Masked LM (BERT 系列)</h2>

      <div class="deep-dive-block">
        <div class="original-text">
          "One popular bidirectional objective function... is the masked language model (MLM)... which aims to predict masked text pieces based on surrounded context."<br><br>
          "In prompting methods, MLMs are generally most suitable for natural language understanding... (e.g., text classification...)"
        </div>
        <div class="analysis-text">
          <p>
            <strong>雙向填空模型 (BERT)</strong>
          </p>
          <p>
            BERT 是一個拿著放大鏡的偵探。它能同時看到上文和下文，任務是填補中間被挖掉的空缺 (Mask)。
          </p>
          <p>
            論文指出，這種模型非常適合做「完形填空」型的 Prompt (Cloze Prompt)。例如：<br>
            <code>"這部電影很無聊。情感：[MASK]"</code>
          </p>
          <div class="chatgpt-insight">
            <span class="chatgpt-badge">現代使用對照</span>
            <p><strong>它現在活在 "Embeddings" 裡。</strong></p>
            <p>雖然你在聊天時很少直接跟 BERT 對話，但它無處不在。當你使用 RAG (檢索增強生成) 搜尋文件時，背後幫你把文字變成向量 (Vector) 進行比對的，通常就是 BERT 的後代。它不善言辭（生成長文能力弱），但非常擅長「理解」和「分類」。</p>
          </div>
        </div>
      </div>

      <h2>3. 翻譯官：Encoder-Decoder (T5, BART)</h2>

      <div class="deep-dive-block">
        <div class="original-text">
          "The encoder-decoder model is a model that uses a left-to-right LM to decode y conditioned on a separate encoder for text x..."<br><br>
          "Pre-trained models... can be naturally used to text generation tasks... However, recent studies reveal that other non-generation tasks... can be reformulated a generation problems."
        </div>
        <div class="analysis-text">
          <p>
            <strong>編碼-解碼模型 (T5)</strong>
          </p>
          <p>
            這是傳統機器翻譯的標準架構：左邊一個 Encoder 負責聽（讀入原文），右邊一個 Decoder 負責說（輸出譯文）。
          </p>
          <p>
            Google 的 T5 (Text-to-Text Transfer Transformer) 提倡「萬物皆文本」。透過 Prompt，它可以把翻譯、摘要、分類都變成「輸入文字 -> 輸出文字」的過程。
          </p>
          <div class="chatgpt-insight">
            <span class="chatgpt-badge">現代使用對照</span>
            <p><strong>這就是 Google Bard (Gemini 前身) 的起源。</strong></p>
            <p>Google 一直堅信 Encoder-Decoder 是更強大的架構（因為 Encoder 可以雙向理解）。雖然現在主流大模型（如 Llama, GPT-4）大多回歸了純 Decoder (GPT-style) 架構，但 Encoder-Decoder 在翻譯和特定任務上依然有獨特優勢。</p>
          </div>
        </div>
      </div>

      <div class="nav-footer">
        <a href="01-four-paradigms.html" class="nav-link">← 上一章：四大範式</a>
        <a href="03-prompt-engineering-basics.html" class="nav-link">下一章：提示工程基礎 →</a>
      </div>

    </article>

  </body>
</html>

