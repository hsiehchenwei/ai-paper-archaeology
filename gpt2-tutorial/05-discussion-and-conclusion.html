<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT-2 ç¬¬5ç« ï¼šè¨è«–ã€é™åˆ¶èˆ‡æœªä¾†å±•æœ›</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="hero-section" style="background-image: url('images/future_path.png'); height: 70vh;">
        <div class="hero-overlay"></div>
        <div class="hero-content">
            <h1>é€šå¾€æœªä¾†çš„æ©‹æ¨‘</h1>
            <p class="hero-subtitle">GPT-2 ä¸æ˜¯çµ‚é»ï¼Œè€Œæ˜¯èµ·é»</p>
            <p class="hero-meta">GPT-2 è«–æ–‡æ·±åº¦è§£æ Â· ç¬¬ 5 ç« </p>
        </div>
    </div>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <a href="index.html">GPT-2 æ•™å­¸</a>
            <span>/</span>
            <span class="current">ç¬¬ 5 ç« </span>
        </div>

        <h2>4. Generalization vs Memorizationï¼ˆæ³›åŒ– vs è¨˜æ†¶ï¼‰</h2>

        <div style="margin: 40px 0; text-align: center;">
            <img src="images/generalization_vs_memorization.png" alt="æ³›åŒ–èˆ‡è¨˜æ†¶çš„æ¦‚å¿µå°æ¯”" style="max-width: 100%; height: auto; border-radius: 12px; box-shadow: 0 8px 30px rgba(0,0,0,0.12);">
            <p class="image-caption">
                æ³›åŒ– vs è¨˜æ†¶ï¼šGPT-2 æ˜¯çœŸçš„ç†è§£äº†æ¨¡å¼ï¼Œé‚„æ˜¯åªæ˜¯è¨˜ä½äº†è¨“ç·´è³‡æ–™ï¼Ÿ
            </p>
        </div>

        <div class="story-container">
            <p class="drop-cap">
                GPT-2 çš„é©šäººè¡¨ç¾å¼•ç™¼äº†ä¸€å€‹é—œéµå•é¡Œï¼š
                å®ƒæ˜¯çœŸçš„ã€Œå­¸æœƒã€äº†åŸ·è¡Œä»»å‹™ï¼Œé‚„æ˜¯åªæ˜¯ã€Œè¨˜ä½ã€äº†è¨“ç·´è³‡æ–™ï¼Ÿ
            </p>
            <div class="section-divider"><span>âœ¦</span></div>
        </div>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "Recent work in computer vision has shown that common image datasets contain a non-trivial amount of near-duplicate images. For instance CIFAR-10 has 3.3% overlap between train and test images (Barz & Denzler, 2019). This results in <strong>an over-reporting of the generalization performance of machine learning systems</strong>. As the size of datasets increases this issue becomes increasingly likely which suggests a similar phenomena could be happening with WebText. Therefore it is important to analyze how much test data also shows up in the training data."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šè¨ˆç®—æ©Ÿè¦–è¦ºçš„æœ€æ–°å·¥ä½œé¡¯ç¤ºï¼Œå¸¸è¦‹çš„åœ–åƒè³‡æ–™é›†åŒ…å«å¤§é‡è¿‘ä¼¼é‡è¤‡çš„åœ–åƒã€‚ä¾‹å¦‚ï¼ŒCIFAR-10 åœ¨è¨“ç·´å’Œæ¸¬è©¦åœ–åƒä¹‹é–“æœ‰ 3.3% çš„é‡ç–Šã€‚é€™å°è‡´<strong>æ©Ÿå™¨å­¸ç¿’ç³»çµ±çš„æ³›åŒ–æ•ˆèƒ½è¢«é«˜ä¼°</strong>ã€‚éš¨è‘—è³‡æ–™é›†è¦æ¨¡çš„å¢åŠ ï¼Œé€™å€‹å•é¡Œè®Šå¾—è¶Šä¾†è¶Šå¯èƒ½ï¼Œé€™æš—ç¤º WebText å¯èƒ½ç™¼ç”Ÿé¡ä¼¼ç¾è±¡ã€‚å› æ­¤ï¼Œåˆ†ææœ‰å¤šå°‘æ¸¬è©¦è³‡æ–™ä¹Ÿå‡ºç¾åœ¨è¨“ç·´è³‡æ–™ä¸­å¾ˆé‡è¦ã€‚
        </div>

        <h3>ğŸ” è³‡æ–™æ±¡æŸ“æª¢æ¸¬ï¼šBloom Filter</h3>

        <div style="margin: 40px 0; text-align: center;">
            <img src="images/data_contamination_table6.png" alt="Table 6 è³‡æ–™æ±¡æŸ“åˆ†æè¦–è¦ºåŒ–" style="max-width: 100%; height: auto; border-radius: 12px; box-shadow: 0 8px 30px rgba(0,0,0,0.12);">
            <p class="image-caption">
                è³‡æ–™æ±¡æŸ“åˆ†æï¼šæª¢æ¸¬è¨“ç·´è³‡æ–™èˆ‡æ¸¬è©¦è³‡æ–™çš„ 8-gram é‡ç–Šç¨‹åº¦
            </p>
        </div>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆæŠ€è¡“æ–¹æ³•ï¼‰</strong><br><br>
            "To study this we created <strong>Bloom filters containing 8-grams of WebText training set tokens</strong>. To improve recall, strings were normalized to contain only lower-cased alphanumeric words with a single space as a delimiter. The Bloom filters were constructed such that the false positive rate is upper bounded by 1/10â¸. We further verified the low false positive rate by generating 1M strings, of which zero were found by the filter."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šç‚ºäº†ç ”ç©¶é€™å€‹å•é¡Œï¼Œæˆ‘å€‘å‰µå»ºäº†<strong>åŒ…å« WebText è¨“ç·´é›† tokens çš„ 8-grams çš„ Bloom filters</strong>ã€‚ç‚ºäº†æé«˜å¬å›ç‡ï¼Œå­—ä¸²è¢«æ¨™æº–åŒ–ç‚ºåªåŒ…å«å°å¯«å­—æ¯æ•¸å­—è©ï¼Œä»¥å–®å€‹ç©ºæ ¼ä½œç‚ºåˆ†éš”ç¬¦ã€‚Bloom filters çš„å»ºæ§‹ä½¿å¾—èª¤å ±ç‡ä¸Šé™ç‚º 1/10â¸ã€‚æˆ‘å€‘é€šéç”Ÿæˆ 100 è¬å€‹å­—ä¸²é€²ä¸€æ­¥é©—è­‰äº†ä½èª¤å ±ç‡ï¼Œå…¶ä¸­é›¶å€‹è¢«éæ¿¾å™¨æ‰¾åˆ°ã€‚
        </div>

        <div class="technical-detail">
            <h4>ğŸ“ ä»€éº¼æ˜¯ 8-gramï¼Ÿ</h4>
            <p>
                8-gram æ˜¯é€£çºŒ 8 å€‹ tokens çš„åºåˆ—ã€‚ä¾‹å¦‚ï¼š
            </p>
            <pre>
æ–‡æœ¬ï¼šã€ŒThe quick brown fox jumps over the lazy dogã€
8-gram ç¯„ä¾‹ï¼š
- "The quick brown fox jumps over the lazy"
- "quick brown fox jumps over the lazy dog"</pre>
            <p>
                å¦‚æœæ¸¬è©¦é›†çš„æŸå€‹ 8-gram ä¹Ÿå‡ºç¾åœ¨è¨“ç·´é›†ä¸­ï¼Œ
                å°±å¯èƒ½å­˜åœ¨è³‡æ–™æ±¡æŸ“ï¼ˆdata contaminationï¼‰ã€‚
            </p>
        </div>

        <h3>ğŸ“Š æ±¡æŸ“åˆ†æçµæœ</h3>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆçµæœï¼‰</strong><br><br>
            "Common LM datasets' test sets have <strong>between 1-6% overlap with WebText train</strong>, with an average of overlap of 3.2%. Somewhat surprisingly, <strong>many datasets have larger overlaps with their own training splits</strong>, with an average of 5.9% overlap."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šå¸¸è¦‹èªè¨€æ¨¡å‹è³‡æ–™é›†çš„æ¸¬è©¦é›†èˆ‡ WebText è¨“ç·´é›†æœ‰ <strong>1-6% çš„é‡ç–Š</strong>ï¼Œå¹³å‡é‡ç–Šç‡ç‚º 3.2%ã€‚æœ‰äº›ä»¤äººé©šè¨çš„æ˜¯ï¼Œ<strong>è¨±å¤šè³‡æ–™é›†èˆ‡å®ƒå€‘è‡ªå·±çš„è¨“ç·´åˆ†å‰²æœ‰æ›´å¤§çš„é‡ç–Š</strong>ï¼Œå¹³å‡é‡ç–Šç‡ç‚º 5.9%ã€‚
        </div>

        <div class="key-concept">
            <h4>ğŸ’¡ æ„å¤–çš„ç™¼ç¾</h4>
            <p>
                OpenAI ç™¼ç¾äº†ä¸€ä»¶æœ‰è¶£çš„äº‹ï¼š
            </p>
            <ul>
                <li>WebText èˆ‡æ¸¬è©¦é›†çš„é‡ç–Šï¼š<strong>å¹³å‡ 3.2%</strong></li>
                <li>è³‡æ–™é›†è‡ªå·±å…§éƒ¨çš„é‡ç–Šï¼š<strong>å¹³å‡ 5.9%</strong>ï¼ˆæ›´é«˜ï¼ï¼‰</li>
            </ul>
            <p>
                é€™æ„å‘³è‘— GPT-2 çš„æ•ˆèƒ½æå‡<strong>ä¸å¤ªå¯èƒ½</strong>æ˜¯å› ç‚ºè³‡æ–™æ±¡æŸ“ï¼Œ
                åè€Œæ˜¯å…¶ä»–è³‡æ–™é›†æœ¬èº«æœ‰æ›´åš´é‡çš„æ±¡æŸ“å•é¡Œã€‚
            </p>
        </div>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆå…·é«”ä¾‹å­ï¼‰</strong><br><br>
            "For instance, we discovered that the test set of WikiText-103 has an article which is also in the training dataset. Since there are only 60 articles in the test set there is at least an overlap of 1.6%. Potentially more worryingly, <strong>1BW has an overlap of nearly 13.2% with its own training set</strong> according to our procedure."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šä¾‹å¦‚ï¼Œæˆ‘å€‘ç™¼ç¾ WikiText-103 çš„æ¸¬è©¦é›†æœ‰ä¸€ç¯‡æ–‡ç« ä¹Ÿåœ¨è¨“ç·´è³‡æ–™é›†ä¸­ã€‚ç”±æ–¼æ¸¬è©¦é›†ä¸­åªæœ‰ 60 ç¯‡æ–‡ç« ï¼Œå› æ­¤è‡³å°‘æœ‰ 1.6% çš„é‡ç–Šã€‚æ›´ä»¤äººæ“”æ†‚çš„æ˜¯ï¼Œ<strong>1BW èˆ‡å…¶è‡ªå·±çš„è¨“ç·´é›†æœ‰è¿‘ 13.2% çš„é‡ç–Š</strong>ã€‚
        </div>

        <h2>6. Discussionï¼ˆè¨è«–ï¼‰</h2>

        <div class="story-container">
            <p>
                GPT-2 è«–æ–‡çš„è¨è«–éƒ¨åˆ†ç›¸å°ç°¡çŸ­ï¼Œä½†æå‡ºäº†ä¸€äº›é—œéµçš„åæ€ã€‚
            </p>
        </div>

        <h3>ğŸ”„ å–®å‘ vs é›™å‘è¡¨ç¤º</h3>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "Much research has been dedicated to learning (Hill et al., 2016), understanding (Rogers et al., 2018), and critically evaluating (Glockner et al., 2018) (Poliak et al., 2018) the representations and reasoning abilities of models pre-trained on natural language inference datasets (Bowman et al., 2015) (Williams et al., 2017). There has been less work focused on evaluating the innate reasoning abilities language models learn. <strong>It is possible the training data and capacity of GPT-2 is sufficient to overcome the inefficiencies of uni-directional representations demonstrated by BERT</strong> (Devlin et al., 2018)."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šå·²æœ‰å¤§é‡ç ”ç©¶è‡´åŠ›æ–¼å­¸ç¿’ã€ç†è§£å’Œæ‰¹åˆ¤æ€§è©•ä¼°åœ¨è‡ªç„¶èªè¨€æ¨ç†è³‡æ–™é›†ä¸Šé è¨“ç·´çš„æ¨¡å‹çš„è¡¨ç¤ºå’Œæ¨ç†èƒ½åŠ›ã€‚ä½†è¼ƒå°‘å·¥ä½œå°ˆæ³¨æ–¼è©•ä¼°èªè¨€æ¨¡å‹å­¸åˆ°çš„å…§åœ¨æ¨ç†èƒ½åŠ›ã€‚<strong>GPT-2 çš„è¨“ç·´è³‡æ–™å’Œå®¹é‡å¯èƒ½è¶³ä»¥å…‹æœ BERT æ‰€å±•ç¤ºçš„å–®å‘è¡¨ç¤ºçš„ä½æ•ˆç‡</strong>ã€‚
        </div>

        <div class="key-concept">
            <h4>ğŸ’¡ GPT-2 vs BERT çš„è¾¯è«–</h4>
            <p>
                é€™æ®µè©±æš—ç¤ºäº†ä¸€å€‹é‡è¦çˆ­è«–ï¼š
            </p>
            <ul>
                <li><strong>BERT çš„å„ªå‹¢</strong>ï¼šé›™å‘è¡¨ç¤ºï¼Œèƒ½åŒæ™‚çœ‹åˆ°å‰å¾Œæ–‡</li>
                <li><strong>GPT-2 çš„è«–é»</strong>ï¼šå–®å‘ä½†æ›´å¤§è¦æ¨¡ï¼Œå¯èƒ½å½Œè£œæ¶æ§‹åŠ£å‹¢</li>
            </ul>
            <p>
                é€™å€‹è¾¯è«–ä¸€ç›´å»¶çºŒåˆ°ä»Šå¤©ï¼šEncoderï¼ˆBERTï¼‰vs Decoderï¼ˆGPTï¼‰å“ªå€‹æ›´å¥½ï¼Ÿ
            </p>
        </div>

        <h2>7. Conclusionï¼ˆçµè«–ï¼‰</h2>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆå®Œæ•´çµè«–ï¼‰</strong><br><br>
            "When a large language model is trained on a sufficiently large and diverse dataset it is able to perform well across many domains and datasets. <strong>GPT-2 zero-shots to state of the art performance on 7 out of 8 tested language modeling datasets.</strong> The diversity of tasks the model is able to perform in a zero-shot setting suggests that <strong>high-capacity models trained to maximize the likelihood of a sufficiently varied text corpus begin to learn how to perform a surprising amount of tasks without the need for explicit supervision.</strong>"
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šç•¶å¤§å‹èªè¨€æ¨¡å‹åœ¨è¶³å¤ å¤§ä¸”å¤šæ¨£åŒ–çš„è³‡æ–™é›†ä¸Šè¨“ç·´æ™‚ï¼Œå®ƒèƒ½å¤ åœ¨è¨±å¤šé ˜åŸŸå’Œè³‡æ–™é›†ä¸Šè¡¨ç¾è‰¯å¥½ã€‚<strong>GPT-2 åœ¨ 8 å€‹æ¸¬è©¦çš„èªè¨€å»ºæ¨¡è³‡æ–™é›†ä¸­çš„ 7 å€‹ä¸Šé”åˆ° Zero-Shot æœ€å…ˆé€²æ•ˆèƒ½ã€‚</strong>æ¨¡å‹èƒ½å¤ åœ¨ Zero-Shot è¨­å®šä¸‹åŸ·è¡Œçš„ä»»å‹™çš„å¤šæ¨£æ€§è¡¨æ˜ï¼Œ<strong>åœ¨è¶³å¤ å¤šæ¨£åŒ–çš„æ–‡æœ¬èªæ–™åº«ä¸Šè¨“ç·´ä»¥æœ€å¤§åŒ–å¯èƒ½æ€§çš„é«˜å®¹é‡æ¨¡å‹é–‹å§‹å­¸ç¿’å¦‚ä½•åŸ·è¡Œå¤§é‡ä»»å‹™ï¼Œè€Œç„¡éœ€æ˜ç¢ºçš„ç›£ç£ã€‚</strong>
        </div>

        <div class="quote-block">
            ã€Œé«˜å®¹é‡æ¨¡å‹ + å¤šæ¨£åŒ–è³‡æ–™ = ç„¡ç›£ç£å¤šä»»å‹™å­¸ç¿’ã€
            <br><br>
            <span style="font-size: 1rem; opacity: 0.9;">
                â€”â€” GPT-2 çš„æ ¸å¿ƒç™¼ç¾
            </span>
        </div>

        <h2>âš ï¸ GPT-2 çš„é™åˆ¶</h2>

        <div class="story-container">
            <p>
                é›–ç„¶ GPT-2 å–å¾—äº†é©šäººçš„æˆæœï¼Œä½†è«–æ–‡ä¹Ÿèª å¯¦åœ°æŒ‡å‡ºäº†å¤šå€‹é™åˆ¶ã€‚
            </p>
        </div>

        <div class="paper-section">
            <h3>1ï¸âƒ£ ä»ç„¶ Underfit WebText</h3>
            
            <div class="key-concept">
                <p>
                    è«–æ–‡æ¨™é¡Œæåˆ° GPT-2 æ˜¯ 1.5B åƒæ•¸çš„ Transformerï¼Œ
                    <strong>ã€Œä½†ä»ç„¶ underfit WebTextã€</strong>ã€‚
                </p>
                <p>
                    é€™æ„å‘³è‘—ï¼šå³ä½¿æ˜¯æœ€å¤§çš„ GPT-2 æ¨¡å‹ï¼Œ
                    ä¹Ÿé‚„æ²’æœ‰å®Œå…¨å­¸ç¿’ WebText è³‡æ–™é›†çš„æ‰€æœ‰æ¨¡å¼ã€‚
                    <strong>é‚„æœ‰å¾ˆå¤§çš„æå‡ç©ºé–“</strong>ã€‚
                </p>
            </div>

            <h3>2ï¸âƒ£ åœ¨æŸäº›ä»»å‹™ä¸Šä»ç„¶å¾ˆå¼±</h3>
            
            <ul>
                <li><strong>One Billion Word</strong>ï¼šæ•ˆèƒ½é¡¯è‘—å·®æ–¼ SOTA</li>
                <li><strong>CoQA</strong>ï¼š55 F1 vs äººé¡ 89 F1ï¼ˆä»æœ‰å·¨å¤§å·®è·ï¼‰</li>
                <li><strong>ç¿»è­¯</strong>ï¼šé›–ç„¶èƒ½åšï¼Œä½†é ä¸å¦‚å°ˆé–€çš„ç¿»è­¯æ¨¡å‹</li>
            </ul>

            <h3>3ï¸âƒ£ æ½›åœ¨çš„ç¤¾æœƒé¢¨éšª</h3>
            
            <div class="solution">
                <h4>ğŸš¨ åˆ†éšæ®µç™¼å¸ƒç­–ç•¥</h4>
                <p>
                    OpenAI å° GPT-2 æ¡å–äº†ã€Œ<strong>åˆ†éšæ®µç™¼å¸ƒ</strong>ã€ç­–ç•¥ï¼š
                </p>
                <ul>
                    <li>2019å¹´2æœˆï¼šåªç™¼å¸ƒ 117M åƒæ•¸æ¨¡å‹</li>
                    <li>å¹¾å€‹æœˆå¾Œï¼šç™¼å¸ƒ 345M å’Œ 762M</li>
                    <li>2019å¹´11æœˆï¼šæœ€çµ‚ç™¼å¸ƒå®Œæ•´çš„ 1.5B æ¨¡å‹</li>
                </ul>
                <p>
                    <strong>åŸå› </strong>ï¼šæ“”å¿ƒ GPT-2 å¯èƒ½è¢«ç”¨æ–¼ï¼š
                </p>
                <ul>
                    <li>âŒ ç”Ÿæˆå‡æ–°è</li>
                    <li>âŒ å¤§è¦æ¨¡åƒåœ¾éƒµä»¶</li>
                    <li>âŒ é‡£é­šæ”»æ“Š</li>
                    <li>âŒ å†’å……ä»–äººèº«ä»½</li>
                </ul>
                <p>
                    é€™æ˜¯ AI å®‰å…¨é ˜åŸŸçš„ä¸€å€‹é‡Œç¨‹ç¢‘æ±ºå®šï¼Œ
                    å¼•ç™¼äº†é—œæ–¼ã€Œè² è²¬ä»»çš„ AI ç™¼å¸ƒã€çš„å»£æ³›è¨è«–ã€‚
                </p>
            </div>
        </div>

        <h2>ğŸ”® GPT-2 ç‚º GPT-3 é‹ªçš„è·¯</h2>

        <div class="story-container">
            <p>
                GPT-2 è«–æ–‡çµå°¾é›–ç„¶æ²’æœ‰æ˜èªªï¼Œä½†æ‰€æœ‰è·¡è±¡éƒ½æŒ‡å‘ä¸€å€‹æ–¹å‘ï¼š
                <strong>ç¹¼çºŒæ“´å¤§è¦æ¨¡</strong>ã€‚
            </p>
        </div>

        <div class="paradigm-grid">
            <div class="paradigm-card">
                <h4>GPT-2 è­‰æ˜äº†ä»€éº¼ï¼Ÿ</h4>
                <ul>
                    <li>âœ… Zero-Shot Learning å¯è¡Œ</li>
                    <li>âœ… è¦æ¨¡åŒ–æ•ˆæ‡‰æœ‰æ•ˆ</li>
                    <li>âœ… ä»ç„¶ Underfitï¼ˆé‚„æœ‰ç©ºé–“ï¼‰</li>
                </ul>
            </div>
            <div class="paradigm-card">
                <h4>GPT-3 æ‡‰è©²æ€éº¼åšï¼Ÿ</h4>
                <ul>
                    <li>â†’ æ›´å¤§çš„æ¨¡å‹ï¼ˆ175Bï¼‰</li>
                    <li>â†’ æ›´å¤šçš„è³‡æ–™</li>
                    <li>â†’ Few-Shot Learning</li>
                </ul>
            </div>
        </div>

        <div class="timeline" style="margin: 60px 0;">
            <h3 style="text-align: center; margin-bottom: 30px;">ğŸ“ˆ å¾ GPT-2 åˆ°ç¾ä»£ AI</h3>
            <div class="paradigm-grid" style="grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));">
                <div class="paradigm-card">
                    <h4>2019 GPT-2</h4>
                    <p>1.5B åƒæ•¸</p>
                    <p>Zero-Shot</p>
                    <p>7/8 SOTA</p>
                </div>
                <div class="paradigm-card">
                    <h4>2020 GPT-3</h4>
                    <p>175B åƒæ•¸</p>
                    <p><strong>Few-Shot</strong></p>
                    <p>é©šäººçš„é€šç”¨èƒ½åŠ›</p>
                </div>
                <div class="paradigm-card">
                    <h4>2022 InstructGPT</h4>
                    <p>åŸºæ–¼ GPT-3</p>
                    <p><strong>RLHF</strong></p>
                    <p>å°é½Šäººé¡æ„åœ–</p>
                </div>
                <div class="paradigm-card">
                    <h4>2022 ChatGPT</h4>
                    <p>InstructGPT + å°è©±</p>
                    <p>å…¨çƒç¾è±¡</p>
                    <p>1å„„ç”¨æˆ¶ï¼ˆ2å€‹æœˆï¼‰</p>
                </div>
                <div class="paradigm-card">
                    <h4>2023 GPT-4</h4>
                    <p>è¦æ¨¡æœªçŸ¥</p>
                    <p><strong>å¤šæ¨¡æ…‹</strong></p>
                    <p>æ¥è¿‘ AGIï¼Ÿ</p>
                </div>
            </div>
        </div>

        <div class="quote-block">
            ã€ŒGPT-2 ä¸æ˜¯çµ‚é»ï¼Œè€Œæ˜¯ä¸€å€‹èµ·é»ã€‚<br>
            å®ƒè­‰æ˜äº†é€šå¾€é€šç”¨ AI çš„é“è·¯æ˜¯å¯è¡Œçš„ï¼Œ<br>
            åªæ˜¯éœ€è¦æ›´å¤§çš„è¦æ¨¡å’Œæ›´å¥½çš„å°é½Šã€‚ã€
        </div>

        <h2>ğŸ“ æœ€çµ‚åæ€</h2>

        <div class="story-container">
            <h3>GPT-2 çš„æ­·å²åœ°ä½</h3>
            <p>
                å›é¡§ 2019 å¹´ï¼ŒGPT-2 çš„ç™¼å¸ƒéœ‡é©šäº† AI ç¤¾ç¾¤ã€‚
                è¨±å¤šäººè³ªç–‘ï¼šä¸€å€‹èªè¨€æ¨¡å‹æ€éº¼å¯èƒ½åœ¨æ²’æœ‰ä»»å‹™ç‰¹å®šè¨“ç·´çš„æƒ…æ³ä¸‹åŸ·è¡Œå¦‚æ­¤å¤šæ¨£çš„ä»»å‹™ï¼Ÿ
            </p>
            <p>
                ç¾åœ¨ï¼ˆ2026å¹´ï¼‰ï¼Œæˆ‘å€‘å·²ç¶“ç¿’æ…£äº† ChatGPT çš„å¼·å¤§èƒ½åŠ›ã€‚
                ä½†æˆ‘å€‘ä¸æ‡‰å¿˜è¨˜ï¼š<strong>é€™ä¸€åˆ‡å§‹æ–¼ GPT-2 çš„çªç ´</strong>ã€‚
            </p>

            <h3>ä¸‰å€‹é—œéµæ´å¯Ÿ</h3>
            <ol>
                <li>
                    <strong>è¦æ¨¡åŒ–æœ‰æ•ˆ</strong>ï¼š
                    æ›´å¤§çš„æ¨¡å‹ + æ›´å¤šçš„è³‡æ–™ = æ›´å¼·çš„èƒ½åŠ›ï¼ˆå»¶çºŒåˆ° GPT-3ã€GPT-4ï¼‰
                </li>
                <li>
                    <strong>è³‡æ–™å“è³ªé‡è¦</strong>ï¼š
                    WebText çš„ç­–å±•æ–¹æ³•ï¼ˆReddit 3+ karmaï¼‰æ¯” Common Crawl æœ‰æ•ˆ
                </li>
                <li>
                    <strong>Zero-Shot â†’ Few-Shot â†’ Instruction Following</strong>ï¼š
                    GPT-2 é–‹å•Ÿçš„é“è·¯ï¼Œæœ€çµ‚å°è‡´ ChatGPT çš„èª•ç”Ÿ
                </li>
            </ol>

            <h3>çµ¦æœªä¾†çš„å•Ÿç¤º</h3>
            <p>
                GPT-2 è«–æ–‡çš„æœ€å¤§åƒ¹å€¼ä¸åœ¨æ–¼å®ƒé”åˆ°äº† 7/8 SOTAï¼Œ
                è€Œåœ¨æ–¼å®ƒ<strong>è­‰æ˜äº†ä¸€å€‹æ–¹å‘</strong>ï¼š
            </p>
            <p style="font-size: 1.2rem; font-weight: 600; color: #667eea; text-align: center; margin: 30px 0;">
                ã€Œèªè¨€æ¨¡å‹å¯ä»¥æˆç‚ºé€šç”¨çš„å¤šä»»å‹™å­¸ç¿’è€…ï¼Œ<br>
                è€Œé€šå¾€é€™å€‹ç›®æ¨™çš„é“è·¯æ˜¯è¦æ¨¡åŒ–ã€‚ã€
            </p>
        </div>

        <div class="navigation">
            <a href="04-experiments-and-results.html" class="nav-button">â† ä¸Šä¸€ç« ï¼šå¯¦é©—çµæœ</a>
            <a href="index.html" class="nav-button">å›åˆ°ç›®éŒ„</a>
        </div>

        <div class="chapter-summary">
            <h3>ğŸ“ æœ¬ç« é‡é»å›é¡§</h3>
            <ul>
                <li><strong>è³‡æ–™æ±¡æŸ“æª¢æ¸¬</strong>ï¼šä½¿ç”¨ Bloom Filter åˆ†æ 8-gram é‡ç–Š</li>
                <li><strong>æ±¡æŸ“ç‡ä½</strong>ï¼šWebText èˆ‡æ¸¬è©¦é›†å¹³å‡é‡ç–Š 3.2%ï¼ˆå¯æ¥å—ï¼‰</li>
                <li><strong>æ„å¤–ç™¼ç¾</strong>ï¼šå…¶ä»–è³‡æ–™é›†å…§éƒ¨é‡ç–Šæ›´åš´é‡ï¼ˆ5.9%ï¼‰</li>
                <li><strong>ä»ç„¶ Underfit</strong>ï¼šGPT-2 é‚„æ²’å®Œå…¨å­¸ç¿’ WebText</li>
                <li><strong>åˆ†éšæ®µç™¼å¸ƒ</strong>ï¼šè€ƒæ…®ç¤¾æœƒé¢¨éšªçš„è² è²¬ä»» AI ç­–ç•¥</li>
                <li><strong>ç‚º GPT-3 é‹ªè·¯</strong>ï¼šè­‰æ˜è¦æ¨¡åŒ–æœ‰æ•ˆä¸”æœªé£½å’Œ</li>
                <li><strong>æ­·å²åœ°ä½</strong>ï¼šé–‹å•Ÿäº†é€šå¾€ ChatGPT çš„é“è·¯</li>
            </ul>
        </div>

        <div class="key-concept" style="margin-top: 40px; background: #fffbeb; border-left-color: #f59e0b;">
            <h3>ğŸŒŸ å®Œæˆ GPT-2 è«–æ–‡å­¸ç¿’ï¼</h3>
            <p>
                æ­å–œä½ å®Œæˆäº† GPT-2 è«–æ–‡çš„å®Œæ•´è§£æï¼
                ä½ ç¾åœ¨ç†è§£äº†ï¼š
            </p>
            <ul>
                <li>âœ… Zero-Shot Learning çš„ç†è«–èˆ‡å¯¦è¸</li>
                <li>âœ… WebText è³‡æ–™é›†çš„æ§‹å»ºæ–¹æ³•</li>
                <li>âœ… Byte-Level BPE çš„æŠ€è¡“ç´°ç¯€</li>
                <li>âœ… å››å€‹æ¨¡å‹è¦æ¨¡çš„å¯¦é©—è¨­è¨ˆ</li>
                <li>âœ… 7/8 SOTA çš„é©šäººçµæœ</li>
                <li>âœ… è¦æ¨¡åŒ–æ•ˆæ‡‰èˆ‡æœªä¾†æ–¹å‘</li>
            </ul>
            <p style="margin-top: 20px;">
                <strong>ä¸‹ä¸€æ­¥ï¼š</strong>
                å»ºè­°é–±è®€ <a href="../gpt3-tutorial/index.html">GPT-3 è«–æ–‡</a>ï¼Œ
                çœ‹çœ‹ OpenAI å¦‚ä½•å°‡ GPT-2 çš„æ´å¯Ÿæ“´å±•åˆ° 175B åƒæ•¸ï¼
            </p>
        </div>
    </div>
</body>
</html>

