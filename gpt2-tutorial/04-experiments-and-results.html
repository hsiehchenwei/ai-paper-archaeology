<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT-2 ç¬¬4ç« ï¼šå¯¦é©—çµæœèˆ‡æ•ˆèƒ½è©•ä¼° - Zero-Shot çš„å¨åŠ›</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="hero-section" style="background-image: url('images/performance_scaling.png'); height: 70vh;">
        <div class="hero-overlay"></div>
        <div class="hero-content">
            <h1>7 out of 8 SOTA</h1>
            <p class="hero-subtitle">åœ¨ 8 å€‹æ¸¬è©¦é›†ä¸­ï¼Œ7 å€‹é”åˆ°ç•¶æ™‚æœ€ä½³æ•ˆèƒ½</p>
            <p class="hero-meta">GPT-2 è«–æ–‡æ·±åº¦è§£æ Â· ç¬¬ 4 ç« </p>
        </div>
    </div>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <a href="index.html">GPT-2 æ•™å­¸</a>
            <span>/</span>
            <span class="current">ç¬¬ 4 ç« </span>
        </div>

        <h2>3. Experimentsï¼ˆå¯¦é©—ï¼‰</h2>

        <div class="story-container">
            <p class="drop-cap">
                è¨“ç·´å®Œå››å€‹ä¸åŒè¦æ¨¡çš„ GPT-2 æ¨¡å‹å¾Œï¼ŒOpenAI é€²è¡Œäº†å»£æ³›çš„è©•ä¼°ã€‚
                é—œéµå•é¡Œæ˜¯ï¼š<strong>Zero-Shot è¨­å®šä¸‹ï¼ŒGPT-2 èƒ½è¡¨ç¾å¤šå¥½ï¼Ÿ</strong>
            </p>
            <div class="section-divider"><span>âœ¦</span></div>
            <p>
                ç­”æ¡ˆä»¤äººéœ‡é©šï¼šåœ¨<strong>æ²’æœ‰ä»»ä½•ä»»å‹™ç‰¹å®šè¨“ç·´</strong>çš„æƒ…æ³ä¸‹ï¼Œ
                GPT-2 åœ¨ 8 å€‹èªè¨€æ¨¡å‹æ¸¬è©¦é›†ä¸­çš„ 7 å€‹é”åˆ°äº†ç•¶æ™‚çš„æœ€ä½³æ•ˆèƒ½ï¼ˆSOTAï¼‰ã€‚
            </p>
        </div>

        <h3>3.1. Language Modelingï¼ˆèªè¨€å»ºæ¨¡ï¼‰</h3>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "We report our main results in Table 3 using invertible de-tokenizers which remove as many of these tokenization / pre-processing artifacts as possible. Since these de-tokenizers are invertible, we can still calculate the log probability of a dataset and they can be thought of as a simple form of domain adaptation. We observe <strong>gains of 2.5 to 5 perplexity for GPT-2 with these de-tokenizers</strong>."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šæˆ‘å€‘åœ¨è¡¨ 3 ä¸­å ±å‘Šä¸»è¦çµæœï¼Œä½¿ç”¨å¯é€†çš„å»åˆ†è©å™¨ï¼Œå„˜å¯èƒ½ç§»é™¤é€™äº›åˆ†è©/é è™•ç†çš„ç—•è·¡ã€‚ç”±æ–¼é€™äº›å»åˆ†è©å™¨æ˜¯å¯é€†çš„ï¼Œæˆ‘å€‘ä»ç„¶å¯ä»¥è¨ˆç®—è³‡æ–™é›†çš„å°æ•¸æ©Ÿç‡ï¼Œå®ƒå€‘å¯ä»¥è¢«è¦–ç‚ºä¸€ç¨®ç°¡å–®çš„é ˜åŸŸé©æ‡‰å½¢å¼ã€‚æˆ‘å€‘è§€å¯Ÿåˆ° <strong>GPT-2 ä½¿ç”¨é€™äº›å»åˆ†è©å™¨å¾Œï¼Œperplexity æ”¹å–„äº† 2.5 åˆ° 5 å€‹é»</strong>ã€‚
        </div>

        <h3>ğŸ“Š Table 3: Zero-Shot çµæœç¸½è¦½</h3>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "<strong>WebText LMs transfer well across domains and datasets, improving the state of the art on 7 out of the 8 datasets in a zero-shot setting.</strong> Large improvements are noticed on small datasets such as Penn Treebank and WikiText-2 which have only 1 to 2 million training tokens. Large improvements are also noticed on datasets created to measure long-term dependencies like LAMBADA (Paperno et al., 2016) and the Children's Book Test (Hill et al., 2015)."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼š<strong>WebText èªè¨€æ¨¡å‹åœ¨ä¸åŒé ˜åŸŸå’Œè³‡æ–™é›†ä¸Šé·ç§»æ•ˆæœè‰¯å¥½ï¼Œåœ¨ Zero-Shot è¨­å®šä¸‹æ”¹å–„äº† 8 å€‹è³‡æ–™é›†ä¸­çš„ 7 å€‹çš„æœ€å…ˆé€²çµæœã€‚</strong>åœ¨å°å‹è³‡æ–™é›†ï¼ˆå¦‚åªæœ‰ 100-200 è¬è¨“ç·´ tokens çš„ Penn Treebank å’Œ WikiText-2ï¼‰ä¸Šè§€å¯Ÿåˆ°å¤§å¹…æ”¹å–„ã€‚åœ¨ç”¨æ–¼æ¸¬é‡é•·æœŸä¾è³´æ€§çš„è³‡æ–™é›†ï¼ˆå¦‚ LAMBADA å’Œ Children's Book Testï¼‰ä¸Šä¹Ÿè§€å¯Ÿåˆ°å¤§å¹…æ”¹å–„ã€‚
        </div>

        <table class="results-table">
            <thead>
                <tr>
                    <th rowspan="2">æ¨¡å‹</th>
                    <th colspan="2">LAMBADA</th>
                    <th colspan="2">CBT</th>
                    <th colspan="4">å…¶ä»–è³‡æ–™é›†</th>
                    <th colspan="2">å¤§å‹è³‡æ–™é›†</th>
                </tr>
                <tr>
                    <th>PPL</th>
                    <th>ACC</th>
                    <th>CN</th>
                    <th>NE</th>
                    <th>Wiki2</th>
                    <th>PTB</th>
                    <th>enwik8</th>
                    <th>text8</th>
                    <th>Wiki103</th>
                    <th>1BW</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>SOTAï¼ˆè¨“ç·´å‰ï¼‰</strong></td>
                    <td>99.8</td>
                    <td>59.23</td>
                    <td>85.7</td>
                    <td>82.3</td>
                    <td>39.14</td>
                    <td>46.54</td>
                    <td class="best-result">0.99</td>
                    <td class="best-result">1.08</td>
                    <td>18.3</td>
                    <td class="best-result">21.8</td>
                </tr>
                <tr>
                    <td><strong>GPT-2 (117M)</strong></td>
                    <td>35.13</td>
                    <td>45.99</td>
                    <td>87.65</td>
                    <td>83.4</td>
                    <td>29.41</td>
                    <td>65.85</td>
                    <td>1.16</td>
                    <td>1.17</td>
                    <td>37.50</td>
                    <td>75.20</td>
                </tr>
                <tr>
                    <td><strong>GPT-2 (345M)</strong></td>
                    <td>15.60</td>
                    <td>55.48</td>
                    <td>92.35</td>
                    <td>87.1</td>
                    <td>22.76</td>
                    <td>47.33</td>
                    <td>1.01</td>
                    <td>1.06</td>
                    <td>26.37</td>
                    <td>55.72</td>
                </tr>
                <tr>
                    <td><strong>GPT-2 (762M)</strong></td>
                    <td>10.87</td>
                    <td>60.12</td>
                    <td>93.45</td>
                    <td>88.0</td>
                    <td>19.93</td>
                    <td>40.31</td>
                    <td class="best-result">0.97</td>
                    <td>1.02</td>
                    <td>22.05</td>
                    <td>44.575</td>
                </tr>
                <tr>
                    <td><strong>GPT-2 (1.5B) â­</strong></td>
                    <td class="best-result">8.63</td>
                    <td class="best-result">63.24</td>
                    <td class="best-result">93.30</td>
                    <td class="best-result">89.05</td>
                    <td class="best-result">18.34</td>
                    <td class="best-result">35.76</td>
                    <td class="best-result">0.93</td>
                    <td class="best-result">0.98</td>
                    <td class="best-result">17.48</td>
                    <td>42.16</td>
                </tr>
            </tbody>
        </table>

        <div class="key-concept">
            <h4>ğŸ“Š è¡¨æ ¼è§£è®€</h4>
            <ul>
                <li><strong>ç¶ è‰²æ ¼å­</strong>ï¼šé”åˆ°æˆ–è¶…è¶Šç•¶æ™‚ SOTA çš„çµæœ</li>
                <li><strong>PPL (Perplexity)</strong>ï¼šè¶Šä½è¶Šå¥½ï¼ˆä»£è¡¨é æ¸¬è¶Šæº–ç¢ºï¼‰</li>
                <li><strong>ACC (Accuracy)</strong>ï¼šè¶Šé«˜è¶Šå¥½ï¼ˆä»£è¡¨ç­”å°ç‡ï¼‰</li>
                <li><strong>7/8 SOTA</strong>ï¼šé™¤äº† 1BWï¼ˆOne Billion Wordï¼‰å¤–ï¼Œå…¨éƒ¨é”åˆ°æœ€ä½³</li>
            </ul>
        </div>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆ1BW å¤±æ•—åŸå› ï¼‰</strong><br><br>
            "Our model is still significantly worse than prior work on the One Billion Word Benchmark (Chelba et al., 2013). This is likely due to a combination of it being both <strong>the largest dataset and having some of the most destructive pre-processing</strong> - 1BW's <strong>sentence level shuffling removes all long-range structure</strong>."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šæˆ‘å€‘çš„æ¨¡å‹åœ¨ One Billion Word Benchmark ä¸Šä»ç„¶æ˜é¡¯å·®æ–¼å…ˆå‰å·¥ä½œã€‚é€™å¯èƒ½æ˜¯å› ç‚ºå®ƒæ—¢æ˜¯<strong>æœ€å¤§çš„è³‡æ–™é›†ï¼Œåˆæœ‰ä¸€äº›æœ€å…·ç ´å£æ€§çš„é è™•ç†</strong> - 1BW çš„<strong>å¥å­ç´šåˆ¥æ´—ç‰Œç§»é™¤äº†æ‰€æœ‰é•·è·é›¢çµæ§‹</strong>ã€‚
        </div>

        <h3>3.2. Children's Book Test (CBT)</h3>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "The Children's Book Test (CBT) (Hill et al., 2015) was created to examine the performance of LMs on different categories of words: named entities, nouns, verbs, and prepositions. Rather than reporting perplexity as an evaluation metric, CBT reports accuracy on an automatically constructed cloze test where the task is to predict which of 10 possible choices for an omitted word is correct."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šå…’ç«¥è®€ç‰©æ¸¬è©¦ï¼ˆCBTï¼‰ç”¨æ–¼æª¢é©—èªè¨€æ¨¡å‹åœ¨ä¸åŒè©é¡ä¸Šçš„è¡¨ç¾ï¼šå‘½åå¯¦é«”ã€åè©ã€å‹•è©å’Œä»‹è©ã€‚CBT ä¸å ±å‘Š perplexityï¼Œè€Œæ˜¯å ±å‘Šè‡ªå‹•æ§‹å»ºçš„å®Œå½¢å¡«ç©ºæ¸¬è©¦çš„æº–ç¢ºç‡ï¼Œä»»å‹™æ˜¯é æ¸¬ 10 å€‹å¯èƒ½é¸é …ä¸­å“ªå€‹è¢«çœç•¥çš„è©æ˜¯æ­£ç¢ºçš„ã€‚
        </div>

        <div class="example">
            <h4>ğŸŒ° CBT ç¯„ä¾‹</h4>
            <p><strong>æ–‡æœ¬ï¼š</strong>"The quick brown fox jumps over the lazy ___."</p>
            <p><strong>é¸é …ï¼š</strong>dog, cat, bird, mouse, elephant, lion, tiger, bear, wolf, deer</p>
            <p><strong>ç­”æ¡ˆï¼š</strong>dogï¼ˆèªè¨€æ¨¡å‹éœ€è¦å¾ 10 å€‹é¸é …ä¸­é¸å‡ºæœ€åˆç†çš„ï¼‰</p>
        </div>

        <div class="paradigm-grid">
            <div class="paradigm-card">
                <h4>Common Nouns (CN)</h4>
                <p>GPT-2: <strong>93.30%</strong></p>
                <p>SOTA: 85.7%</p>
                <p>æå‡ <span style="color: #10b981; font-weight: 600;">+7.6%</span></p>
            </div>
            <div class="paradigm-card">
                <h4>Named Entities (NE)</h4>
                <p>GPT-2: <strong>89.05%</strong></p>
                <p>SOTA: 82.3%</p>
                <p>æå‡ <span style="color: #10b981; font-weight: 600;">+6.75%</span></p>
            </div>
        </div>

        <h3>3.3. LAMBADA</h3>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "The LAMBADA dataset (Paperno et al., 2016) tests the ability of systems to model long-range dependencies in text. The task is to predict the final word of sentences which require reading a paragraph of context."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šLAMBADA è³‡æ–™é›†æ¸¬è©¦ç³»çµ±å»ºæ¨¡æ–‡æœ¬é•·è·é›¢ä¾è³´æ€§çš„èƒ½åŠ›ã€‚ä»»å‹™æ˜¯é æ¸¬éœ€è¦é–±è®€ä¸€æ®µä¸Šä¸‹æ–‡çš„å¥å­çš„æœ€å¾Œä¸€å€‹è©ã€‚
        </div>

        <div class="key-concept">
            <h4>ğŸ’¡ é©šäººçš„æ”¹å–„</h4>
            <p>
                GPT-2 åœ¨ LAMBADA ä¸Šçš„è¡¨ç¾æ¥µå…¶çªå‡ºï¼š
            </p>
            <ul>
                <li><strong>Perplexity</strong>ï¼š99.8 â†’ 8.63ï¼ˆ<span style="color: #10b981; font-weight: 600;">æ”¹å–„ 91.2 é»</span>ï¼‰</li>
                <li><strong>Accuracy</strong>ï¼š59.23% â†’ 63.24%</li>
            </ul>
            <p>
                é€™è­‰æ˜ GPT-2 æœ‰èƒ½åŠ›<strong>ç†è§£ä¸¦åˆ©ç”¨é•·è·é›¢ä¸Šä¸‹æ–‡</strong>ï¼Œ
                é€™å°æ–¼ Zero-Shot Learning è‡³é—œé‡è¦ã€‚
            </p>
        </div>

        <h3>3.4. Winograd Schema Challenge</h3>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "The Winograd Schema challenge (Levesque et al., 2012) was constructed to measure the capability of a system to perform commonsense reasoning by measuring its ability to resolve ambiguities in text. Recently Trinh & Le (2018) demonstrated significant progress on this challenge using LMs, by predicting the resolution of the ambiguity with higher probability. We follow their problem formulation and visualize the performance of our models with both full and partial scoring techniques in Figure 3. <strong>GPT-2 improves state of the art accuracy by 7%, achieving 70.70%.</strong> The dataset is quite small with only 273 examples so we recommend reading Trichelair et al. (2018) to help contextualize this result."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šWinograd Schema æŒ‘æˆ°ç”¨æ–¼æ¸¬é‡ç³»çµ±é€éè§£æ±ºæ–‡æœ¬ä¸­çš„æ­§ç¾©ä¾†åŸ·è¡Œå¸¸è­˜æ¨ç†çš„èƒ½åŠ›ã€‚Trinh & Le (2018) æœ€è¿‘ä½¿ç”¨èªè¨€æ¨¡å‹åœ¨æ­¤æŒ‘æˆ°ä¸Šå–å¾—äº†é¡¯è‘—é€²å±•ï¼Œé€éé æ¸¬æ›´é«˜æ©Ÿç‡çš„æ­§ç¾©è§£æ±ºæ–¹æ¡ˆã€‚æˆ‘å€‘éµå¾ªä»–å€‘çš„å•é¡Œè¡¨è¿°ï¼Œä¸¦åœ¨åœ– 3 ä¸­ä½¿ç”¨å®Œæ•´å’Œéƒ¨åˆ†è©•åˆ†æŠ€è¡“è¦–è¦ºåŒ–æ¨¡å‹çš„è¡¨ç¾ã€‚<strong>GPT-2 å°‡æœ€å…ˆé€²çš„æº–ç¢ºç‡æé«˜äº† 7%ï¼Œé”åˆ° 70.70%ã€‚</strong>è³‡æ–™é›†ç›¸ç•¶å°ï¼Œåªæœ‰ 273 å€‹ç¯„ä¾‹ï¼Œå› æ­¤æˆ‘å€‘å»ºè­°é–±è®€ Trichelair et al. (2018) ä»¥å¹«åŠ©ç†è§£æ­¤çµæœã€‚
        </div>

        <div class="example">
            <h4>ğŸŒ° Winograd Schema ç¯„ä¾‹</h4>
            <p><strong>å¥å­ï¼š</strong>"The trophy doesn't fit into the brown suitcase because <strong>it</strong> is too large."</p>
            <p><strong>å•é¡Œï¼š</strong>"it" æŒ‡çš„æ˜¯ä»€éº¼ï¼Ÿ</p>
            <p><strong>é¸é …ï¼š</strong>trophyï¼ˆçç›ƒï¼‰æˆ– suitcaseï¼ˆè¡Œæç®±ï¼‰</p>
            <p><strong>ç­”æ¡ˆï¼š</strong>trophyï¼ˆéœ€è¦å¸¸è­˜æ¨ç†ï¼šçç›ƒå¤ªå¤§æ‰è£ä¸é€²è¡Œæç®±ï¼‰</p>
        </div>

        <h3>3.5. Reading Comprehensionï¼ˆé–±è®€ç†è§£ï¼‰</h3>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "The Conversation Question Answering dataset (CoQA) Reddy et al. (2018) consists of documents from 7 different domains paired with natural language dialogues between a question asker and a question answerer about the document. CoQA tests reading comprehension capabilities and also the ability of models to answer questions that depend on conversation history (such as 'Why?')."
            <br><br>
            "Greedy decoding from GPT-2 when conditioned on a document, the history of the associated conversation, and a final token A: <strong>achieves 55 F1 on the development set</strong>. This matches or exceeds the performance of 3 out of 4 baseline systems <strong>without using the 127,000+ manually collected question answer pairs those baselines were trained on</strong>."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šå°è©±å¼å•ç­”è³‡æ–™é›†ï¼ˆCoQAï¼‰åŒ…å«ä¾†è‡ª 7 å€‹ä¸åŒé ˜åŸŸçš„æ–‡ä»¶ï¼Œé…å°è‘—é—œæ–¼æ–‡ä»¶çš„å•é¡Œè€…å’Œå›ç­”è€…ä¹‹é–“çš„è‡ªç„¶èªè¨€å°è©±ã€‚CoQA æ¸¬è©¦é–±è®€ç†è§£èƒ½åŠ›ï¼Œä»¥åŠæ¨¡å‹å›ç­”ä¾è³´å°è©±æ­·å²çš„å•é¡Œï¼ˆå¦‚ã€Œç‚ºä»€éº¼ï¼Ÿã€ï¼‰çš„èƒ½åŠ›ã€‚
            <br><br>
            ç•¶ GPT-2 åœ¨çµ¦å®šæ–‡ä»¶ã€ç›¸é—œå°è©±æ­·å²å’Œæœ€çµ‚ token A: çš„æ¢ä»¶ä¸‹é€²è¡Œè²ªå©ªè§£ç¢¼æ™‚ï¼Œ<strong>åœ¨é–‹ç™¼é›†ä¸Šé”åˆ° 55 F1</strong>ã€‚é€™åŒ¹é…æˆ–è¶…è¶Šäº† 4 å€‹åŸºæº–ç³»çµ±ä¸­çš„ 3 å€‹çš„è¡¨ç¾ï¼Œ<strong>è€Œæ²’æœ‰ä½¿ç”¨é‚£äº›åŸºæº–ç³»çµ±è¨“ç·´æ™‚ç”¨çš„ 127,000+ å€‹äººå·¥æ”¶é›†çš„å•ç­”å°</strong>ã€‚
        </div>

        <div class="key-concept">
            <h4>ğŸ’¡ Zero-Shot çš„é©šäººè¡¨ç¾</h4>
            <p>
                <strong>55 F1</strong> çœ‹ä¼¼ä¸é«˜ï¼ˆäººé¡æ˜¯ 89 F1ï¼‰ï¼Œä½†è€ƒæ…®åˆ°ï¼š
            </p>
            <ul>
                <li>âœ… <strong>å®Œå…¨æ²’æœ‰è¨“ç·´è³‡æ–™</strong>ï¼š0 å€‹å•ç­”ç¯„ä¾‹</li>
                <li>âœ… <strong>è¶…è¶Š 3/4 åŸºæº–ç³»çµ±</strong>ï¼šé‚£äº›ç³»çµ±ç”¨äº† 127,000+ ç¯„ä¾‹</li>
                <li>âœ… <strong>åªé èªè¨€å»ºæ¨¡</strong>ï¼šæ²’æœ‰ä»»å‹™ç‰¹å®šçš„æ¶æ§‹æˆ–è¨“ç·´</li>
            </ul>
            <p>
                é€™è­‰æ˜äº† Zero-Shot Learning çš„å¯è¡Œæ€§ã€‚
            </p>
        </div>

        <h3>3.6. Summarizationï¼ˆæ‘˜è¦ï¼‰</h3>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "We test GPT-2's ability to perform summarization on the CNN and Daily Mail dataset (Nallapati et al., 2016). To induce summarization behavior we add the text <strong>TL;DR:</strong> after the article and generate 100 tokens with Top-k random sampling (Fan et al., 2018) with k=2 which reduces repetition and encourages more abstractive summaries than greedy decoding. We use the first 3 generated sentences in these 100 tokens as the summary."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šæˆ‘å€‘åœ¨ CNN å’Œ Daily Mail è³‡æ–™é›†ä¸Šæ¸¬è©¦ GPT-2 çš„æ‘˜è¦èƒ½åŠ›ã€‚ç‚ºäº†èª˜å°æ‘˜è¦è¡Œç‚ºï¼Œæˆ‘å€‘åœ¨æ–‡ç« å¾Œæ·»åŠ æ–‡å­— <strong>TL;DR:</strong>ï¼ˆå¤ªé•·äº†ï¼›æ²’è®€ï¼‰ï¼Œä¸¦ä½¿ç”¨ Top-k éš¨æ©ŸæŠ½æ¨£ï¼ˆk=2ï¼‰ç”Ÿæˆ 100 å€‹ tokensï¼Œé€™æ¸›å°‘äº†é‡è¤‡ä¸¦æ¯”è²ªå©ªè§£ç¢¼æ›´é¼“å‹µæŠ½è±¡å¼æ‘˜è¦ã€‚æˆ‘å€‘ä½¿ç”¨é€™ 100 å€‹ tokens ä¸­çš„å‰ 3 å€‹ç”Ÿæˆå¥å­ä½œç‚ºæ‘˜è¦ã€‚
        </div>

        <div class="example">
            <h4>ğŸŒ° Prompt Engineering çš„æ—©æœŸç¯„ä¾‹</h4>
            <p>
                é€™æ˜¯ <strong>Prompt Engineering</strong> çš„ä¸€å€‹ç¶“å…¸ä¾‹å­ï¼
                OpenAI ç™¼ç¾åªè¦åœ¨æ–‡ç« å¾ŒåŠ ä¸Š <code>TL;DR:</code>ï¼Œ
                GPT-2 å°±æœƒã€Œç†è§£ã€é€™æ˜¯è¦åšæ‘˜è¦ï¼Œä¸¦è‡ªå‹•ç”Ÿæˆæ‘˜è¦å…§å®¹ã€‚
            </p>
            <p>
                é€™å€‹ç™¼ç¾ç›´æ¥å•Ÿç™¼äº† GPT-3 çš„ Few-Shot Prompting æ–¹æ³•ã€‚
            </p>
        </div>

        <h3>3.7. Translationï¼ˆç¿»è­¯ï¼‰</h3>

        <div class="story-container">
            <p>
                ç¿»è­¯æ˜¯ä¸€å€‹å›°é›£çš„ä»»å‹™ï¼Œéœ€è¦é›™èªçŸ¥è­˜ã€‚
                GPT-2 ä¸»è¦åœ¨è‹±æ–‡è³‡æ–™ä¸Šè¨“ç·´ï¼Œèƒ½ç¿»è­¯å—ï¼Ÿ
            </p>
        </div>

        <div style="margin: 40px 0; text-align: center;">
            <img src="images/translation_struggle_comic.png" alt="GPT-2 ç¿»è­¯å›°é›£çš„å¹½é»˜æ¼«ç•«" style="max-width: 100%; height: auto; border-radius: 12px; box-shadow: 0 8px 30px rgba(0,0,0,0.12);">
            <p class="image-caption">
                ç¿»è­¯ä»»å‹™å° GPT-2 ä¾†èªªä¸¦ä¸ç°¡å–®ï¼šè‹±ç¿»æ³•åªæœ‰ 5 BLEUï¼Œæ³•ç¿»è‹±ç¨å¥½ä¸€äº›é”åˆ° 11.5 BLEU
            </p>
        </div>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "We test whether GPT-2 has begun to learn how to translate from one language to another. In order to help it infer that this is the desired task, we condition the language model on a context of example pairs of the format <strong>english sentence = french sentence</strong> and then after a final prompt of <strong>english sentence =</strong> we sample from the model with greedy decoding and use the first generated sentence as the translation."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šæˆ‘å€‘æ¸¬è©¦ GPT-2 æ˜¯å¦é–‹å§‹å­¸ç¿’å¦‚ä½•å¾ä¸€ç¨®èªè¨€ç¿»è­¯åˆ°å¦ä¸€ç¨®èªè¨€ã€‚ç‚ºäº†å¹«åŠ©å®ƒæ¨æ–·é€™æ˜¯æ‰€éœ€çš„ä»»å‹™ï¼Œæˆ‘å€‘åœ¨èªè¨€æ¨¡å‹ä¸Šæ¢ä»¶åŒ–ä¸€å€‹ç¯„ä¾‹å°çš„ä¸Šä¸‹æ–‡ï¼Œæ ¼å¼ç‚º <strong>english sentence = french sentence</strong>ï¼Œç„¶å¾Œåœ¨æœ€å¾Œçš„æç¤º <strong>english sentence =</strong> ä¹‹å¾Œï¼Œæˆ‘å€‘ä½¿ç”¨è²ªå©ªè§£ç¢¼å¾æ¨¡å‹ä¸­æ¡æ¨£ï¼Œä¸¦ä½¿ç”¨ç¬¬ä¸€å€‹ç”Ÿæˆçš„å¥å­ä½œç‚ºç¿»è­¯ã€‚
        </div>

        <div class="technical-detail">
            <h4>ğŸ“ ç¿»è­¯çš„ Prompt æ ¼å¼</h4>
            <p>GPT-2 ä½¿ç”¨ç°¡å–®çš„æ ¼å¼ä¾†æç¤ºç¿»è­¯ä»»å‹™ï¼š</p>
            <pre>
Contextï¼ˆä¸Šä¸‹æ–‡ç¯„ä¾‹ï¼‰:
english sentence = french sentence
english sentence = french sentence
...

Promptï¼ˆè¦ç¿»è­¯çš„å¥å­ï¼‰:
english sentence = [æ¨¡å‹ç”Ÿæˆç¿»è­¯]</pre>
            <p>
                é€™ç¨®æ ¼å¼åˆ©ç”¨äº† Table 1 ä¸­æåˆ°çš„ WebText è£¡è‡ªç„¶å‡ºç¾çš„ç¿»è­¯ç¤ºç¯„ã€‚
            </p>
        </div>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆçµæœï¼‰</strong><br><br>
            "On the WMT-14 English-French test set, <strong>GPT-2 gets 5 BLEU</strong>, which is slightly worse than a word-by-word substitution with a bilingual lexicon inferred in previous work on unsupervised word translation (Conneau et al., 2017b). On the WMT-14 French-English test set, GPT-2 is able to leverage its very strong English language model to perform significantly better, achieving <strong>11.5 BLEU</strong>."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šåœ¨ WMT-14 è‹±æ³•æ–‡æ¸¬è©¦é›†ä¸Šï¼Œ<strong>GPT-2 å¾—åˆ° 5 BLEU</strong>ï¼Œç•¥å·®æ–¼ä½¿ç”¨å…ˆå‰ç„¡ç›£ç£è©ç¿»è­¯å·¥ä½œä¸­æ¨æ–·çš„é›™èªè©å…¸é€²è¡Œé€è©æ›¿æ›ã€‚åœ¨ WMT-14 æ³•è‹±æ–‡æ¸¬è©¦é›†ä¸Šï¼ŒGPT-2 èƒ½å¤ åˆ©ç”¨å…¶éå¸¸å¼·å¤§çš„è‹±æ–‡èªè¨€æ¨¡å‹è¡¨ç¾å¾—æ›´å¥½ï¼Œé”åˆ° <strong>11.5 BLEU</strong>ã€‚
        </div>

        <div class="paradigm-grid">
            <div class="paradigm-card">
                <h4>è‹±æ–‡ â†’ æ³•æ–‡</h4>
                <p>5 BLEU</p>
                <p style="color: #ef4444;">âŒ è¡¨ç¾è¼ƒå·®</p>
                <p style="font-size: 0.85rem;">ç•¥å·®æ–¼é€è©æ›¿æ›</p>
            </div>
            <div class="paradigm-card">
                <h4>æ³•æ–‡ â†’ è‹±æ–‡</h4>
                <p>11.5 BLEU</p>
                <p style="color: #f59e0b;">âš ï¸ ç¨å¥½ä½†ä»ä¸ç†æƒ³</p>
                <p style="font-size: 0.85rem;">å—ç›Šæ–¼å¼·å¤§çš„è‹±æ–‡ LM</p>
            </div>
            <div class="paradigm-card" style="opacity: 0.6;">
                <h4>å°ˆæ¥­ç¿»è­¯ç³»çµ±</h4>
                <p>30-40 BLEU</p>
                <p style="color: #10b981;">âœ… SOTA</p>
                <p style="font-size: 0.85rem;">éœ€è¦å¤§é‡é›™èªè³‡æ–™</p>
            </div>
        </div>

        <div class="key-concept">
            <h4>ğŸ’¡ ç‚ºä»€éº¼ç¿»è­¯æ•ˆæœä¸å¥½ï¼Ÿ</h4>
            <ul>
                <li>âŒ <strong>è¨“ç·´è³‡æ–™ä¸å¹³è¡¡</strong>ï¼šWebText ä¸»è¦æ˜¯è‹±æ–‡ï¼Œæ³•æ–‡å…§å®¹å¾ˆå°‘</li>
                <li>âŒ <strong>ç¼ºä¹é›™èªå°é½Š</strong>ï¼šæ²’æœ‰å°ˆé–€çš„é›™èªå¹³è¡Œèªæ–™</li>
                <li>âŒ <strong>Zero-Shot çš„é™åˆ¶</strong>ï¼šç¿»è­¯éœ€è¦æ›´å¤šçš„ç¤ºç¯„</li>
                <li>âœ… <strong>ä½†ä»æœ‰æ„ç¾©</strong>ï¼šè­‰æ˜ GPT-2 èƒ½ã€Œæ¨æ–·ã€ç¿»è­¯ä»»å‹™</li>
            </ul>
            <p>
                é›–ç„¶æ•ˆæœä¸å¥½ï¼Œä½†é€™è­‰æ˜äº† GPT-2 ç¢ºå¯¦å¾è‡ªç„¶èªè¨€ä¸­å­¸åˆ°äº†ä¸€äº›ç¿»è­¯èƒ½åŠ›â€”â€”
                å³ä½¿é€™ä¸æ˜¯å®ƒçš„å¼·é …ã€‚
            </p>
        </div>

        <h3>3.8. Question Answering - Natural Questions ç²¾é¸ç¯„ä¾‹</h3>

        <div class="story-container">
            <p>
                è«–æ–‡çš„ Table 5 å±•ç¤ºäº† GPT-2 åœ¨ Natural Questions è³‡æ–™é›†ä¸Š
                <strong>æœ€æœ‰ä¿¡å¿ƒçš„ 30 å€‹ç­”æ¡ˆ</strong>ã€‚
                è®“æˆ‘å€‘çœ‹çœ‹å®ƒç­”å°äº†ä»€éº¼ã€ç­”éŒ¯äº†ä»€éº¼ã€‚
            </p>
        </div>

        <div style="margin: 40px 0; text-align: center;">
            <img src="images/table5_qa_comic.png" alt="GPT-2 å•ç­”ä¿¡å¿ƒåº¦æ¼«ç•«" style="max-width: 100%; height: auto; border-radius: 12px; box-shadow: 0 8px 30px rgba(0,0,0,0.12);">
            <p class="image-caption">
                GPT-2 çš„éåº¦è‡ªä¿¡ï¼šå³ä½¿ç­”éŒ¯ä¹Ÿå¾ˆæœ‰ä¿¡å¿ƒï¼
            </p>
        </div>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆTable 5 èªªæ˜ï¼‰</strong><br><br>
            "The 30 most confident answers generated by GPT-2 on the development set of Natural Questions sorted by their probability according to GPT-2. <strong>None of these questions appear in WebText</strong> according to the procedure described in Section 4."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šGPT-2 åœ¨ Natural Questions é–‹ç™¼é›†ä¸Šç”Ÿæˆçš„ 30 å€‹æœ€æœ‰ä¿¡å¿ƒçš„ç­”æ¡ˆï¼ŒæŒ‰ç…§ GPT-2 çš„æ©Ÿç‡æ’åºã€‚<strong>æ ¹æ“šç¬¬ 4 ç¯€æè¿°çš„ç¨‹åºï¼Œé€™äº›å•é¡Œéƒ½æ²’æœ‰å‡ºç¾åœ¨ WebText ä¸­</strong>ã€‚
        </div>

        <table class="results-table" style="font-size: 0.85rem;">
            <thead>
                <tr>
                    <th style="width: 50%;">å•é¡Œ</th>
                    <th style="width: 25%;">GPT-2 çš„ç­”æ¡ˆ</th>
                    <th style="width: 10%;">æ­£ç¢º?</th>
                    <th style="width: 15%;">ä¿¡å¿ƒåº¦</th>
                </tr>
            </thead>
            <tbody>
                <tr class="best-result">
                    <td>Who wrote the book the origin of species?</td>
                    <td>Charles Darwin</td>
                    <td>âœ…</td>
                    <td>83.4%</td>
                </tr>
                <tr class="best-result">
                    <td>Who is the founder of the ubuntu project?</td>
                    <td>Mark Shuttleworth</td>
                    <td>âœ…</td>
                    <td>82.0%</td>
                </tr>
                <tr class="best-result">
                    <td>Who came up with the theory of relativity?</td>
                    <td>Albert Einstein</td>
                    <td>âœ…</td>
                    <td>76.4%</td>
                </tr>
                <tr class="best-result">
                    <td>Who took the first steps on the moon in 1969?</td>
                    <td>Neil Armstrong</td>
                    <td>âœ…</td>
                    <td>66.8%</td>
                </tr>
                <tr class="best-result">
                    <td>Who was the author of the art of war?</td>
                    <td>Sun Tzu</td>
                    <td>âœ…</td>
                    <td>59.6%</td>
                </tr>
                <tr style="background: #fee2e2;">
                    <td>Largest state in the us by land mass?</td>
                    <td>California âŒ</td>
                    <td>âŒ</td>
                    <td>59.2%</td>
                </tr>
                <tr style="background: #fee2e2;">
                    <td>Green algae is an example of which type of reproduction?</td>
                    <td>parthenogenesis âŒ</td>
                    <td>âŒ</td>
                    <td>56.5%</td>
                </tr>
                <tr style="background: #fee2e2;">
                    <td>Who plays ser davos in game of thrones?</td>
                    <td>Peter Dinklage âŒ</td>
                    <td>âŒ</td>
                    <td>52.1%</td>
                </tr>
            </tbody>
        </table>

        <div class="key-concept">
            <h4>ğŸ“Š Table 5 åˆ†æ</h4>
            <ul>
                <li>âœ… <strong>æ­£ç¢ºç­”æ¡ˆ</strong>ï¼šå¤§å¤šæ•¸æ˜¯å¸¸è­˜æ€§å•é¡Œï¼ˆåäººã€æ­·å²äº‹ä»¶ï¼‰</li>
                <li>âŒ <strong>éŒ¯èª¤ç­”æ¡ˆ</strong>ï¼š
                    <ul>
                        <li>Largest US state â†’ Californiaï¼ˆæ‡‰è©²æ˜¯ Alaskaï¼‰</li>
                        <li>Ser Davos æ¼”å“¡ â†’ Peter Dinklageï¼ˆæ‡‰è©²æ˜¯ Liam Cunninghamï¼ŒPeter Dinklage æ¼” Tyrionï¼‰</li>
                    </ul>
                </li>
                <li>ğŸ’¡ <strong>è§€å¯Ÿ</strong>ï¼š
                    <ul>
                        <li>GPT-2 å°ã€ŒçŸ¥åã€çš„ç­”æ¡ˆæ›´æœ‰ä¿¡å¿ƒ</li>
                        <li>ä½†æœ‰æ™‚æœƒæ··æ·†ç›¸é—œæ¦‚å¿µï¼ˆå¦‚åŒä¸€éƒ¨åŠ‡çš„ä¸åŒæ¼”å“¡ï¼‰</li>
                        <li>å³ä½¿ç­”éŒ¯ï¼Œä¿¡å¿ƒåº¦ä»ç„¶å¾ˆé«˜ï¼ˆ>50%ï¼‰</li>
                    </ul>
                </li>
            </ul>
        </div>

        <div class="example">
            <h4>ğŸŒ° ç‚ºä»€éº¼æœƒç­”éŒ¯ï¼Ÿ</h4>
            <p><strong>ä¾‹å­ï¼šLargest US state</strong></p>
            <p>
                GPT-2 å›ç­” California è€Œé Alaskaï¼Œå¯èƒ½æ˜¯å› ç‚ºï¼š
            </p>
            <ul>
                <li>California åœ¨è¨“ç·´è³‡æ–™ä¸­å‡ºç¾é »ç‡æ›´é«˜</li>
                <li>ã€ŒCalifornia is the largest...ã€çš„å¥å‹æ›´å¸¸è¦‹ï¼ˆå¦‚äººå£æœ€å¤šã€ç¶“æ¿Ÿè¦æ¨¡æœ€å¤§ï¼‰</li>
                <li>Zero-Shot ç¼ºä¹ç²¾ç¢ºçš„äº‹å¯¦æª¢æŸ¥æ©Ÿåˆ¶</li>
            </ul>
            <p>
                é€™é¡éŒ¯èª¤ç›´æ¥ä¿ƒæˆäº†å¾ŒçºŒçš„æ”¹é€²ï¼šInstructGPT çš„ RLHF è¨“ç·´ï¼Œ
                ä»¥åŠ GPT-4 å°äº‹å¯¦æº–ç¢ºæ€§çš„æå‡ã€‚
            </p>
        </div>

        <div class="example">
            <h4>ğŸŒ° Prompt Engineering çš„æ—©æœŸç¯„ä¾‹</h4>
            <p>
                é€™æ˜¯ <strong>Prompt Engineering</strong> çš„ä¸€å€‹ç¶“å…¸ä¾‹å­ï¼
                OpenAI ç™¼ç¾åªè¦åœ¨æ–‡ç« å¾ŒåŠ ä¸Š <code>TL;DR:</code>ï¼Œ
                GPT-2 å°±æœƒã€Œç†è§£ã€é€™æ˜¯è¦åšæ‘˜è¦ï¼Œä¸¦è‡ªå‹•ç”Ÿæˆæ‘˜è¦å…§å®¹ã€‚
            </p>
            <p>
                é€™å€‹ç™¼ç¾ç›´æ¥å•Ÿç™¼äº† GPT-3 çš„ Few-Shot Prompting æ–¹æ³•ã€‚
            </p>
        </div>

        <h2>ğŸ“ˆ è¦æ¨¡åŒ–æ•ˆæ‡‰ï¼šLog-Linear é—œä¿‚</h2>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆæœ€é‡è¦çš„ç™¼ç¾ï¼‰</strong><br><br>
            "Our experiments, while much noisier across tasks, suggest similar trends hold for sub-tasks of an objective and <strong>continue into the 1B+ parameter regime</strong>."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šæˆ‘å€‘çš„å¯¦é©—é›–ç„¶åœ¨ä¸åŒä»»å‹™ä¸Šæ›´å˜ˆé›œï¼Œä½†æš—ç¤ºé¡ä¼¼çš„è¶¨å‹¢å°ç›®æ¨™çš„å­ä»»å‹™æˆç«‹ï¼Œä¸¦<strong>å»¶çºŒåˆ° 10 å„„ä»¥ä¸Šåƒæ•¸çš„ç¯„åœ</strong>ã€‚
        </div>

        <div class="story-container">
            <p>
                é€™æ˜¯ GPT-2 è«–æ–‡æœ€é—œéµçš„ç™¼ç¾ä¹‹ä¸€ï¼š
            </p>
            <p style="font-size: 1.2rem; font-weight: 600; color: #667eea; text-align: center; margin: 30px 0;">
                ã€Œæ¨¡å‹è¶Šå¤§ï¼ŒZero-Shot æ•ˆèƒ½è¶Šå¥½ï¼Œ<br>è€Œä¸”é€™å€‹é—œä¿‚æŒçºŒåˆ° 1.5B åƒæ•¸ä»æœªé£½å’Œã€‚ã€
            </p>
            <p>
                é€™ç›´æ¥ä¿ƒæˆäº† GPT-3 çš„èª•ç”Ÿï¼ˆ175B åƒæ•¸ï¼‰ã€‚
                OpenAI æ„è­˜åˆ°ï¼šå¦‚æœè¶¨å‹¢æŒçºŒï¼Œæ›´å¤§çš„æ¨¡å‹æœƒå¸¶ä¾†æ›´é©šäººçš„èƒ½åŠ›ã€‚
            </p>
        </div>

        <div class="navigation">
            <a href="03-model-architecture.html" class="nav-button">â† ä¸Šä¸€ç« ï¼šæ¨¡å‹æ¶æ§‹</a>
            <a href="05-discussion-and-conclusion.html" class="nav-button">ä¸‹ä¸€ç« ï¼šè¨è«–èˆ‡çµè«– â†’</a>
        </div>

        <div class="chapter-summary">
            <h3>ğŸ“ æœ¬ç« é‡é»å›é¡§</h3>
            <ul>
                <li><strong>7/8 SOTA</strong>ï¼šåœ¨ 8 å€‹æ¸¬è©¦é›†ä¸­ï¼Œ7 å€‹é”åˆ°ç•¶æ™‚æœ€ä½³æ•ˆèƒ½ï¼ˆZero-Shotï¼‰</li>
                <li><strong>LAMBADA</strong>ï¼šPerplexity å¾ 99.8 â†’ 8.63ï¼Œé©šäººæ”¹å–„</li>
                <li><strong>CBT</strong>ï¼šCommon Nouns 93.30%ï¼ŒNamed Entities 89.05%</li>
                <li><strong>CoQA</strong>ï¼š55 F1ï¼Œè¶…è¶Š 3/4 åŸºæº–ï¼ˆæ²’ç”¨è¨“ç·´è³‡æ–™ï¼‰</li>
                <li><strong>Winograd</strong>ï¼š70.70% æº–ç¢ºç‡ï¼Œæå‡ 7%</li>
                <li><strong>Summarization</strong>ï¼šç”¨ "TL;DR:" èª˜å°æ‘˜è¦è¡Œç‚º</li>
                <li><strong>è¦æ¨¡åŒ–æ•ˆæ‡‰</strong>ï¼šæŒçºŒåˆ° 1.5B åƒæ•¸ä»æœªé£½å’Œï¼Œç‚º GPT-3 é‹ªè·¯</li>
            </ul>
        </div>

        <div class="key-concept" style="margin-top: 40px;">
            <h3>ğŸ¯ ç‚ºä»€éº¼é€™äº›çµæœå¾ˆé‡è¦ï¼Ÿ</h3>
            <p>
                GPT-2 è­‰æ˜äº†ä¸‰ä»¶äº‹ï¼š
            </p>
            <ol>
                <li><strong>Zero-Shot Learning å¯è¡Œ</strong>ï¼šç„¡éœ€ä»»å‹™ç‰¹å®šè¨“ç·´å³å¯åŸ·è¡Œä»»å‹™</li>
                <li><strong>è¦æ¨¡åŒ–æœ‰æ•ˆ</strong>ï¼šæ›´å¤§çš„æ¨¡å‹ = æ›´å¼·çš„èƒ½åŠ›</li>
                <li><strong>Prompt Engineering é‡è¦</strong>ï¼šç°¡å–®çš„æç¤ºï¼ˆå¦‚ "TL;DR:"ï¼‰å°±èƒ½å¼•å°è¡Œç‚º</li>
            </ol>
            <p>
                é€™ä¸‰å€‹æ´å¯Ÿç›´æ¥å°è‡´äº† GPT-3 çš„ Few-Shot Learningã€
                InstructGPT çš„ RLHFï¼Œä»¥åŠç¾ä»£ ChatGPT çš„èª•ç”Ÿã€‚
            </p>
        </div>
    </div>
</body>
</html>

