<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT-2 ç¬¬2ç« ï¼šæ–¹æ³•è«–èˆ‡è¨“ç·´è³‡æ–™ - WebText çš„èª•ç”Ÿ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="hero-section">
        <div class="hero-overlay"></div>
        <div class="hero-content">
            <h1>WebTextï¼šå“è³ªå‹æ–¼æ•¸é‡</h1>
            <p class="hero-subtitle">800 è¬å€‹ç¶²é ï¼Œ40GB çš„ç²¾å¿ƒç­–å±•</p>
            <p class="hero-meta">GPT-2 è«–æ–‡æ·±åº¦è§£æ Â· ç¬¬ 2 ç« </p>
        </div>
    </div>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <a href="index.html">GPT-2 æ•™å­¸</a>
            <span>/</span>
            <span class="current">ç¬¬ 2 ç« </span>
        </div>

        <h2>2. Approachï¼ˆæ–¹æ³•è«–ï¼‰</h2>

        <div class="story-container">
            <p class="drop-cap">
                GPT-2 çš„æ ¸å¿ƒæ–¹æ³•ç°¡å–®å»å¼·å¤§ï¼š<strong>èªè¨€å»ºæ¨¡</strong>ï¼ˆLanguage Modelingï¼‰ã€‚
                ä½†èˆ‡å‚³çµ±æ–¹æ³•ä¸åŒçš„æ˜¯ï¼ŒGPT-2 è©¦åœ–è­‰æ˜ä¸€ä»¶äº‹ï¼š
                å¦‚æœæ¨¡å‹å¤ å¤§ã€è³‡æ–™å¤ å¥½ï¼Œèªè¨€æ¨¡å‹æœ¬èº«å°±èƒ½æˆç‚ºå¤šä»»å‹™å­¸ç¿’è€…ã€‚
            </p>
            
            <div class="section-divider"><span>âœ¦</span></div>
        </div>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "At the core of our approach is language modeling. Language modeling is usually framed as unsupervised distribution estimation from a set of examples (xâ‚, xâ‚‚, ..., xâ‚™) each composed of variable length sequences of symbols (sâ‚, sâ‚‚, ..., sâ‚™)."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šæˆ‘å€‘æ–¹æ³•çš„æ ¸å¿ƒæ˜¯èªè¨€å»ºæ¨¡ã€‚èªè¨€å»ºæ¨¡é€šå¸¸è¢«æ¡†æ¶ç‚ºå¾ä¸€çµ„ç¯„ä¾‹ä¸­é€²è¡Œç„¡ç›£ç£çš„åˆ†ä½ˆä¼°è¨ˆï¼Œæ¯å€‹ç¯„ä¾‹ç”±å¯è®Šé•·åº¦çš„ç¬¦è™Ÿåºåˆ—çµ„æˆã€‚
        </div>

        <div class="paper-section">
            <h3>ğŸ¯ æ ¸å¿ƒæ€æƒ³ï¼šä»»å‹™æ¢ä»¶åŒ–</h3>
            
            <div class="original-quote">
                <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                "Learning to perform a single task can be expressed in a probabilistic framework as estimating a conditional distribution p(output|input). Since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed. That is, it should model p(output|input, task)."
                <br><br>
                <strong>ç¿»è­¯</strong>ï¼šå­¸ç¿’åŸ·è¡Œå–®ä¸€ä»»å‹™å¯ä»¥ç”¨æ©Ÿç‡æ¡†æ¶è¡¨é”ç‚ºä¼°è¨ˆæ¢ä»¶åˆ†ä½ˆ p(è¼¸å‡º|è¼¸å…¥)ã€‚ç”±æ–¼é€šç”¨ç³»çµ±æ‡‰è©²èƒ½åŸ·è¡Œå¤šç¨®ä¸åŒä»»å‹™ï¼Œå³ä½¿å°æ–¼ç›¸åŒè¼¸å…¥ï¼Œå®ƒæ‡‰è©²ä¸åƒ…æ ¹æ“šè¼¸å…¥é€²è¡Œæ¢ä»¶åŒ–ï¼Œé‚„è¦æ ¹æ“šè¦åŸ·è¡Œçš„ä»»å‹™é€²è¡Œæ¢ä»¶åŒ–ã€‚ä¹Ÿå°±æ˜¯èªªï¼Œå®ƒæ‡‰è©²å»ºæ¨¡ p(è¼¸å‡º|è¼¸å…¥ï¼Œä»»å‹™)ã€‚
            </div>

            <div style="margin: 40px 0; text-align: center;">
                <img src="images/multitask_formula.png" alt="p(output|input,task) å¤šä»»å‹™å­¸ç¿’å…¬å¼è¦–è¦ºåŒ–" style="max-width: 100%; height: auto; border-radius: 12px; box-shadow: 0 8px 30px rgba(0,0,0,0.12);">
                <p class="image-caption">
                    å¤šä»»å‹™å­¸ç¿’çš„æ ¸å¿ƒå…¬å¼ï¼šp(output|input, task) - æ ¹æ“šè¼¸å…¥å’Œä»»å‹™å…±åŒæ±ºå®šè¼¸å‡º
                </p>
            </div>

            <div class="key-concept">
                <h4>ğŸ’¡ é©å‘½æ€§æ´å¯Ÿ</h4>
                <p>
                    å‚³çµ±æ–¹æ³•ç”¨ä¸åŒçš„<strong>æ¨¡å‹æ¶æ§‹</strong>æˆ–<strong>è¨“ç·´æ¼”ç®—æ³•</strong>ä¾†æŒ‡å®šä»»å‹™ã€‚
                    GPT-2 çš„å‰µæ–°ï¼š<strong>ç”¨è‡ªç„¶èªè¨€ä¾†æŒ‡å®šä»»å‹™</strong>ï¼
                </p>
            </div>

            <div class="example">
                <h4>ğŸŒ° è«–æ–‡ä¸­çš„ç¯„ä¾‹</h4>
                <div class="original-quote">
                    <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                    "For example, a translation training example can be written as the sequence (translate to french, english text, french text). Likewise, a reading comprehension training example can be written as (answer the question, document, question, answer)."
                    <br><br>
                    <strong>ç¿»è­¯</strong>ï¼šä¾‹å¦‚ï¼Œç¿»è­¯è¨“ç·´ç¯„ä¾‹å¯ä»¥å¯«æˆåºåˆ—ï¼ˆç¿»è­¯æˆæ³•æ–‡ï¼Œè‹±æ–‡æ–‡æœ¬ï¼Œæ³•æ–‡æ–‡æœ¬ï¼‰ã€‚åŒæ¨£åœ°ï¼Œé–±è®€ç†è§£è¨“ç·´ç¯„ä¾‹å¯ä»¥å¯«æˆï¼ˆå›ç­”å•é¡Œï¼Œæ–‡ä»¶ï¼Œå•é¡Œï¼Œç­”æ¡ˆï¼‰ã€‚
                </div>
                
                <p>é€™ç¨®æ ¼å¼è®“èªè¨€æ¨¡å‹èƒ½å¾è‡ªç„¶èªè¨€ä¸­ã€Œæ¨æ–·ã€å‡ºè¦åŸ·è¡Œä»€éº¼ä»»å‹™ï¼</p>
            </div>

            <h3>ğŸ”¬ ç†è«–åŸºç¤ï¼šç„¡ç›£ç£åŒ…å«ç›£ç£</h3>
            
            <div class="original-quote">
                <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆé—œéµç†è«–ï¼‰</strong><br><br>
                "Language modeling is also able to, in principle, learn the tasks of McCann et al. (2018) without the need for explicit supervision of which symbols are the outputs to be predicted. Since the supervised objective is the same as the unsupervised objective but only evaluated on a subset of the sequence, <strong>the global minimum of the unsupervised objective is also the global minimum of the supervised objective</strong>."
                <br><br>
                <strong>ç¿»è­¯</strong>ï¼šèªè¨€å»ºæ¨¡åŸå‰‡ä¸Šä¹Ÿèƒ½å¤ å­¸ç¿’ McCann ç­‰äººï¼ˆ2018ï¼‰çš„ä»»å‹™ï¼Œè€Œç„¡éœ€æ˜ç¢ºç›£ç£å“ªäº›ç¬¦è™Ÿæ˜¯è¦é æ¸¬çš„è¼¸å‡ºã€‚ç”±æ–¼ç›£ç£ç›®æ¨™èˆ‡ç„¡ç›£ç£ç›®æ¨™ç›¸åŒï¼Œåªæ˜¯åœ¨åºåˆ—çš„å­é›†ä¸Šé€²è¡Œè©•ä¼°ï¼Œ<strong>å› æ­¤ç„¡ç›£ç£ç›®æ¨™çš„å…¨åŸŸæœ€å°å€¼ä¹Ÿæ˜¯ç›£ç£ç›®æ¨™çš„å…¨åŸŸæœ€å°å€¼</strong>ã€‚
            </div>

            <div class="technical-detail">
                <h4>ğŸ“ é€™æ„å‘³è‘—ä»€éº¼ï¼Ÿ</h4>
                <p>
                    æƒ³åƒä½ æœ‰ä¸€å€‹åºåˆ—ï¼š<code>ã€Œç¿»è­¯æˆæ³•æ–‡ï¼šI love AI â†’ J'aime l'IAã€</code>
                </p>
                <ul>
                    <li><strong>ç„¡ç›£ç£ç›®æ¨™</strong>ï¼šé æ¸¬æ•´å€‹åºåˆ—çš„æ¯å€‹å­—</li>
                    <li><strong>ç›£ç£ç›®æ¨™</strong>ï¼šåªé æ¸¬ <code>J'aime l'IA</code> é€™éƒ¨åˆ†</li>
                </ul>
                <p>
                    å¦‚æœæ¨¡å‹å­¸æœƒé æ¸¬æ•´å€‹åºåˆ—ï¼ˆç„¡ç›£ç£ï¼‰ï¼Œå®ƒè‡ªç„¶å°±å­¸æœƒäº†ç¿»è­¯ä»»å‹™ï¼ˆç›£ç£ï¼‰ï¼
                    é€™æ˜¯ GPT-2 çš„ç†è«–åŸºç¤ã€‚
                </p>
            </div>

            <div class="original-quote">
                <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆå¯¦é©—é©—è­‰ï¼‰</strong><br><br>
                "Preliminary experiments confirmed that sufficiently large language models are able to perform multitask learning in this toy-ish setup but learning is much slower than in explicitly supervised approaches."
                <br><br>
                <strong>ç¿»è­¯</strong>ï¼šåˆæ­¥å¯¦é©—è­‰å¯¦ï¼Œè¶³å¤ å¤§çš„èªè¨€æ¨¡å‹èƒ½å¤ åœ¨é€™ç¨®ç©å…·å¼è¨­å®šä¸­åŸ·è¡Œå¤šä»»å‹™å­¸ç¿’ï¼Œä½†å­¸ç¿’é€Ÿåº¦æ¯”æ˜ç¢ºç›£ç£çš„æ–¹æ³•æ…¢å¾—å¤šã€‚
            </div>
        </div>

        <h2>2.1. Training Datasetï¼ˆè¨“ç·´è³‡æ–™é›†ï¼‰</h2>

        <img src="images/webtext_visualization.png" alt="WebText è³‡æ–™é›†è¦–è¦ºåŒ–" class="image-full">
        <p class="image-caption">WebText è³‡æ–™é›†ï¼šå¾ Reddit ç²å¾— 3+ upvotes çš„é€£çµä¸­ç­–å±•é«˜å“è³ªå…§å®¹</p>

        <div class="story-container">
            <h3>ğŸ“š ç‚ºä»€éº¼éœ€è¦æ–°è³‡æ–™é›†ï¼Ÿ</h3>
            <p>
                åœ¨ GPT-2 ä¹‹å‰ï¼Œå¤§å¤šæ•¸èªè¨€æ¨¡å‹éƒ½åœ¨<strong>å–®ä¸€é ˜åŸŸ</strong>çš„æ–‡æœ¬ä¸Šè¨“ç·´ï¼š
            </p>
        </div>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "Most prior work trained language models on a single domain of text, such as news articles (Jozefowicz et al., 2016), Wikipedia (Merity et al., 2016), or fiction books (Kiros et al., 2015). <strong>Our approach motivates building as large and diverse a dataset as possible</strong> in order to collect natural language demonstrations of tasks in as varied of domains and contexts as possible."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šå¤§å¤šæ•¸å…ˆå‰å·¥ä½œåœ¨å–®ä¸€é ˜åŸŸçš„æ–‡æœ¬ä¸Šè¨“ç·´èªè¨€æ¨¡å‹ï¼Œä¾‹å¦‚æ–°èæ–‡ç« ã€Wikipedia æˆ–å°èªªæ›¸ç±ã€‚<strong>æˆ‘å€‘çš„æ–¹æ³•éœ€è¦å»ºç«‹å„˜å¯èƒ½å¤§ä¸”å¤šæ¨£åŒ–çš„è³‡æ–™é›†</strong>ï¼Œä»¥ä¾¿åœ¨å„˜å¯èƒ½å¤šæ¨£åŒ–çš„é ˜åŸŸå’Œæƒ…å¢ƒä¸­æ”¶é›†ä»»å‹™çš„è‡ªç„¶èªè¨€ç¤ºç¯„ã€‚
        </div>

        <div class="paper-section">
            <h3>ğŸŒ Common Crawl çš„å•é¡Œ</h3>
            
            <div class="original-quote">
                <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                "A promising source of diverse and nearly unlimited text is web scrapes such as Common Crawl. While these archives are many orders of magnitude larger than current language modeling datasets, <strong>they have significant data quality issues</strong>. Trinh & Le (2018) used Common Crawl in their work on commonsense reasoning but noted a large amount of documents 'whose content are mostly unintelligible'."
                <br><br>
                <strong>ç¿»è­¯</strong>ï¼šç¶²é çˆ¬èŸ²ï¼ˆå¦‚ Common Crawlï¼‰æ˜¯å¤šæ¨£åŒ–ä¸”å¹¾ä¹ç„¡é™æ–‡æœ¬çš„ç†æƒ³ä¾†æºã€‚é›–ç„¶é€™äº›æª”æ¡ˆæ¯”ç›®å‰çš„èªè¨€å»ºæ¨¡è³‡æ–™é›†å¤§å¥½å¹¾å€‹æ•¸é‡ç´šï¼Œ<strong>ä½†å®ƒå€‘æœ‰é¡¯è‘—çš„è³‡æ–™å“è³ªå•é¡Œ</strong>ã€‚Trinh & Leï¼ˆ2018ï¼‰åœ¨å¸¸è­˜æ¨ç†å·¥ä½œä¸­ä½¿ç”¨äº† Common Crawlï¼Œä½†æ³¨æ„åˆ°å¤§é‡æ–‡ä»¶ã€Œå…§å®¹å¤§å¤šé›£ä»¥ç†è§£ã€ã€‚
            </div>

            <div class="solution">
                <h4>âœ¨ GPT-2 çš„è§£æ±ºæ–¹æ¡ˆï¼šWebText</h4>
                
                <div class="original-quote">
                    <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆé—œéµç­–ç•¥ï¼‰</strong><br><br>
                    "To address this, we created a new web scrape which emphasizes document quality. To do this we only scraped web pages which have been <strong>curated/filtered by humans</strong>. Manually filtering a full web scrape would be exceptionally expensive so as a starting point, we scraped all outbound links from Reddit, a social media platform, <strong>which received at least 3 karma</strong>. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny."
                    <br><br>
                    <strong>ç¿»è­¯</strong>ï¼šç‚ºäº†è§£æ±ºé€™å€‹å•é¡Œï¼Œæˆ‘å€‘å‰µå»ºäº†ä¸€å€‹å¼·èª¿æ–‡ä»¶å“è³ªçš„æ–°ç¶²é çˆ¬èŸ²ã€‚ç‚ºæ­¤ï¼Œæˆ‘å€‘åªçˆ¬å–<strong>ç¶“éäººé¡ç­–å±•/éæ¿¾</strong>çš„ç¶²é ã€‚æ‰‹å‹•éæ¿¾å®Œæ•´çš„ç¶²é çˆ¬èŸ²æœƒéå¸¸æ˜‚è²´ï¼Œå› æ­¤ä½œç‚ºèµ·é»ï¼Œæˆ‘å€‘çˆ¬å–äº† Redditï¼ˆä¸€å€‹ç¤¾äº¤åª’é«”å¹³å°ï¼‰ä¸Šæ‰€æœ‰<strong>è‡³å°‘ç²å¾— 3 å€‹ karmaï¼ˆè®šï¼‰</strong>çš„å¤–éƒ¨é€£çµã€‚é€™å¯ä»¥è¢«è¦–ç‚ºä¸€ç¨®å•Ÿç™¼å¼æŒ‡æ¨™ï¼Œè¡¨æ˜å…¶ä»–ä½¿ç”¨è€…æ˜¯å¦ç™¼ç¾è©²é€£çµæœ‰è¶£ã€æœ‰æ•™è‚²æ„ç¾©ï¼Œæˆ–åªæ˜¯å¥½ç¬‘ã€‚
                </div>

                <div class="key-concept">
                    <h4>ğŸ’¡ WebText çš„å“è³ªæ§åˆ¶ç­–ç•¥</h4>
                    <p>
                        é€™æ˜¯ä¸€å€‹è°æ˜çš„ã€Œç¾¤çœ¾å¤–åŒ…å“è³ªéæ¿¾ã€ç­–ç•¥ï¼š
                    </p>
                    <ul>
                        <li>âœ… Reddit æ˜¯ä¸€å€‹ç”±äººé¡ç­–å±•å…§å®¹çš„å¹³å°</li>
                        <li>âœ… è‡³å°‘ 3 å€‹è®šä»£è¡¨è‡³å°‘æœ‰äººè¦ºå¾—é€™å…§å®¹å€¼å¾—ä¸€çœ‹</li>
                        <li>âœ… é€™æ¯” Common Crawl çš„ã€Œä»€éº¼éƒ½æŠ“ã€è¦ç²¾æº–å¾—å¤š</li>
                        <li>âœ… è€Œä¸”æ¯”æ‰‹å‹•æ¨™è¨»ä¾¿å®œå¤ªå¤š</li>
                    </ul>
                </div>
            </div>

            <h3>ğŸ“Š WebText è³‡æ–™é›†è¦æ¨¡</h3>
            
            <div class="original-quote">
                <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                "The resulting dataset, WebText, contains the text subset of these 45 million links. To extract the text from HTML responses we use a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper content extractors. All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains <strong>slightly over 8 million documents for a total of 40 GB of text</strong>."
                <br><br>
                <strong>ç¿»è­¯</strong>ï¼šç”¢ç”Ÿçš„è³‡æ–™é›† WebText åŒ…å«é€™ 4500 è¬å€‹é€£çµçš„æ–‡æœ¬å­é›†ã€‚ç‚ºäº†å¾ HTML å›æ‡‰ä¸­æå–æ–‡æœ¬ï¼Œæˆ‘å€‘çµåˆä½¿ç”¨äº† Dragnet å’Œ Newspaper å…§å®¹æå–å™¨ã€‚æœ¬æ–‡ä¸­å‘ˆç¾çš„æ‰€æœ‰çµæœéƒ½ä½¿ç”¨ WebText çš„åˆæ­¥ç‰ˆæœ¬ï¼Œè©²ç‰ˆæœ¬ä¸åŒ…å« 2017 å¹´ 12 æœˆä¹‹å¾Œå‰µå»ºçš„é€£çµï¼Œåœ¨å»é‡å’Œä¸€äº›å•Ÿç™¼å¼æ¸…ç†å¾Œï¼ŒåŒ…å«<strong>ç•¥è¶…é 800 è¬ä»½æ–‡ä»¶ï¼Œç¸½è¨ˆ 40 GB æ–‡æœ¬</strong>ã€‚
            </div>

            <div class="paradigm-grid">
                <div class="paradigm-card">
                    <h4>4500 è¬é€£çµ</h4>
                    <p>Reddit ä¸Šç²å¾— 3+ karma çš„å¤–éƒ¨é€£çµç¸½æ•¸</p>
                </div>
                <div class="paradigm-card">
                    <h4>800 è¬æ–‡ä»¶</h4>
                    <p>å»é‡å’Œæ¸…ç†å¾Œçš„æœ€çµ‚æ–‡ä»¶æ•¸é‡</p>
                </div>
                <div class="paradigm-card">
                    <h4>40 GB æ–‡æœ¬</h4>
                    <p>æœ€çµ‚è¨“ç·´è³‡æ–™é›†çš„å¤§å°</p>
                </div>
                <div class="paradigm-card">
                    <h4>ç§»é™¤ Wikipedia</h4>
                    <p>é¿å…èˆ‡æ¸¬è©¦é›†é‡ç–Šé€ æˆã€Œè³‡æ–™æ±¡æŸ“ã€</p>
                </div>
            </div>

            <div class="original-quote">
                <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆå»é™¤ Wikipediaï¼‰</strong><br><br>
                "We also <strong>removed all Wikipedia documents from WebText</strong> since it is a common data source for other datasets and could complicate analysis due to overlapping training data with test evaluation tasks."
                <br><br>
                <strong>ç¿»è­¯</strong>ï¼šæˆ‘å€‘é‚„å¾ WebText ä¸­<strong>ç§»é™¤äº†æ‰€æœ‰ Wikipedia æ–‡ä»¶</strong>ï¼Œå› ç‚ºå®ƒæ˜¯å…¶ä»–è³‡æ–™é›†çš„å¸¸è¦‹è³‡æ–™ä¾†æºï¼Œå¯èƒ½å› è¨“ç·´è³‡æ–™èˆ‡æ¸¬è©¦è©•ä¼°ä»»å‹™é‡ç–Šè€Œä½¿åˆ†æè®Šå¾—è¤‡é›œã€‚
            </div>

            <h3>ğŸ“‹ Table 1: WebText ä¸­çš„è‡ªç„¶èªè¨€ç¤ºç¯„</h3>
            
            <div class="story-container">
                <p>
                    WebText çš„ä¸€å€‹é—œéµç‰¹æ€§æ˜¯ï¼šå®ƒåŒ…å«äº†<strong>è‡ªç„¶ç™¼ç”Ÿçš„ä»»å‹™ç¤ºç¯„</strong>ã€‚
                    è«–æ–‡ä¸­èˆ‰äº†ä¸€å€‹ç¿»è­¯çš„ä¾‹å­ï¼š
                </p>
            </div>

            <div style="margin: 40px 0; text-align: center;">
                <img src="images/table1_natural_demo.png" alt="WebText ä¸­çš„è‡ªç„¶èªè¨€ç¿»è­¯ç¤ºç¯„" style="max-width: 100%; height: auto; border-radius: 12px; box-shadow: 0 8px 30px rgba(0,0,0,0.12);">
                <p class="image-caption">
                    WebText ä¸­è‡ªç„¶å‡ºç¾çš„ç¿»è­¯ç¤ºç¯„ - GPT-2 å¾é€™äº›è‡ªç„¶æ ¼å¼å­¸ç¿’åŸ·è¡Œä»»å‹™
                </p>
            </div>

            <div class="original-quote">
                <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆTable 1ï¼‰</strong><br><br>
                "Brevet Sans Garantie Du Gouvernement", translated to English: "Patented without government warranty"
                <br><br>
                <strong>èªªæ˜</strong>ï¼šTable 1 å±•ç¤ºäº†åœ¨ WebText è¨“ç·´é›†ä¸­è‡ªç„¶å‡ºç¾çš„è‹±æ³•æ–‡ç¿»è­¯ç¤ºç¯„ã€‚é€™ç¨®è‡ªç„¶èªè¨€æ ¼å¼çš„ç¿»è­¯ç¯„ä¾‹ï¼Œè®“èªè¨€æ¨¡å‹èƒ½å¤ å¾ä¸­ã€Œæ¨æ–·ã€å‡ºç¿»è­¯ä»»å‹™ã€‚
            </div>

            <div class="key-concept">
                <h4>ğŸ’¡ ç‚ºä»€éº¼é€™å¾ˆé‡è¦ï¼Ÿ</h4>
                <p>
                    é€™å€‹ä¾‹å­è­‰æ˜äº† GPT-2 çš„æ ¸å¿ƒå‡è¨­ï¼š
                </p>
                <ul>
                    <li>âœ… ç¶²éš›ç¶²è·¯ä¸Š<strong>è‡ªç„¶åŒ…å«</strong>å„ç¨®ä»»å‹™çš„ç¤ºç¯„</li>
                    <li>âœ… é€™äº›ç¤ºç¯„ä»¥<strong>è‡ªç„¶èªè¨€</strong>çš„å½¢å¼å‡ºç¾ï¼ˆå¦‚ "translated to English:"ï¼‰</li>
                    <li>âœ… èªè¨€æ¨¡å‹å¯ä»¥å¾é€™äº›è‡ªç„¶ç¤ºç¯„ä¸­<strong>å­¸ç¿’åŸ·è¡Œä»»å‹™</strong></li>
                    <li>âœ… ä¸éœ€è¦äººå·¥æ¨™è¨»çš„ã€Œè¼¸å…¥-è¼¸å‡ºã€å°</li>
                </ul>
                <p>
                    é€™å°±æ˜¯ç‚ºä»€éº¼ GPT-2 èƒ½åš Zero-Shot Translationâ€”â€”
                    å®ƒåœ¨è¨“ç·´è³‡æ–™ä¸­è¦‹éé¡ä¼¼çš„ç¿»è­¯ç¤ºç¯„æ ¼å¼ï¼
                </p>
            </div>
        </div>

        <h2>2.2. Input Representationï¼ˆè¼¸å…¥è¡¨ç¤ºï¼‰</h2>

        <img src="images/bpe_tokenization.png" alt="BPE Tokenization è¦–è¦ºåŒ–" class="image-full">
        <p class="image-caption">BPEï¼ˆByte Pair Encodingï¼‰å°‡æ–‡å­—æ‹†è§£ç‚ºæ¬¡è©å–®ä½ï¼ˆsubword unitsï¼‰</p>

        <div class="story-container">
            <h3>ğŸ”¤ å¦‚ä½•è®“æ¨¡å‹ã€Œè®€æ‡‚ã€ä»»ä½•æ–‡å­—ï¼Ÿ</h3>
            <p>
                ä¸€å€‹é€šç”¨çš„èªè¨€æ¨¡å‹æ‡‰è©²èƒ½å¤ è™•ç†<strong>ä»»ä½• Unicode å­—ä¸²</strong>ã€‚
                ä½†å¦‚ä½•åšåˆ°é€™ä¸€é»ä¸¦ä¸ç°¡å–®ã€‚
            </p>
        </div>

        <div class="original-quote">
            <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
            "A general language model should be able to compute the probability of (and also generate) <strong>any string</strong>. Current large scale LMs include pre-processing steps such as lowercasing, tokenization, and out-of-vocabulary tokens which restrict the space of model-able strings."
            <br><br>
            <strong>ç¿»è­¯</strong>ï¼šé€šç”¨èªè¨€æ¨¡å‹æ‡‰è©²èƒ½å¤ è¨ˆç®—ï¼ˆä¸¦ç”Ÿæˆï¼‰<strong>ä»»ä½•å­—ä¸²</strong>çš„æ©Ÿç‡ã€‚ç›®å‰çš„å¤§è¦æ¨¡èªè¨€æ¨¡å‹åŒ…å«é è™•ç†æ­¥é©Ÿï¼Œå¦‚å°å¯«åŒ–ã€åˆ†è©å’Œè©å½™å¤–æ¨™è¨˜ï¼Œé€™äº›é™åˆ¶äº†å¯å»ºæ¨¡å­—ä¸²çš„ç©ºé–“ã€‚
        </div>

        <div class="paper-section">
            <h3>âš–ï¸ å­—å…ƒç´š vs è©ç´šçš„æ¬Šè¡¡</h3>
            
            <div class="original-quote">
                <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                "While processing Unicode strings as a sequence of UTF-8 bytes elegantly fulfills this requirement as exemplified in work such as Gillick et al. (2015), <strong>current byte-level LMs are not competitive with word-level LMs</strong> on large scale datasets such as the One Billion Word Benchmark (Al-Rfou et al., 2018). We observed a similar performance gap in our own attempts to train standard byte-level LMs on WebText."
                <br><br>
                <strong>ç¿»è­¯</strong>ï¼šé›–ç„¶å°‡ Unicode å­—ä¸²è™•ç†ç‚º UTF-8 ä½å…ƒçµ„åºåˆ—å„ªé›…åœ°æ»¿è¶³äº†é€™å€‹è¦æ±‚ï¼ˆå¦‚ Gillick ç­‰äºº 2015 å¹´çš„å·¥ä½œï¼‰ï¼Œ<strong>ç›®å‰çš„ä½å…ƒçµ„ç´šèªè¨€æ¨¡å‹åœ¨å¤§è¦æ¨¡è³‡æ–™é›†ï¼ˆå¦‚ One Billion Word Benchmarkï¼‰ä¸Šç„¡æ³•èˆ‡è©ç´šèªè¨€æ¨¡å‹ç«¶çˆ­</strong>ã€‚æˆ‘å€‘åœ¨ WebText ä¸Šè¨“ç·´æ¨™æº–ä½å…ƒçµ„ç´šèªè¨€æ¨¡å‹çš„å˜—è©¦ä¸­ä¹Ÿè§€å¯Ÿåˆ°é¡ä¼¼çš„æ•ˆèƒ½å·®è·ã€‚
            </div>

            <div class="comparison-table">
                <h4>ğŸ“Š ä¸åŒ Tokenization æ–¹æ³•çš„æ¯”è¼ƒ</h4>
                <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                    <thead>
                        <tr style="background: #667eea; color: white;">
                            <th style="padding: 12px; text-align: left;">æ–¹æ³•</th>
                            <th style="padding: 12px; text-align: left;">å„ªé»</th>
                            <th style="padding: 12px; text-align: left;">ç¼ºé»</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="background: #f8f9fa;">
                            <td style="padding: 12px;"><strong>å­—å…ƒç´š</strong></td>
                            <td style="padding: 12px;">âœ… å¯ä»¥è™•ç†ä»»ä½•å­—ä¸²</td>
                            <td style="padding: 12px;">âŒ æ•ˆèƒ½è¼ƒå·®ï¼Œåºåˆ—å¤ªé•·</td>
                        </tr>
                        <tr>
                            <td style="padding: 12px;"><strong>è©ç´š</strong></td>
                            <td style="padding: 12px;">âœ… æ•ˆèƒ½å¥½ï¼Œåºåˆ—çŸ­</td>
                            <td style="padding: 12px;">âŒ ç„¡æ³•è™•ç†æœªçŸ¥è©</td>
                        </tr>
                        <tr style="background: #f8f9fa;">
                            <td style="padding: 12px;"><strong>BPEï¼ˆGPT-2ï¼‰</strong></td>
                            <td style="padding: 12px;">âœ… å…©è€…å„ªé»å…¼å…·</td>
                            <td style="padding: 12px;">âœ… æ—¢é€šç”¨åˆé«˜æ•ˆ</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3>ğŸ”§ GPT-2 çš„ Byte-Level BPE</h3>
            
            <div class="original-quote">
                <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆé—œéµå‰µæ–°ï¼‰</strong><br><br>
                "<strong>Byte Pair Encoding (BPE)</strong> (Sennrich et al., 2015) is a practical middle ground between character and word level language modeling which effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences."
                <br><br>
                <strong>ç¿»è­¯</strong>ï¼š<strong>Byte Pair Encoding (BPE)</strong> æ˜¯å­—å…ƒç´šå’Œè©ç´šèªè¨€å»ºæ¨¡ä¹‹é–“çš„å¯¦ç”¨ä¸­é–“åœ°å¸¶ï¼Œæœ‰æ•ˆåœ°åœ¨å¸¸è¦‹ç¬¦è™Ÿåºåˆ—ä½¿ç”¨è©ç´šè¼¸å…¥ï¼Œåœ¨ä¸å¸¸è¦‹ç¬¦è™Ÿåºåˆ—ä½¿ç”¨å­—å…ƒç´šè¼¸å…¥ä¹‹é–“é€²è¡Œæ’å€¼ã€‚
            </div>

            <div class="key-concept">
                <h4>ğŸ’¡ BPE çš„é‹ä½œåŸç†</h4>
                <p>æƒ³åƒä½ æœ‰é€™äº›è©ï¼š</p>
                <pre>
dog
dog.
dog!
dog?</pre>
                <p>
                    å‚³çµ± BPE æœƒç‚ºæ¯å€‹è®Šé«”å‰µå»ºä¸åŒçš„ tokenï¼Œæµªè²»è©å½™ç©ºé–“ã€‚
                    GPT-2 çš„ Byte-Level BPE æ”¹é€²äº†é€™ä¸€é»ï¼š
                </p>
            </div>

            <div class="original-quote">
                <strong>ğŸ“„ è«–æ–‡åŸæ–‡ï¼ˆGPT-2 çš„æ”¹é€²ï¼‰</strong><br><br>
                "Despite its name, reference BPE implementations often operate on Unicode code points and not byte sequences. These implementations would require including the full space of Unicode symbols in order to model all Unicode strings. This would result in a <strong>base vocabulary of over 130,000</strong> before any multi-symbol tokens are added. This is prohibitively large compared to the 32,000 to 64,000 token vocabularies often used with BPE. <strong>In contrast, a byte-level version of BPE only requires a base vocabulary of size 256</strong>."
                <br><br>
                <strong>ç¿»è­¯</strong>ï¼šå„˜ç®¡åç‚º BPEï¼Œåƒè€ƒå¯¦ä½œé€šå¸¸æ“ä½œåœ¨ Unicode ç¢¼é»è€Œéä½å…ƒçµ„åºåˆ—ä¸Šã€‚é€™äº›å¯¦ä½œéœ€è¦åŒ…å«å®Œæ•´çš„ Unicode ç¬¦è™Ÿç©ºé–“æ‰èƒ½å»ºæ¨¡æ‰€æœ‰ Unicode å­—ä¸²ã€‚é€™æœƒå°è‡´åœ¨æ·»åŠ ä»»ä½•å¤šç¬¦è™Ÿæ¨™è¨˜ä¹‹å‰ï¼Œ<strong>åŸºç¤è©å½™è¡¨å°±è¶…é 130,000</strong>ã€‚ç›¸æ¯”æ–¼ BPE å¸¸ç”¨çš„ 32,000 åˆ° 64,000 å€‹æ¨™è¨˜è©å½™è¡¨ï¼Œé€™å¤ªå¤§äº†ã€‚<strong>ç›¸æ¯”ä¹‹ä¸‹ï¼Œä½å…ƒçµ„ç´šç‰ˆæœ¬çš„ BPE åªéœ€è¦å¤§å°ç‚º 256 çš„åŸºç¤è©å½™è¡¨</strong>ã€‚
            </div>

            <div class="solution">
                <h4>âœ¨ GPT-2 çš„æœ€çµ‚æ–¹æ¡ˆ</h4>
                <div class="original-quote">
                    <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                    "However, directly applying BPE to the byte sequence results in sub-optimal merges due to BPE using a greedy frequency based heuristic for building the token vocabulary. We observed BPE including many versions of common words like dog since they occur in many variations such as dog. dog! dog? . This results in a sub-optimal allocation of limited vocabulary slots and model capacity. <strong>To avoid this, we prevent BPE from merging across character categories for any byte sequence</strong>. We add an exception for spaces which significantly improves the compression efficiency while adding only minimal fragmentation of words across multiple vocab tokens."
                    <br><br>
                    <strong>ç¿»è­¯</strong>ï¼šç„¶è€Œï¼Œç›´æ¥å°‡ BPE æ‡‰ç”¨æ–¼ä½å…ƒçµ„åºåˆ—æœƒå°è‡´æ¬¡å„ªçš„åˆä½µï¼Œå› ç‚º BPE ä½¿ç”¨åŸºæ–¼é »ç‡çš„è²ªå©ªå•Ÿç™¼å¼ä¾†å»ºæ§‹æ¨™è¨˜è©å½™è¡¨ã€‚æˆ‘å€‘è§€å¯Ÿåˆ° BPE åŒ…å«äº†è¨±å¤šå¸¸è¦‹è©çš„ç‰ˆæœ¬ï¼Œå¦‚ dogï¼Œå› ç‚ºå®ƒå€‘ä»¥è¨±å¤šè®Šé«”å‡ºç¾ï¼Œå¦‚ dog. dog! dog? ã€‚é€™å°è‡´æœ‰é™è©å½™æ§½å’Œæ¨¡å‹å®¹é‡çš„æ¬¡å„ªåˆ†é…ã€‚<strong>ç‚ºäº†é¿å…é€™ç¨®æƒ…æ³ï¼Œæˆ‘å€‘é˜²æ­¢ BPE è·¨è¶Šä»»ä½•ä½å…ƒçµ„åºåˆ—çš„å­—å…ƒé¡åˆ¥é€²è¡Œåˆä½µ</strong>ã€‚æˆ‘å€‘ç‚ºç©ºæ ¼æ·»åŠ äº†ä¾‹å¤–ï¼Œé€™é¡¯è‘—æé«˜äº†å£“ç¸®æ•ˆç‡ï¼ŒåŒæ™‚åªå¢åŠ äº†æ¥µå°‘çš„è·¨å¤šå€‹è©å½™æ¨™è¨˜çš„è©ç¢ç‰‡åŒ–ã€‚
                </div>

                <div class="key-concept">
                    <h4>ğŸ¯ æœ€çµ‚æ•ˆæœ</h4>
                    <div class="original-quote">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        "This input representation allows us to <strong>combine the empirical benefits of word-level LMs with the generality of byte-level approaches</strong>. Since our approach can assign a probability to any Unicode string, this allows us to evaluate our LMs on any dataset regardless of pre-processing, tokenization, or vocab size."
                        <br><br>
                        <strong>ç¿»è­¯</strong>ï¼šé€™ç¨®è¼¸å…¥è¡¨ç¤ºè®“æˆ‘å€‘èƒ½å¤ <strong>çµåˆè©ç´šèªè¨€æ¨¡å‹çš„å¯¦è­‰å„ªå‹¢èˆ‡ä½å…ƒçµ„ç´šæ–¹æ³•çš„é€šç”¨æ€§</strong>ã€‚ç”±æ–¼æˆ‘å€‘çš„æ–¹æ³•å¯ä»¥ç‚ºä»»ä½• Unicode å­—ä¸²åˆ†é…æ©Ÿç‡ï¼Œé€™ä½¿æˆ‘å€‘èƒ½å¤ åœ¨ä»»ä½•è³‡æ–™é›†ä¸Šè©•ä¼°èªè¨€æ¨¡å‹ï¼Œç„¡è«–é è™•ç†ã€åˆ†è©æˆ–è©å½™è¡¨å¤§å°å¦‚ä½•ã€‚
                    </div>
                </div>
            </div>
        </div>

        <div class="navigation">
            <a href="01-abstract-and-introduction.html" class="nav-button">â† ä¸Šä¸€ç« ï¼šæ‘˜è¦èˆ‡å¼•è¨€</a>
            <a href="03-model-architecture.html" class="nav-button">ä¸‹ä¸€ç« ï¼šæ¨¡å‹æ¶æ§‹ â†’</a>
        </div>

        <div class="chapter-summary">
            <h3>ğŸ“ æœ¬ç« é‡é»å›é¡§</h3>
            <ul>
                <li><strong>æ ¸å¿ƒæ–¹æ³•</strong>ï¼šèªè¨€å»ºæ¨¡ + ä»»å‹™æ¢ä»¶åŒ–ï¼Œç”¨è‡ªç„¶èªè¨€æŒ‡å®šä»»å‹™</li>
                <li><strong>ç†è«–åŸºç¤</strong>ï¼šç„¡ç›£ç£ç›®æ¨™çš„å…¨åŸŸæœ€å°å€¼åŒ…å«ç›£ç£ç›®æ¨™çš„å…¨åŸŸæœ€å°å€¼</li>
                <li><strong>WebText è³‡æ–™é›†</strong>ï¼š800è¬æ–‡ä»¶ï¼Œ40GBï¼Œå¾ Reddit 3+ karma é€£çµç­–å±•</li>
                <li><strong>å“è³ªæ§åˆ¶</strong>ï¼šç¾¤çœ¾å¤–åŒ…å“è³ªéæ¿¾ï¼Œç§»é™¤ Wikipedia é¿å…æ±¡æŸ“</li>
                <li><strong>Byte-Level BPE</strong>ï¼š256 åŸºç¤è©å½™ï¼Œå…¼å…·é€šç”¨æ€§èˆ‡æ•ˆç‡</li>
                <li><strong>é˜²æ­¢è·¨é¡åˆ¥åˆä½µ</strong>ï¼šé¿å… dog. dog! dog? ç”¢ç”Ÿä¸åŒ token</li>
            </ul>
        </div>

        <div class="key-concept" style="margin-top: 40px;">
            <h3>ğŸ¯ ç‚ºä»€éº¼é€™äº›å¾ˆé‡è¦ï¼Ÿ</h3>
            <p>
                <strong>WebText çš„å“è³ªç­–å±•</strong>è­‰æ˜äº†ã€Œè³‡æ–™å“è³ªæ¯”æ•¸é‡é‡è¦ã€â€”â€”é€™ç‚ºå¾ŒçºŒçš„ GPT-3ã€GPT-4 ç­‰æ¨¡å‹çš„è³‡æ–™å·¥ç¨‹å¥ å®šäº†åŸºç¤ã€‚
            </p>
            <p>
                <strong>Byte-Level BPE</strong> æˆç‚ºäº† GPT ç³»åˆ—çš„æ¨™æº– tokenization æ–¹æ³•ï¼Œä¸€ç›´æ²¿ç”¨åˆ°ä»Šå¤©çš„ GPT-4ã€‚
            </p>
        </div>
    </div>
</body>
</html>

