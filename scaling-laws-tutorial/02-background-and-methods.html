<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scaling Laws ç¬¬2ç« ï¼šèƒŒæ™¯èˆ‡æ–¹æ³•è«– - å¯¦é©—è¨­è¨ˆèˆ‡åƒæ•¸è¨ˆç®—</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="hero-section" style="background-image: url('images/chapter02_hero.png'); height: 80vh;">
        <div class="hero-overlay"></div>
        <div class="hero-content">
            <h1>å¯¦é©—è¨­è¨ˆçš„ç§‘å­¸åŸºç¤</h1>
            <p class="hero-subtitle">ç†è§£å¦‚ä½•è¨ˆç®—åƒæ•¸èˆ‡è¨ˆç®—é‡</p>
            <p class="hero-meta">Scaling Laws è«–æ–‡æ·±åº¦è§£æ Â· ç¬¬ 2 ç« </p>
        </div>
    </div>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <a href="index.html">Scaling Laws æ•™å­¸</a>
            <span>/</span>
            <span class="current">ç¬¬ 2 ç« </span>
        </div>

        <div class="story-container">
            <p class="story-lead drop-cap">
                è¦ç†è§£ Scaling Lawsï¼Œé¦–å…ˆå¿…é ˆç†è§£å¯¦é©—æ˜¯å¦‚ä½•è¨­è¨ˆçš„ã€‚
                é€™ç¯‡è«–æ–‡çš„æ ¸å¿ƒå‰µæ–°ä¹‹ä¸€ï¼Œæ˜¯ç™¼ç¾äº†<strong>æ’é™¤ Embedding åƒæ•¸</strong>å¾Œï¼Œæ€§èƒ½èˆ‡æ¨¡å‹å¤§å°çš„é—œä¿‚è®Šå¾—æ›´åŠ æ¸…æ™°ã€‚
            </p>

            <div class="section-divider"><span>âœ¦</span></div>

            <p>
                åœ¨æ·±å…¥ç ”ç©¶ä¸‰å€‹æ ¸å¿ƒå…¬å¼ä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦ç†è§£ï¼š
            </p>
            <ul>
                <li><strong>å¦‚ä½•è¨ˆç®—æ¨¡å‹åƒæ•¸æ•¸é‡</strong>ï¼ˆç‰¹åˆ¥æ˜¯ç‚ºä»€éº¼è¦æ’é™¤ Embeddingï¼‰</li>
                <li><strong>å¦‚ä½•ä¼°ç®—è¨“ç·´è¨ˆç®—é‡</strong>ï¼ˆFLOPs çš„è¨ˆç®—æ–¹æ³•ï¼‰</li>
                <li><strong>å¯¦é©—ä½¿ç”¨çš„è³‡æ–™é›†</strong>ï¼ˆWebText2 çš„æ§‹å»ºæ–¹å¼ï¼‰</li>
                <li><strong>è¨“ç·´æµç¨‹</strong>ï¼ˆå„ªåŒ–å™¨ã€å­¸ç¿’ç‡èª¿åº¦ç­‰ï¼‰</li>
            </ul>

            <p>
                é€™äº›ç´°ç¯€çœ‹ä¼¼æŠ€è¡“æ€§ï¼Œä½†å»æ˜¯ç†è§£ Scaling Laws çš„åŸºç¤ã€‚
                åªæœ‰ç†è§£äº†å¯¦é©—è¨­è¨ˆï¼Œä½ æ‰èƒ½çœŸæ­£ç†è§£ç‚ºä»€éº¼é€™äº›å…¬å¼èƒ½å¤ é æ¸¬ GPT-3 çš„æˆåŠŸã€‚
            </p>
        </div>

        <h2>ğŸ“Š è³‡æ–™é›†ï¼šWebText2</h2>

        <div class="paper-section">
            <div class="original-quote">
                <strong>åŸæ–‡</strong>
                <p>
                    We train our models on an extended version of the WebText dataset described in \cite{radford2019language}. 
                    The original WebText dataset was a web scrape of outbound links from Reddit through December 2017 which received at least 3 karma. 
                    In the second version, WebText2, we added outbound Reddit links from the period of January to October 2018, 
                    also with a minimum of 3 karma. The karma threshold served as a heuristic for whether people found the link interesting or useful.
                </p>
            </div>

            <div class="translation">
                <h4>ğŸ“ ä¸­æ–‡ç¿»è­¯</h4>
                <p>
                    æˆ‘å€‘åœ¨ WebText è³‡æ–™é›†çš„æ“´å±•ç‰ˆæœ¬ä¸Šè¨“ç·´æ¨¡å‹ï¼ˆå¦‚ \cite{radford2019language} æ‰€è¿°ï¼‰ã€‚
                    åŸå§‹ WebText è³‡æ–™é›†æ˜¯å¾ Reddit å¤–éˆæŠ“å–çš„ç¶²é ï¼Œæ™‚é–“ç¯„åœç‚º 2017 å¹´ 12 æœˆä¹‹å‰ï¼Œä¸”è‡³å°‘ç²å¾— 3 å€‹ karmaã€‚
                    åœ¨ç¬¬äºŒç‰ˆ WebText2 ä¸­ï¼Œæˆ‘å€‘æ·»åŠ äº† 2018 å¹´ 1 æœˆè‡³ 10 æœˆæœŸé–“çš„ Reddit å¤–éˆï¼ŒåŒæ¨£è¦æ±‚è‡³å°‘ 3 å€‹ karmaã€‚
                    Karma é–¾å€¼ä½œç‚ºåˆ¤æ–·äººå€‘æ˜¯å¦èªç‚ºé€£çµæœ‰è¶£æˆ–æœ‰ç”¨çš„å•Ÿç™¼å¼æŒ‡æ¨™ã€‚
                </p>
            </div>

            <div class="key-concept">
                <h4>ğŸ’¡ é—œéµæ•¸å­—</h4>
                <ul>
                    <li><strong>20.3M æ–‡ä»¶</strong>ï¼šç¸½å…±åŒ…å« 2030 è¬å€‹æ–‡ä»¶</li>
                    <li><strong>96 GB æ–‡å­—</strong>ï¼šåŸå§‹æ–‡å­—å¤§å°</li>
                    <li><strong>1.62 Ã— 10Â¹â° è©</strong>ï¼šä½¿ç”¨ <code>wc</code> çµ±è¨ˆçš„è©æ•¸</li>
                    <li><strong>2.29 Ã— 10Â¹â° tokens</strong>ï¼šä½¿ç”¨å¯é€† tokenizer è™•ç†å¾Œçš„ token æ•¸</li>
                    <li><strong>6.6 Ã— 10â¸ tokens</strong>ï¼šä¿ç•™ä½œç‚ºæ¸¬è©¦é›†</li>
                </ul>
            </div>

            <div class="explanation">
                <h4>ğŸ” ç‚ºä»€éº¼é¸æ“‡ Reddit é€£çµï¼Ÿ</h4>
                <p>
                    Reddit çš„ karma ç³»çµ±æä¾›äº†ä¸€å€‹è‡ªç„¶çš„ã€Œå“è³ªéæ¿¾å™¨ã€ï¼š
                    åªæœ‰è¢«ç¤¾ç¾¤èªç‚ºæœ‰åƒ¹å€¼çš„å…§å®¹æ‰æœƒç²å¾—è¶³å¤ çš„ karmaã€‚
                    é€™æ¯”ç›´æ¥ä½¿ç”¨ Common Crawl ç­‰æœªéæ¿¾çš„è³‡æ–™é›†æ›´èƒ½ä¿è­‰è³‡æ–™å“è³ªã€‚
                </p>
            </div>
        </div>

        <h2>ğŸ”¢ åƒæ•¸è¨ˆç®—ï¼šç‚ºä»€éº¼è¦æ’é™¤ Embeddingï¼Ÿ</h2>

        <div class="paper-section">
            <div class="original-quote">
                <strong>åŸæ–‡</strong>
                <p>
                    We use $N$ to denote the model size, which we define as the number of \emph{non-embedding} parameters.
                    Our models also have $n_{\rm vocab} d_{{\rm model}}$ parameters in an embedding matrix, 
                    and use $n_{\rm ctx} d_{{\rm model}}$ parameters for positional embeddings, 
                    but we do not include these when discussing the `model size' $N$; 
                    we will see that this produces significantly cleaner scaling laws.
                </p>
            </div>

            <div class="translation">
                <h4>ğŸ“ ä¸­æ–‡ç¿»è­¯</h4>
                <p>
                    æˆ‘å€‘ä½¿ç”¨ $N$ è¡¨ç¤ºæ¨¡å‹å¤§å°ï¼Œå®šç¾©ç‚º<strong>éåµŒå…¥åƒæ•¸</strong>çš„æ•¸é‡ã€‚
                    æˆ‘å€‘çš„æ¨¡å‹é‚„åŒ…å« $n_{\rm vocab} d_{{\rm model}}$ å€‹åƒæ•¸çš„åµŒå…¥çŸ©é™£ï¼Œ
                    ä»¥åŠ $n_{\rm ctx} d_{{\rm model}}$ å€‹åƒæ•¸çš„ä½ç½®åµŒå…¥ï¼Œ
                    ä½†åœ¨è¨è«–ã€Œæ¨¡å‹å¤§å°ã€$N$ æ™‚ï¼Œæˆ‘å€‘ä¸åŒ…å«é€™äº›åƒæ•¸ï¼›
                    æˆ‘å€‘å°‡çœ‹åˆ°ï¼Œé€™æœƒç”¢ç”Ÿé¡¯è‘—æ›´æ¸…æ™°çš„æ“´å±•æ³•å‰‡ã€‚
                </p>
            </div>

            <div class="key-concept">
                <h4>ğŸ“ æ¨¡å‹å¤§å°å…¬å¼</h4>
                <div class="math-block">
                    $$N \approx 2d_{{\rm model}}n_{{\rm layer}}\left(2d_{{\rm attn}}+d_{{\rm ff}}\right)$$
                    <p style="margin-top: 10px; font-size: 0.9em; color: var(--text-secondary);">
                        ä½¿ç”¨æ¨™æº–é…ç½® $d_{\rm attn} = d_{\rm ff}/4 = d_{{\rm model}}$ æ™‚ï¼š
                    </p>
                    $$N = 12 n_{\rm layer} d_{{\rm model}}^2$$
                </div>
            </div>

            <div class="explanation">
                <h4>ğŸ¤” ç‚ºä»€éº¼æ’é™¤ Embeddingï¼Ÿ</h4>
                <p>
                    <strong>é—œéµç™¼ç¾</strong>ï¼šç•¶åŒ…å« Embedding åƒæ•¸æ™‚ï¼Œæ€§èƒ½èˆ‡ç¸½åƒæ•¸æ•¸é‡çš„é—œä¿‚æœƒå—åˆ°æ¨¡å‹æ·±åº¦ï¼ˆå±¤æ•¸ï¼‰çš„å¼·çƒˆå½±éŸ¿ã€‚
                    ä½†ç•¶åªè€ƒæ…®éåµŒå…¥åƒæ•¸ $N$ æ™‚ï¼Œä¸åŒæ·±åº¦çš„æ¨¡å‹éƒ½éµå¾ªåŒä¸€æ¢å†ªå¾‹æ›²ç·šã€‚
                </p>
                <p>
                    é€™æ„å‘³è‘—ï¼š<strong>Embedding åƒæ•¸å°æ€§èƒ½çš„è²¢ç»èˆ‡ Transformer å±¤çš„åƒæ•¸ä¸åŒ</strong>ã€‚
                    æ’é™¤å®ƒå€‘å¾Œï¼Œæˆ‘å€‘å¾—åˆ°äº†ä¸€å€‹æ›´ã€Œç´”ç²¹ã€çš„æ“´å±•æ³•å‰‡ï¼Œåªåæ˜  Transformer æ¶æ§‹æœ¬èº«çš„æ“´å±•ç‰¹æ€§ã€‚
                </p>
            </div>

            <div class="figure figure-original">
                <img src="images/original/PerfVsModelSizeNonEmbed.png" alt="Performance vs Model Size (Non-Embedding)">
                <div class="caption">
                    <strong>Figure:</strong> æ’é™¤ Embedding åƒæ•¸å¾Œï¼Œä¸åŒæ·±åº¦çš„æ¨¡å‹éƒ½éµå¾ªåŒä¸€æ¢å†ªå¾‹æ›²ç·šã€‚
                </div>
            </div>
        </div>

        <h2>âš™ï¸ Transformer åƒæ•¸èˆ‡è¨ˆç®—é‡è©³è§£</h2>

        <div class="paper-section">
            <div class="key-concept">
                <h4>ğŸ“Š åƒæ•¸è¨ˆç®—è¡¨</h4>
                <table class="arch-table">
                    <thead>
                        <tr>
                            <th>æ“ä½œ</th>
                            <th>åƒæ•¸æ•¸é‡</th>
                            <th>æ¯ Token FLOPs</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Embed</td>
                            <td>$(n_{\rm vocab} + n_{\rm ctx})d_{\rm model}$</td>
                            <td>$4d_{\rm model}$</td>
                        </tr>
                        <tr>
                            <td>Attention: QKV</td>
                            <td>$n_{\rm layer}d_{\rm model} \cdot 3d_{\rm attn}$</td>
                            <td>$2n_{\rm layer}d_{\rm model} \cdot 3d_{\rm attn}$</td>
                        </tr>
                        <tr>
                            <td>Attention: Mask</td>
                            <td>---</td>
                            <td>$2n_{\rm layer}n_{\rm ctx}d_{\rm attn}$</td>
                        </tr>
                        <tr>
                            <td>Attention: Project</td>
                            <td>$n_{\rm layer}d_{\rm attn}d_{\rm model}$</td>
                            <td>$2n_{\rm layer}d_{\rm attn}d_{\rm model}$</td>
                        </tr>
                        <tr>
                            <td>Feedforward</td>
                            <td>$n_{\rm layer} \cdot 2d_{\rm model}d_{\rm ff}$</td>
                            <td>$2n_{\rm layer} \cdot 2d_{\rm model}d_{\rm ff}$</td>
                        </tr>
                        <tr>
                            <td>De-embed</td>
                            <td>---</td>
                            <td>$2d_{\rm model}n_{\rm vocab}$</td>
                        </tr>
                        <tr style="background: var(--accent-light); font-weight: 600;">
                            <td><strong>Total (Non-Embedding)</strong></td>
                            <td><strong>$N = 2d_{\rm model}n_{\rm layer}(2d_{\rm attn}+d_{\rm ff})$</strong></td>
                            <td><strong>$C_{\rm forward} = 2N + 2n_{\rm layer}n_{\rm ctx}d_{\rm attn}$</strong></td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="explanation">
                <h4>ğŸ” è¨ˆç®—é‡ä¼°ç®—</h4>
                <p>
                    å°æ–¼å‰å‘å‚³æ’­ï¼Œè¨ˆç®—é‡ç´„ç‚ºï¼š
                </p>
                <div class="math-block">
                    $$C_{\rm forward} \approx 2N + 2n_{\rm layer}n_{\rm ctx}d_{{\rm model}}$$
                </div>
                <p>
                    ç•¶ $d_{\rm model} \gg n_{\rm ctx}/12$ æ™‚ï¼ˆé€™åœ¨æˆ‘å€‘ç ”ç©¶çš„æ¨¡å‹ä¸­é€šå¸¸æˆç«‹ï¼‰ï¼Œ
                    ä¸Šä¸‹æ–‡ç›¸é—œçš„è¨ˆç®—æˆæœ¬ç›¸å°è¼ƒå°ã€‚
                    å› æ­¤ï¼Œæˆ‘å€‘å¯ä»¥ç°¡åŒ–ç‚ºï¼š
                </p>
                <div class="math-block">
                    $$C \approx 6N \text{ FLOPs per training token}$$
                </div>
                <p>
                    å…¶ä¸­å› å­ 6 ä¾†è‡ªï¼šå‰å‘å‚³æ’­ç´„ $2N$ï¼Œåå‘å‚³æ’­ç´„ $4N$ï¼ˆé€šå¸¸æ˜¯å‰å‘çš„å…©å€ï¼‰ã€‚
                </p>
            </div>
        </div>

        <h2>ğŸ¯ è¨“ç·´æµç¨‹</h2>

        <div class="paper-section">
            <div class="original-quote">
                <strong>åŸæ–‡</strong>
                <p>
                    Unless otherwise noted, we train models with the Adam optimizer for a fixed $2.5 \times 10^5$ steps 
                    with a batch size of $512$ sequences of $1024$ tokens.
                    Due to memory constraints, our largest models (more than 1B parameters) were trained with Adafactor.
                    We found that results at convergence were largely independent of learning rate schedule.
                    Unless otherwise noted, all training runs included in our data used a learning rate schedule 
                    with a 3000 step linear warmup followed by a cosine decay to zero.
                </p>
            </div>

            <div class="translation">
                <h4>ğŸ“ ä¸­æ–‡ç¿»è­¯</h4>
                <p>
                    é™¤éå¦æœ‰èªªæ˜ï¼Œæˆ‘å€‘ä½¿ç”¨ Adam å„ªåŒ–å™¨è¨“ç·´æ¨¡å‹ï¼Œå›ºå®š $2.5 \times 10^5$ æ­¥ï¼Œ
                    æ‰¹æ¬¡å¤§å°ç‚º $512$ å€‹åºåˆ—ï¼Œæ¯å€‹åºåˆ— $1024$ å€‹ tokensã€‚
                    ç”±æ–¼è¨˜æ†¶é«”é™åˆ¶ï¼Œæˆ‘å€‘æœ€å¤§çš„æ¨¡å‹ï¼ˆè¶…é 10 å„„åƒæ•¸ï¼‰ä½¿ç”¨ Adafactor è¨“ç·´ã€‚
                    æˆ‘å€‘ç™¼ç¾æ”¶æ–‚æ™‚çš„çµæœåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šèˆ‡å­¸ç¿’ç‡èª¿åº¦ç„¡é—œã€‚
                    é™¤éå¦æœ‰èªªæ˜ï¼Œæ‰€æœ‰è¨“ç·´é‹è¡Œéƒ½ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦ï¼š
                    3000 æ­¥ç·šæ€§é ç†±ï¼Œç„¶å¾Œé¤˜å¼¦è¡°æ¸›åˆ°é›¶ã€‚
                </p>
            </div>

            <div class="key-concept">
                <h4>âš™ï¸ è¨“ç·´é…ç½®ç¸½çµ</h4>
                <ul>
                    <li><strong>å„ªåŒ–å™¨</strong>ï¼šAdamï¼ˆå°æ¨¡å‹ï¼‰æˆ– Adafactorï¼ˆå¤§æ¨¡å‹ > 1Bï¼‰</li>
                    <li><strong>è¨“ç·´æ­¥æ•¸</strong>ï¼š$2.5 \times 10^5$ï¼ˆ25 è¬æ­¥ï¼‰</li>
                    <li><strong>æ‰¹æ¬¡å¤§å°</strong>ï¼š$512$ å€‹åºåˆ— Ã— $1024$ tokens = $524,288$ tokens/batch</li>
                    <li><strong>ä¸Šä¸‹æ–‡é•·åº¦</strong>ï¼š$1024$ tokensï¼ˆå¤§å¤šæ•¸å¯¦é©—ï¼‰</li>
                    <li><strong>å­¸ç¿’ç‡èª¿åº¦</strong>ï¼š3000 æ­¥é ç†± + é¤˜å¼¦è¡°æ¸›</li>
                </ul>
            </div>
        </div>

        <h2>ğŸ’¡ ç”Ÿæ´»é¡æ¯”ï¼šå»ºç¯‰é¢ç© vs ä½¿ç”¨é¢ç©</h2>

        <div class="analogy">
            <h4>ğŸ  é¡æ¯”ï¼šæˆ¿ç”¢é¢ç©è¨ˆç®—</h4>
            <p>
                åœ¨æˆ¿åœ°ç”¢ä¸­ï¼Œæœ‰ã€Œå»ºç¯‰é¢ç©ã€å’Œã€Œä½¿ç”¨é¢ç©ã€çš„å€åˆ¥ï¼š
            </p>
            <ul>
                <li><strong>å»ºç¯‰é¢ç©</strong>ï¼šåŒ…å«å…¬æ”¤ã€é›»æ¢¯é–“ç­‰ï¼ˆå°æ‡‰ç¸½åƒæ•¸ï¼ŒåŒ…å« Embeddingï¼‰</li>
                <li><strong>ä½¿ç”¨é¢ç©</strong>ï¼šå¯¦éš›å¯ç”¨çš„å±…ä½ç©ºé–“ï¼ˆå°æ‡‰éåµŒå…¥åƒæ•¸ $N$ï¼‰</li>
            </ul>
            <p>
                åŒæ¨£åœ°ï¼Œåœ¨èªè¨€æ¨¡å‹ä¸­ï¼š
            </p>
            <ul>
                <li><strong>ç¸½åƒæ•¸</strong>ï¼šåŒ…å« Embeddingï¼Œä½†é€™éƒ¨åˆ†åƒæ•¸å°æ€§èƒ½çš„è²¢ç»æ–¹å¼ä¸åŒ</li>
                <li><strong>éåµŒå…¥åƒæ•¸ $N$</strong>ï¼šçœŸæ­£åæ˜  Transformer æ¶æ§‹çš„ã€Œæ ¸å¿ƒèƒ½åŠ›ã€</li>
            </ul>
            <p>
                å°±åƒæˆ¿åƒ¹ä¸»è¦å–æ±ºæ–¼ã€Œä½¿ç”¨é¢ç©ã€è€Œéã€Œå»ºç¯‰é¢ç©ã€ä¸€æ¨£ï¼Œ
                èªè¨€æ¨¡å‹çš„æ€§èƒ½ä¸»è¦å–æ±ºæ–¼ã€ŒéåµŒå…¥åƒæ•¸ã€è€Œéã€Œç¸½åƒæ•¸ã€ã€‚
            </p>
        </div>

        <h2>ğŸ”§ å·¥ç¨‹é¡æ¯”ï¼šCPU æ ¸å¿ƒæ•¸ vs ç¸½æ™‚è„ˆ</h2>

        <div class="analogy">
            <h4>ğŸ’» é¡æ¯”ï¼šCPU æ€§èƒ½æŒ‡æ¨™</h4>
            <p>
                åœ¨ CPU æ€§èƒ½è©•ä¼°ä¸­ï¼š
            </p>
            <ul>
                <li><strong>ç¸½æ™‚è„ˆ</strong>ï¼šæ‰€æœ‰æ ¸å¿ƒçš„æ™‚è„ˆç¸½å’Œï¼ˆå°æ‡‰ç¸½åƒæ•¸ï¼‰</li>
                <li><strong>æ ¸å¿ƒæ•¸ Ã— å–®æ ¸æ€§èƒ½</strong>ï¼šæ›´æº–ç¢ºçš„æ€§èƒ½æŒ‡æ¨™ï¼ˆå°æ‡‰éåµŒå…¥åƒæ•¸ï¼‰</li>
            </ul>
            <p>
                åŒæ¨£åœ°ï¼Œåœ¨èªè¨€æ¨¡å‹ä¸­ï¼š
            </p>
            <ul>
                <li><strong>ç¸½åƒæ•¸</strong>ï¼šåŒ…å« Embeddingï¼Œä½†é€™éƒ¨åˆ†åƒæ•¸çš„ã€Œè¨ˆç®—æ•ˆç‡ã€ä¸åŒ</li>
                <li><strong>éåµŒå…¥åƒæ•¸ $N$</strong>ï¼šåæ˜  Transformer å±¤çš„ã€Œæ ¸å¿ƒè¨ˆç®—èƒ½åŠ›ã€</li>
            </ul>
            <p>
                å°±åƒ CPU æ€§èƒ½ä¸»è¦å–æ±ºæ–¼ã€Œæ ¸å¿ƒæ•¸ Ã— å–®æ ¸æ€§èƒ½ã€è€Œéã€Œç¸½æ™‚è„ˆã€ä¸€æ¨£ï¼Œ
                èªè¨€æ¨¡å‹æ€§èƒ½ä¸»è¦å–æ±ºæ–¼ã€ŒéåµŒå…¥åƒæ•¸ã€è€Œéã€Œç¸½åƒæ•¸ã€ã€‚
            </p>
        </div>

        <div class="quote-block">
            ã€Œæ’é™¤ Embedding åƒæ•¸å¾Œï¼Œæˆ‘å€‘å¾—åˆ°äº†ä¸€å€‹æ›´æ¸…æ™°çš„æ“´å±•æ³•å‰‡ï¼Œ
            é€™ä½¿å¾—æ€§èƒ½èˆ‡æ¨¡å‹å¤§å°çš„é—œä¿‚è®Šå¾—å¯é æ¸¬ä¸”ä¸€è‡´ã€‚ã€
        </div>

        <div class="navigation">
            <a href="01-introduction.html" class="nav-link">â† ä¸Šä¸€ç« ï¼šæ‘˜è¦èˆ‡æ ¸å¿ƒç™¼ç¾</a>
            <a href="03-empirical-power-laws.html" class="nav-link">ä¸‹ä¸€ç« ï¼šå¯¦è­‰çµæœèˆ‡åŸºæœ¬å†ªå¾‹ â†’</a>
        </div>
    </div>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
</body>
</html>

