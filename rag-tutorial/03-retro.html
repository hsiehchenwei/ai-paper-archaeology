<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>RETRO：2 兆 Token 的檢索奇蹟 | RAG 演進史</title>
    <meta name="description" content="Improving Language Models by Retrieving from Trillions of Tokens 深度解析。DeepMind 使用 2 兆 token 資料庫，用 25 倍少的參數達到 GPT-3 水準。" />
    <link rel="stylesheet" href="../styles/global.css" />
    <link rel="stylesheet" href="../styles/paper-reading.css" />
    <link rel="stylesheet" href="../styles/articles.css" />
  </head>
  <body>
    <div class="container">
      <!-- 麵包屑導航 -->
      <div class="breadcrumb">
        <a href="../index.html">🏠 首頁</a> / 
        <a href="index.html">📚 RAG 演進史</a> / 
        RETRO
      </div>

      <!-- 論文資訊卡 -->
      <div class="index-header" style="background: linear-gradient(135deg, var(--accent-color), #f59e0b);">
        <h1>🚀 RETRO</h1>
        <p>Improving Language Models by<br />Retrieving from Trillions of Tokens</p>
        <p style="font-size: 0.9rem; margin-top: 15px; opacity: 0.95;">
          Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, et al.<br />
          <strong>DeepMind</strong> · 2021 年 12 月
        </p>
        <p style="background: rgba(255,255,255,0.2); padding: 8px 16px; border-radius: 20px; margin-top: 15px; font-size: 0.9rem;">
          💡 用 25 倍少的參數達到 GPT-3 水準
        </p>
      </div>

      <!-- 一句話總結 -->
      <div class="key-concept">
        <h4>💡 一句話總結</h4>
        <p style="font-size: 1.1rem;">
          RETRO 使用 <strong>2 兆 token</strong> 的檢索資料庫，讓 7.5B 參數的模型達到 GPT-3 175B 的水準！
          證明檢索可以大幅提升參數效率，用更少的參數獲得更好的效果。
        </p>
      </div>

      <!-- 論文 PDF 連結 -->
      <div style="background: var(--accent-light); padding: 15px; border-radius: var(--radius-md); margin: 30px 0; text-align: center;">
        <a href="papers/retro.pdf" target="_blank" style="color: var(--accent-color); font-weight: 600; text-decoration: none; font-size: 1.05rem;">
          📄 下載論文原文 PDF →
        </a>
      </div>

      <!-- Abstract 摘要 -->
      <h2>📄 Abstract (摘要)</h2>

      <div class="original-quote">
        <strong>📄 論文原文</strong><br><br>
        "Large language models have been shown to memorize a vast amount of knowledge during pretraining, including information that is unlikely to appear in their training data more than once. We investigate improving language models by retrieving from a large corpus of documents at inference time. We introduce the Retrieval-Enhanced Transformer (RETRO), a language model that conditions on large-scale document chunks retrieved based on local similarity with preceding tokens."
        <br><br>
        <strong>翻譯</strong>：大型語言模型已被證明在預訓練期間記憶了大量知識，包括不太可能在訓練資料中出現超過一次的資訊。我們研究透過在推理時從大型文檔語料庫中檢索來改進語言模型。我們引入了檢索增強 Transformer（RETRO），這是一個基於與前面 token 的局部相似性檢索大規模文檔塊的語言模型。
      </div>

      <div class="explanation">
        <h4>🔍 核心洞察</h4>
        <p>RETRO 的關鍵發現：</p>
        <ul>
          <li><strong>模型會記憶罕見資訊</strong>：即使只出現一次，模型也會記住</li>
          <li><strong>記憶效率低</strong>：把這些資訊存在參數裡很浪費</li>
          <li><strong>解決方案</strong>：用外部資料庫儲存，需要時再檢索</li>
        </ul>
      </div>

      <div class="original-quote">
        <strong>📄 論文原文（驚人結果）</strong><br><br>
        "By utilizing a 2 trillion token database, RETRO achieves performance comparable to models like GPT-3 and Jurassic-1 on the Pile dataset, despite using 25 times fewer parameters. After fine-tuning, RETRO's performance extends to downstream knowledge-intensive tasks such as question answering."
        <br><br>
        <strong>翻譯</strong>：透過使用 2 兆 token 的資料庫，RETRO 在 Pile 資料集上達到與 GPT-3 和 Jurassic-1 相當的表現，儘管使用的參數少了 25 倍。經過微調後，RETRO 的表現擴展到下游知識密集型任務，如問答。
      </div>

      <div class="key-concept" style="background: linear-gradient(135deg, #fef3c7, #fde68a); padding: 30px; border-radius: var(--radius-md); margin: 30px 0;">
        <h4 style="color: var(--accent-color); margin-top: 0;">📊 參數效率對比</h4>
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin-top: 20px;">
          <div style="background: white; padding: 20px; border-radius: var(--radius-md); text-align: center;">
            <div style="font-size: 2rem; font-weight: 700; color: var(--primary-color);">175B</div>
            <div style="font-size: 0.9rem; color: var(--text-secondary); margin-top: 8px;">GPT-3 參數</div>
          </div>
          <div style="background: white; padding: 20px; border-radius: var(--radius-md); text-align: center;">
            <div style="font-size: 2rem; font-weight: 700; color: var(--accent-color);">7.5B</div>
            <div style="font-size: 0.9rem; color: var(--text-secondary); margin-top: 8px;">RETRO 參數</div>
          </div>
          <div style="background: white; padding: 20px; border-radius: var(--radius-md); text-align: center; border: 3px solid var(--secondary-color);">
            <div style="font-size: 2rem; font-weight: 700; color: var(--secondary-color);">25×</div>
            <div style="font-size: 0.9rem; color: var(--text-secondary); margin-top: 8px;">參數效率提升</div>
          </div>
        </div>
        <p style="margin-top: 20px; font-weight: 600; color: var(--accent-color);">
          用 25 倍少的參數，達到相同的效果！這就是檢索的威力。
        </p>
      </div>

      <!-- 核心架構 -->
      <h2>🔧 核心架構</h2>

      <div class="section-block">
        <h3 style="color: var(--accent-color);">三大核心組件</h3>
        
        <div style="background: var(--primary-light); padding: 20px; border-radius: var(--radius-md); margin: 20px 0;">
          <h4 style="margin-top: 0;">🔍 Frozen BERT Retriever</h4>
          <ul>
            <li>使用預訓練的 BERT 編碼器（不更新參數）</li>
            <li>將查詢和文檔編碼成向量</li>
            <li>從 2 兆 token 資料庫中檢索相關文檔</li>
          </ul>
        </div>

        <div style="background: var(--secondary-light); padding: 20px; border-radius: var(--radius-md); margin: 20px 0;">
          <h4 style="margin-top: 0;">📦 Chunked Cross-Attention</h4>
          <ul>
            <li>將檢索到的文檔分成多個 chunk</li>
            <li>每個 chunk 獨立進行 cross-attention</li>
            <li>允許模型同時關注多個文檔片段</li>
          </ul>
        </div>

        <div style="background: var(--accent-light); padding: 20px; border-radius: var(--radius-md); margin: 20px 0;">
          <h4 style="margin-top: 0;">🔄 Differentiable Encoder</h4>
          <ul>
            <li>可微分的編碼器，允許端到端訓練</li>
            <li>整合檢索到的知識到生成過程中</li>
            <li>支援從頭訓練或適配預訓練模型</li>
          </ul>
        </div>
      </div>

      <div class="analogy">
        <h4>🎯 生活類比：圖書館 vs 記憶</h4>
        <p><strong>GPT-3</strong> = 把所有書都背在腦中</p>
        <ul>
          <li>需要 175B 個「記憶單元」</li>
          <li>記住所有知識，但很浪費</li>
        </ul>
        <p><strong>RETRO</strong> = 只記住索引，需要時去圖書館查</p>
        <ul>
          <li>只需要 7.5B 個「記憶單元」</li>
          <li>圖書館有 2 兆 token 的書</li>
          <li>需要時再去查，效率高 25 倍</li>
        </ul>
      </div>

      <!-- 技術創新 -->
      <h2>💡 技術創新</h2>

      <!-- 視覺化圖解 -->
      <div style="text-align: center; margin: 40px 0;">
        <img src="images/retro-chunked-attention.png" alt="RETRO Chunked Cross-Attention 機制圖" style="max-width: 100%; border-radius: 12px; box-shadow: var(--shadow-lg);" />
        <p style="margin-top: 15px; color: var(--text-secondary); font-size: 0.9rem;">
          📊 RETRO 的核心創新：Chunked Cross-Attention 分塊交叉注意力機制
        </p>
      </div>

      <div class="key-concept">
        <h4>💡 論文原始圖表</h4>
        <p>
          RETRO 論文中的 <strong>Figure 1</strong> 展示了 Chunked Cross-Attention 的完整架構：
        </p>
        <ul>
          <li><strong>Input Chunking</strong>：將輸入序列分割成固定大小的 chunks</li>
          <li><strong>Retrieval per Chunk</strong>：為每個 chunk 檢索相關的鄰居文檔</li>
          <li><strong>RETRO-block</strong>：特殊的 Transformer block，包含 Chunked Cross-Attention (CCA) 層</li>
          <li><strong>Encoder-Decoder Architecture</strong>：檢索到的文檔通過獨立編碼器處理</li>
        </ul>
        <p style="margin-top: 10px; font-size: 0.9rem; color: var(--text-secondary);">
          📄 請參考 <a href="papers/retro.pdf" target="_blank" style="color: var(--primary-color);">論文 PDF</a> 第 3 頁 Figure 1 與第 4 頁 Figure 2
        </p>
      </div>

      <div class="key-concept">
        <h4>🔄 Chunked Cross-Attention 機制</h4>
        <p>RETRO 的核心創新是將檢索到的文檔分成多個 chunk，每個 chunk 獨立處理：</p>
        <ol style="line-height: 2;">
          <li>檢索到 k 個相關文檔</li>
          <li>每個文檔分成多個 chunk（例如 64 tokens）</li>
          <li>對每個 chunk 進行 cross-attention</li>
          <li>模型可以同時關注多個文檔片段</li>
        </ol>
        <p style="margin-top: 15px; color: var(--text-secondary);">
          這種設計讓模型能夠更靈活地使用檢索到的知識，而不是只依賴單一文檔。
        </p>
      </div>

      <!-- 實驗結果 -->
      <h2>📊 實驗結果</h2>

      <div class="solution">
        <h4>✅ 在 Pile 資料集上的表現</h4>
        <p>RETRO 在語言建模任務上達到與 GPT-3 相當的表現：</p>
        <table style="margin-top: 20px;">
          <thead>
            <tr>
              <th>模型</th>
              <th>參數</th>
              <th>Perplexity (Pile)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>GPT-3</td>
              <td>175B</td>
              <td>~10.8</td>
            </tr>
            <tr style="background: var(--accent-light);">
              <td><strong>RETRO</strong></td>
              <td><strong>7.5B</strong></td>
              <td style="font-weight: bold; color: var(--accent-color);">~10.9</strong></td>
            </tr>
          </tbody>
        </table>
        <p style="margin-top: 15px; color: var(--text-secondary);">
          用 25 倍少的參數，達到幾乎相同的效果！
        </p>
      </div>

      <!-- 歷史意義 -->
      <h2>📜 歷史意義</h2>

      <div class="key-concept">
        <h4>🌟 RETRO 證明了什麼？</h4>
        <ul>
          <li><strong>檢索可以大幅提升參數效率</strong>：不需要把所有知識存在參數裡</li>
          <li><strong>大規模檢索資料庫可行</strong>：2 兆 token 的資料庫可以實際使用</li>
          <li><strong>Chunked Cross-Attention 有效</strong>：可以同時處理多個文檔片段</li>
          <li><strong>啟發後續研究</strong>：為更大規模的檢索系統鋪路</li>
        </ul>
        <p style="margin-top: 15px;">
          RETRO 發表於 2021 年 12 月，比 RAG 晚了一年半。
          它證明了檢索不僅能提升效果，還能<strong>大幅降低參數需求</strong>。
        </p>
      </div>

      <!-- 與 RAG 的比較 -->
      <h2>🔄 RETRO vs RAG</h2>

      <div class="section-block">
        <table>
          <thead>
            <tr>
              <th>面向</th>
              <th>RAG</th>
              <th>RETRO</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>發布時間</strong></td>
              <td>2020 年 5 月</td>
              <td>2021 年 12 月</td>
            </tr>
            <tr>
              <td><strong>機構</strong></td>
              <td>Meta AI</td>
              <td>DeepMind</td>
            </tr>
            <tr>
              <td><strong>檢索資料庫</strong></td>
              <td>Wikipedia (21M 文檔)</td>
              <td>2 兆 token</td>
            </tr>
            <tr>
              <td><strong>主要創新</strong></td>
              <td>端到端訓練</td>
              <td>Chunked Cross-Attention</td>
            </tr>
            <tr>
              <td><strong>參數效率</strong></td>
              <td>標準</td>
              <td>25× 提升</td>
            </tr>
          </tbody>
        </table>
      </div>

      <!-- 延伸閱讀 -->
      <div class="quick-links" style="margin-top: 40px;">
        <a href="02-rag-original.html" class="quick-link">← 上一篇：RAG 原始論文</a>
        <a href="index.html" class="quick-link">回到 RAG 總覽</a>
        <a href="04-self-rag.html" class="quick-link">下一篇：Self-RAG →</a>
      </div>

      <div class="quick-links" style="margin-top: 15px;">
        <a href="https://arxiv.org/abs/2112.04426" class="quick-link" target="_blank">📄 RETRO 論文 (arXiv)</a>
        <a href="https://deepmind.google/research/publications/improving-language-models-by-retrieving-from-trillions-of-tokens" class="quick-link" target="_blank">🔗 DeepMind Research</a>
      </div>
    </div>
  </body>
</html>
