<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å¾æ–‡å­—åˆ°å‘é‡:Tokenizer èˆ‡ Embedding å®Œå…¨è§£æ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        .pipeline-stage {
            background: white;
            padding: 30px;
            border-radius: var(--radius-lg);
            box-shadow: var(--shadow-md);
            margin: 20px 0;
            border-left: 5px solid var(--primary-color);
        }
        .stage-number {
            display: inline-block;
            width: 40px;
            height: 40px;
            background: var(--primary-color);
            color: white;
            border-radius: 50%;
            text-align: center;
            line-height: 40px;
            font-weight: bold;
            margin-right: 10px;
        }
        .code-demo {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 20px;
            border-radius: var(--radius-md);
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
        }
        .code-demo .comment { color: #6a9955; }
        .code-demo .keyword { color: #569cd6; }
        .code-demo .string { color: #ce9178; }
        .code-demo .number { color: #b5cea8; }
        .matrix-visual {
            display: grid;
            gap: 10px;
            margin: 20px 0;
        }
        .vector-vis {
            display: flex;
            gap: 3px;
            flex-wrap: wrap;
            margin: 10px 0;
        }
        .vector-cell {
            width: 30px;
            height: 30px;
            background: var(--primary-color);
            border-radius: 3px;
            font-size: 0.7rem;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
        }
        .paper-timeline {
            position: relative;
            padding: 20px 0;
        }
        .paper-item {
            background: white;
            padding: 20px;
            border-left: 4px solid var(--primary-color);
            margin: 20px 0;
            border-radius: var(--radius-md);
            box-shadow: var(--shadow-sm);
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <span class="current">ğŸ”¬ æŠ€è¡“å°ˆé¡Œ: Tokenizer & Embedding</span>
        </div>

        <h1>ğŸ”¤ å¾æ–‡å­—åˆ°å‘é‡:Tokenizer èˆ‡ Embedding å®Œå…¨è§£æ</h1>
        
        <div class="key-concept">
            <h3>ğŸ¯ æ ¸å¿ƒå•é¡Œ</h3>
            <p><strong>Q: å¦‚ä½•å°‡ã€Œæˆ‘æ„› AIã€è½‰æ›æˆ 512 ç¶­å‘é‡ï¼Ÿ</strong></p>
            <p><strong>A: éœ€è¦å…©å€‹æ­¥é©Ÿï¼</strong></p>
            <ol>
                <li><strong>Tokenization</strong>: æ–‡å­— â†’ Token IDs (æ•´æ•¸)</li>
                <li><strong>Embedding</strong>: Token IDs â†’ é«˜ç¶­å‘é‡ (512/12288 ç¶­)</li>
            </ol>
            <p style="color: var(--danger-color); font-weight: bold;">âš ï¸ é—œéµ:Tokenizer â‰  Embeddingï¼é€™æ˜¯å…©å€‹å®Œå…¨ä¸åŒçš„æ­¥é©Ÿï¼</p>
        </div>

        <h2>ğŸ“Š å®Œæ•´æµç¨‹åœ–</h2>

        <div style="margin: 40px 0; text-align: center;">
            <img src="../images/user_generate_image_20260101095157_cd40.png" 
                 alt="æ–‡å­—åˆ°å‘é‡å®Œæ•´æµç¨‹" 
                 style="max-width: 100%; border-radius: var(--radius-lg); box-shadow: var(--shadow-lg);">
            <p class="caption">
                <strong>ğŸ”„ å¾æ–‡å­—åˆ°å‘é‡çš„å®Œæ•´è½‰æ›æµç¨‹</strong><br>
                æ­¥é©Ÿ 1 (è—):Tokenization â†’ æ­¥é©Ÿ 2 (ç¶ ):Token IDs<br>
                æ­¥é©Ÿ 3 (ç´«):Embedding Lookup â†’ æ­¥é©Ÿ 4 (æ©™):Positional Encoding<br>
                æœ€çµ‚:æ¯å€‹ token éƒ½è®Šæˆ <strong>512 ç¶­å‘é‡</strong>ï¼
            </p>
        </div>

        <div class="explanation" style="background: linear-gradient(135deg, var(--primary-light), var(--secondary-light)); padding: 30px;">
            <pre style="background: white; padding: 20px; border-radius: var(--radius-md); line-height: 2;">
åŸå§‹æ–‡å­—: <strong>"I love AI"</strong>
    â†“
<span style="color: var(--primary-color);">ã€æ­¥é©Ÿ 1: Tokenizationã€‘</span>
åˆ†è©å™¨å°‡æ–‡å­—åˆ‡æˆ tokens
    â†“
Tokens: <strong>["I", "love", "AI"]</strong>
    â†“
æŸ¥è©¢è©å½™è¡¨ (Vocabulary)
    â†“
Token IDs: <strong>[1045, 2293, 9932]</strong>  â† é€™åªæ˜¯æ•´æ•¸ï¼
    â†“
<span style="color: var(--secondary-color);">ã€æ­¥é©Ÿ 2: Embeddingã€‘</span>
æŸ¥è©¢ Embedding Matrix (å­¸ç¿’å¾—åˆ°)
    â†“
Token Embeddings: 
  [1045] â†’ [0.23, -0.51, 0.82, ..., 0.15]  (512 ç¶­)
  [2293] â†’ [0.67, 0.31, -0.22, ..., 0.91]  (512 ç¶­)
  [9932] â†’ [-0.12, 0.64, 0.41, ..., -0.33] (512 ç¶­)
    â†“
<span style="color: var(--accent-color);">ã€æ­¥é©Ÿ 3: åŠ ä¸Šä½ç½®è³‡è¨Šã€‘</span>
Position Encoding
    â†“
æœ€çµ‚è¼¸å…¥: <strong>æ¯å€‹ token éƒ½æ˜¯ 512 ç¶­å‘é‡ï¼</strong>
</pre>
        </div>

        <h2>ğŸ” æ­¥é©Ÿ 1: Tokenization (åˆ†è©)</h2>

        <div class="pipeline-stage">
            <h3><span class="stage-number">1</span>ä»€éº¼æ˜¯ Tokenization?</h3>
            
            <p><strong>å®šç¾©:</strong>å°‡é€£çºŒçš„æ–‡å­—åˆ‡åˆ†æˆã€Œtokensã€(åŸºæœ¬å–®ä½)</p>
            
            <div class="explanation">
                <h4>ä¸‰ç¨®ä¸»æµæ–¹æ³•</h4>
                
                <h5>1ï¸âƒ£ Word-Level Tokenization (è©ç´š)</h5>
                <p><strong>è«–æ–‡:</strong>Word2Vec (2013), GloVe (2014)</p>
                <div class="code-demo">
<span class="string">"I love artificial intelligence"</span>
â†’ [<span class="string">"I"</span>, <span class="string">"love"</span>, <span class="string">"artificial"</span>, <span class="string">"intelligence"</span>]
                </div>
                <p><strong>å•é¡Œ:</strong></p>
                <ul>
                    <li>è©å½™è¡¨å¤ªå¤§ (è‹±æ–‡æœ‰å¹¾åè¬å€‹è©)</li>
                    <li>æœªç™»éŒ„è© (OOV) å•é¡Œ:ã€ŒChatGPTã€ç„¡æ³•è™•ç†</li>
                    <li>ç„¡æ³•è™•ç†å½¢æ…‹è®ŠåŒ–:ã€Œrunã€ã€Œrunningã€ã€Œranã€éƒ½æ˜¯ä¸åŒè©</li>
                </ul>

                <h5>2ï¸âƒ£ Character-Level Tokenization (å­—å…ƒç´š)</h5>
                <div class="code-demo">
<span class="string">"AI"</span>
â†’ [<span class="string">"A"</span>, <span class="string">"I"</span>]
                </div>
                <p><strong>å„ªé»:</strong>è©å½™è¡¨å¾ˆå° (åªæœ‰å¹¾åå€‹å­—æ¯)</p>
                <p><strong>å•é¡Œ:</strong>åºåˆ—å¤ªé•·,å¤±å»è©ç¾©è³‡è¨Š</p>

                <h5>3ï¸âƒ£ Subword Tokenization (å­è©ç´š) â­</h5>
                <p><strong>ç¾ä»£æ¨¡å‹éƒ½ç”¨é€™å€‹ï¼</strong></p>
                <div class="code-demo">
<span class="string">"ChatGPT"</span>
â†’ [<span class="string">"Chat"</span>, <span class="string">"G"</span>, <span class="string">"PT"</span>]

<span class="string">"unbelievable"</span>
â†’ [<span class="string">"un"</span>, <span class="string">"believ"</span>, <span class="string">"able"</span>]
                </div>
                <p><strong>å„ªé»:</strong></p>
                <ul>
                    <li>âœ… è©å½™è¡¨å¤§å°é©ä¸­ (é€šå¸¸ 30,000-50,000)</li>
                    <li>âœ… æ²’æœ‰ OOV å•é¡Œ</li>
                    <li>âœ… ä¿ç•™è©ç¾©è³‡è¨Š</li>
                </ul>
            </div>
        </div>

        <h2>ğŸ“š é‡è¦è«–æ–‡èˆ‡æŠ€è¡“æ¼”é€²</h2>

        <div class="paper-timeline">
            <div class="paper-item" style="border-left-color: #e74c3c;">
                <h4>ğŸ“„ Word2Vec (2013)</h4>
                <p><strong>ä½œè€…:</strong> Mikolov et al. (Google)</p>
                <p><strong>è«–æ–‡:</strong> "Efficient Estimation of Word Representations in Vector Space"</p>
                <p><strong>è²¢ç»:</strong></p>
                <ul>
                    <li>æå‡º <strong>Word Embedding</strong> æ¦‚å¿µ</li>
                    <li>å°‡è©æ˜ å°„åˆ°é€£çºŒå‘é‡ç©ºé–“ (é€šå¸¸ 100-300 ç¶­)</li>
                    <li>è©ç¾©ç›¸è¿‘çš„è©åœ¨å‘é‡ç©ºé–“ä¸­ä¹Ÿæ¥è¿‘</li>
                </ul>
                <p><strong>æ–¹æ³•:</strong>Word-Level Tokenization</p>
                <div class="code-demo">
<span class="comment"># Word2Vec ç¯„ä¾‹</span>
<span class="string">"king"</span>   â†’ [<span class="number">0.23</span>, <span class="number">-0.51</span>, <span class="number">0.82</span>, ..., <span class="number">0.15</span>]  (<span class="number">300</span> ç¶­)
<span class="string">"queen"</span>  â†’ [<span class="number">0.21</span>, <span class="number">-0.49</span>, <span class="number">0.84</span>, ..., <span class="number">0.13</span>]  (ç›¸è¿‘!)
<span class="string">"man"</span>    â†’ [<span class="number">0.67</span>, <span class="number">0.31</span>, <span class="number">-0.22</span>, ..., <span class="number">0.91</span>]
                </div>
                <p><strong>é™åˆ¶:</strong>åªæ˜¯ embedding,æ²’æœ‰æ·±åº¦æ¨¡å‹</p>
            </div>

            <div class="paper-item" style="border-left-color: #3498db;">
                <h4>ğŸ“„ BPE - Byte Pair Encoding (2016)</h4>
                <p><strong>ä½œè€…:</strong> Sennrich et al.</p>
                <p><strong>è«–æ–‡:</strong> "Neural Machine Translation of Rare Words with Subword Units"</p>
                <p><strong>è²¢ç»:</strong>ç¬¬ä¸€å€‹å¯¦ç”¨çš„ <strong>Subword Tokenization</strong> æ–¹æ³•</p>
                <p><strong>æ ¸å¿ƒç®—æ³•:</strong></p>
                <ol>
                    <li>å¾å­—å…ƒç´šé–‹å§‹</li>
                    <li>åè¦†åˆä½µæœ€å¸¸å‡ºç¾çš„ pair</li>
                    <li>ç›´åˆ°é”åˆ°ç›®æ¨™è©å½™è¡¨å¤§å°</li>
                </ol>
                <div class="code-demo">
<span class="comment"># BPE è¨“ç·´éç¨‹ç¯„ä¾‹</span>
åˆå§‹: <span class="string">"l o w"</span>, <span class="string">"l o w e r"</span>, <span class="string">"n e w e s t"</span>
    â†“ åˆä½µ (<span class="string">"e"</span>, <span class="string">"r"</span>) æœ€å¸¸è¦‹
<span class="string">"l o w"</span>, <span class="string">"l o w er"</span>, <span class="string">"n e w e s t"</span>
    â†“ åˆä½µ (<span class="string">"er"</span>, <span class="string">"s"</span>)...
...
                </div>
                <p><strong>GPT-2 å’Œ GPT-3 éƒ½ç”¨ BPEï¼</strong></p>
            </div>

            <div class="paper-item" style="border-left-color: #2ecc71;">
                <h4>ğŸ“„ Transformer (2017) â­</h4>
                <p><strong>è«–æ–‡:</strong> "Attention Is All You Need"</p>
                <p><strong>Embedding ç´°ç¯€:</strong></p>
                <ul>
                    <li><strong>Tokenizer:</strong> BPE (è©å½™è¡¨ ~37,000)</li>
                    <li><strong>d_model:</strong> 512 ç¶­</li>
                    <li><strong>Embedding Matrix:</strong> [vocab_size, d_model] = [37000, 512]</li>
                </ul>
                <div class="code-demo">
<span class="comment"># Transformer Embedding éç¨‹</span>
<span class="keyword">class</span> <span class="string">TokenEmbedding</span>:
    <span class="keyword">def</span> __init__(<span class="keyword">self</span>):
        <span class="comment"># å¯å­¸ç¿’çš„åƒæ•¸çŸ©é™£</span>
        <span class="keyword">self</span>.embedding = nn.Embedding(
            num_embeddings=<span class="number">37000</span>,  <span class="comment"># è©å½™è¡¨å¤§å°</span>
            embedding_dim=<span class="number">512</span>      <span class="comment"># å‘é‡ç¶­åº¦</span>
        )
    
    <span class="keyword">def</span> forward(<span class="keyword">self</span>, token_ids):
        <span class="comment"># token_ids: [batch, seq_len]</span>
        <span class="comment"># è¼¸å‡º: [batch, seq_len, 512]</span>
        <span class="keyword">return</span> <span class="keyword">self</span>.embedding(token_ids)
                </div>
                <p><strong>é—œéµå‰µæ–°:</strong>Embedding æ˜¯<strong>å¯å­¸ç¿’çš„åƒæ•¸</strong>,éš¨è‘—æ¨¡å‹è¨“ç·´è€Œå„ªåŒ–ï¼</p>
            </div>

            <div class="paper-item" style="border-left-color: #9b59b6;">
                <h4>ğŸ“„ BERT (2018)</h4>
                <p><strong>è«–æ–‡:</strong> "BERT: Pre-training of Deep Bidirectional Transformers"</p>
                <p><strong>Tokenizer:</strong> WordPiece (Google é–‹ç™¼)</p>
                <ul>
                    <li><strong>è©å½™è¡¨:</strong> 30,522 tokens</li>
                    <li><strong>d_model (Base):</strong> 768 ç¶­</li>
                    <li><strong>d_model (Large):</strong> 1024 ç¶­</li>
                </ul>
                <p><strong>ç‰¹æ®Š tokens:</strong></p>
                <div class="code-demo">
[CLS] <span class="comment"># å¥å­é–‹é ­,ç”¨æ–¼åˆ†é¡ä»»å‹™</span>
[SEP] <span class="comment"># å¥å­åˆ†éš”ç¬¦</span>
[MASK] <span class="comment"># é®è”½ token,ç”¨æ–¼é è¨“ç·´</span>
[PAD] <span class="comment"># å¡«å……,è®“åºåˆ—é•·åº¦ä¸€è‡´</span>
                </div>
                <p><strong>Embedding çµ„æˆ:</strong></p>
                <div class="code-demo">
æœ€çµ‚ Embedding = Token Embedding 
               + Segment Embedding  <span class="comment"># å€åˆ†å¥å­ A/B</span>
               + Position Embedding <span class="comment"># ä½ç½®è³‡è¨Š</span>
                </div>
            </div>

            <div class="paper-item" style="border-left-color: #e67e22;">
                <h4>ğŸ“„ GPT-3 (2020)</h4>
                <p><strong>è«–æ–‡:</strong> "Language Models are Few-Shot Learners"</p>
                <p><strong>Tokenizer:</strong> BPE (è·Ÿ GPT-2 ç›¸åŒ)</p>
                <ul>
                    <li><strong>è©å½™è¡¨:</strong> 50,257 tokens</li>
                    <li><strong>d_model:</strong> 12,288 ç¶­ (175B ç‰ˆæœ¬)</li>
                    <li><strong>Embedding Matrix:</strong> [50257, 12288] = <strong>6.15 å„„å€‹åƒæ•¸</strong>!</li>
                </ul>
                <div class="code-demo">
<span class="comment"># GPT-3 çš„ Embedding</span>
token_embedding = Embedding(<span class="number">50257</span>, <span class="number">12288</span>)
position_embedding = Embedding(<span class="number">2048</span>, <span class="number">12288</span>)  <span class="comment"># æœ€å¤š 2048 tokens</span>

<span class="comment"># å‰å‘å‚³æ’­</span>
x = token_embedding(input_ids) + position_embedding(positions)
<span class="comment"># x.shape = [batch, seq_len, 12288]</span>
                </div>
            </div>
        </div>

        <h2>ğŸ”¬ æ·±å…¥ç†è§£:Embedding Matrix</h2>

        <div class="explanation">
            <h4>Embedding Matrix æ˜¯ä»€éº¼?</h4>
            
            <p><strong>æœ¬è³ª:</strong>ä¸€å€‹å·¨å¤§çš„æŸ¥è©¢è¡¨ (Lookup Table)</p>

            <h5>Transformer (512 ç¶­) çš„ Embedding Matrix</h5>
            <div class="code-demo">
Shape: [vocab_size, d_model] = [<span class="number">37000</span>, <span class="number">512</span>]

è¦–è¦ºåŒ–:
     ç¶­åº¦ â†’  [ 0 ]  [ 1 ]  [ 2 ]  ...  [511]
Token ID
   [  0 ] â†’ [<span class="number">0.12</span>][<span class="number">-0.34</span>][ <span class="number">0.56</span>] ... [<span class="number">0.78</span>]  â† [PAD] token
   [  1 ] â†’ [<span class="number">0.23</span>][ <span class="number">0.45</span>][<span class="number">-0.12</span>] ... [<span class="number">-0.34</span>]  â† [START] token
   [  2 ] â†’ [<span class="number">-0.45</span>][ <span class="number">0.67</span>][ <span class="number">0.89</span>] ... [<span class="number">0.12</span>]  â† "the"
   [ 1045] â†’ [<span class="number">0.67</span>][ <span class="number">0.31</span>][<span class="number">-0.22</span>] ... [<span class="number">0.91</span>]  â† "I"
   [ 2293] â†’ [<span class="number">0.82</span>][<span class="number">-0.15</span>][ <span class="number">0.43</span>] ... [<span class="number">-0.56</span>]  â† "love"
   ...
   [36999] â†’ [<span class="number">-0.12</span>][ <span class="number">0.34</span>][<span class="number">-0.56</span>] ... [<span class="number">0.78</span>]  â† æœ€å¾Œä¸€å€‹ token

ç¸½åƒæ•¸: <span class="number">37000</span> Ã— <span class="number">512</span> = <strong><span class="number">18,944,000</span> å€‹åƒæ•¸</strong>
            </div>

            <h5>GPT-3 (12288 ç¶­) çš„ Embedding Matrix</h5>
            <div class="code-demo">
Shape: [<span class="number">50257</span>, <span class="number">12288</span>]

ç¸½åƒæ•¸: <span class="number">50257</span> Ã— <span class="number">12288</span> = <strong><span class="number">617,558,016</span> å€‹åƒæ•¸</strong>
<span class="comment"># å…‰æ˜¯ Embedding å°±æœ‰ 6.17 å„„åƒæ•¸!</span>
<span class="comment"># ä½” GPT-3 ç¸½åƒæ•¸ (175B) çš„ 0.35%</span>
            </div>
        </div>

        <h2>ğŸ’» å¯¦éš›ç¨‹å¼ç¢¼ç¯„ä¾‹</h2>

        <div class="pipeline-stage">
            <h3>å¾æ–‡å­—åˆ°å‘é‡:å®Œæ•´æµç¨‹</h3>

            <h4>ç¯„ä¾‹ 1: Transformer (512 ç¶­)</h4>
            <div class="code-demo">
<span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn

<span class="comment"># æ­¥é©Ÿ 1: Tokenization (å‡è¨­å·²ç¶“å®Œæˆ)</span>
text = <span class="string">"I love AI"</span>
token_ids = [<span class="number">1045</span>, <span class="number">2293</span>, <span class="number">9932</span>]  <span class="comment"># æŸ¥è¡¨å¾—åˆ°</span>
token_ids = torch.tensor(token_ids).unsqueeze(<span class="number">0</span>)  <span class="comment"># [1, 3]</span>

<span class="comment"># æ­¥é©Ÿ 2: Token Embedding</span>
vocab_size = <span class="number">37000</span>
d_model = <span class="number">512</span>

token_embedding = nn.Embedding(vocab_size, d_model)
embedded = token_embedding(token_ids)
<span class="keyword">print</span>(embedded.shape)  <span class="comment"># torch.Size([1, 3, 512])</span>

<span class="comment"># æ­¥é©Ÿ 3: åŠ ä¸Šä½ç½®ç·¨ç¢¼</span>
<span class="keyword">def</span> positional_encoding(seq_len, d_model):
    <span class="comment"># Transformer ä½¿ç”¨ sin/cos å‡½æ•¸</span>
    position = torch.arange(seq_len).unsqueeze(<span class="number">1</span>)
    div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * 
                        -(<span class="number">9.210340371976184</span> / d_model))
    pe = torch.zeros(seq_len, d_model)
    pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)
    pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)
    <span class="keyword">return</span> pe

pos_encoding = positional_encoding(<span class="number">3</span>, d_model)
final_input = embedded + pos_encoding
<span class="keyword">print</span>(final_input.shape)  <span class="comment"># torch.Size([1, 3, 512])</span>

<span class="comment"># å®Œæˆï¼é€™å°±æ˜¯é€é€² Transformer çš„è¼¸å…¥</span>
            </div>

            <h4>ç¯„ä¾‹ 2: GPT-3 (12288 ç¶­)</h4>
            <div class="code-demo">
<span class="comment"># GPT-3 ä½¿ç”¨å­¸ç¿’å¼ä½ç½®ç·¨ç¢¼</span>
vocab_size = <span class="number">50257</span>
d_model = <span class="number">12288</span>
max_seq_len = <span class="number">2048</span>

<span class="comment"># Token Embedding</span>
token_embedding = nn.Embedding(vocab_size, d_model)

<span class="comment"># Position Embedding (å¯å­¸ç¿’)</span>
position_embedding = nn.Embedding(max_seq_len, d_model)

<span class="comment"># å‰å‘å‚³æ’­</span>
token_ids = torch.tensor([[<span class="number">1045</span>, <span class="number">2293</span>, <span class="number">9932</span>]])  <span class="comment"># [1, 3]</span>
positions = torch.arange(<span class="number">3</span>).unsqueeze(<span class="number">0</span>)  <span class="comment"># [1, 3]: [0, 1, 2]</span>

x = token_embedding(token_ids) + position_embedding(positions)
<span class="keyword">print</span>(x.shape)  <span class="comment"># torch.Size([1, 3, 12288])</span>

<span class="comment"># æ¯å€‹ token ç¾åœ¨æ˜¯ 12288 ç¶­å‘é‡ï¼</span>
            </div>
        </div>

        <h2>ğŸ¯ è¨“ç·´éç¨‹:Embedding å¦‚ä½•å­¸ç¿’?</h2>

        <div class="explanation">
            <h4>Embedding ä¸æ˜¯æ‰‹å‹•è¨­è¨ˆçš„,æ˜¯<strong>è¨“ç·´</strong>å¾—åˆ°çš„ï¼</h4>

            <h5>åˆå§‹åŒ– (è¨“ç·´å‰)</h5>
            <div class="code-demo">
<span class="comment"># éš¨æ©Ÿåˆå§‹åŒ–</span>
embedding = nn.Embedding(<span class="number">50257</span>, <span class="number">12288</span>)
<span class="comment"># æ¯å€‹ token çš„å‘é‡éƒ½æ˜¯éš¨æ©Ÿæ•¸</span>
<span class="keyword">print</span>(embedding.weight[<span class="number">1045</span>])  
<span class="comment"># tensor([0.0234, -0.0123, 0.0456, ...])  â† éš¨æ©Ÿå€¼</span>
            </div>

            <h5>è¨“ç·´éç¨‹</h5>
            <pre style="background: white; padding: 20px; border-radius: var(--radius-md); line-height: 1.8;">
1. è¼¸å…¥ä¸€æ‰¹æ–‡å­—,ç¶“é Tokenization â†’ Token IDs
2. æŸ¥è©¢ Embedding Matrix â†’ å‘é‡
3. é€šé Transformer å±¤
4. é æ¸¬ä¸‹ä¸€å€‹ token
5. è¨ˆç®— Loss (é æ¸¬éŒ¯èª¤)
6. <strong>åå‘å‚³æ’­:æ›´æ–°æ‰€æœ‰åƒæ•¸,åŒ…æ‹¬ Embedding Matrixï¼</strong>
7. é‡è¤‡æ•¸ç™¾è¬æ¬¡...

çµæœ:
- ç›¸ä¼¼è©çš„ embedding å‘é‡æœƒè®Šå¾—æ¥è¿‘
- ä¸åŒè©çš„ embedding å‘é‡æœƒè®Šå¾—é é›¢
- Embedding è‡ªå‹•å­¸æœƒèªç¾©é—œä¿‚ï¼
            </pre>

            <h5>è¨“ç·´å¾Œ (æ”¶æ–‚)</h5>
            <div class="code-demo">
<span class="comment"># èªç¾©ç›¸è¿‘çš„è©,å‘é‡ä¹Ÿæ¥è¿‘</span>
embedding.weight[<span class="number">1045</span>]  <span class="comment"># "I"</span>
<span class="comment"># tensor([0.67, 0.31, -0.22, ...])</span>

embedding.weight[<span class="number">2293</span>]  <span class="comment"># "you"</span>
<span class="comment"># tensor([0.65, 0.29, -0.24, ...])  â† æ¥è¿‘!</span>

embedding.weight[<span class="number">9932</span>]  <span class="comment"># "dog"</span>
<span class="comment"># tensor([-0.12, 0.84, 0.56, ...])  â† å®Œå…¨ä¸åŒ</span>
            </div>
        </div>

        <h2>ğŸ“ ç¶­åº¦å¾ 512 æ“´å¤§åˆ° 12288 çš„æ„ç¾©</h2>

        <div class="key-concept">
            <h4>æ›´é«˜ç¶­åº¦ = æ›´è±å¯Œçš„è¡¨é”ç©ºé–“</h4>

            <table>
                <thead>
                    <tr>
                        <th>ç¶­åº¦</th>
                        <th>è¡¨é”èƒ½åŠ›</th>
                        <th>é¡æ¯”</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>2 ç¶­</strong></td>
                        <td>åªèƒ½åœ¨å¹³é¢ä¸Šå€åˆ†</td>
                        <td>ğŸ“„ ä¸€å¼µç´™</td>
                    </tr>
                    <tr>
                        <td><strong>3 ç¶­</strong></td>
                        <td>å¯ä»¥ç”¨é«˜åº¦å€åˆ†</td>
                        <td>ğŸ“¦ ä¸€å€‹ç›’å­</td>
                    </tr>
                    <tr>
                        <td><strong>512 ç¶­</strong></td>
                        <td>èƒ½ç·¨ç¢¼åŸºæœ¬èªç¾©é—œä¿‚</td>
                        <td>ğŸ  ä¸€æ£Ÿæˆ¿å­</td>
                    </tr>
                    <tr>
                        <td><strong>12,288 ç¶­</strong></td>
                        <td>èƒ½ç·¨ç¢¼æ¥µå…¶ç´°å¾®çš„å·®ç•°</td>
                        <td>ğŸ™ï¸ ä¸€æ•´åº§åŸå¸‚</td>
                    </tr>
                </tbody>
            </table>

            <p><strong>å¯¦éš›æ•ˆæœ:</strong></p>
            <ul>
                <li>512 ç¶­:èƒ½åˆ†è¾¨ã€Œå¿«æ¨‚ã€vsã€Œæ‚²å‚·ã€</li>
                <li>12,288 ç¶­:èƒ½åˆ†è¾¨ã€Œæ¬£å–œè‹¥ç‹‚ã€vsã€Œå¿ƒæ»¿æ„è¶³ã€vsã€Œæ·ºæ·ºå¾®ç¬‘ã€</li>
            </ul>
        </div>

        <h2>ğŸ“ ç¸½çµ:é—œéµæŠ€è¡“çš„è«–æ–‡ä¾†æº</h2>

        <div class="key-concept">
            <table>
                <thead>
                    <tr>
                        <th>æŠ€è¡“</th>
                        <th>è«–æ–‡</th>
                        <th>å¹´ä»½</th>
                        <th>è²¢ç»</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Word Embedding</strong></td>
                        <td>Word2Vec</td>
                        <td>2013</td>
                        <td>æå‡ºè©å‘é‡æ¦‚å¿µ</td>
                    </tr>
                    <tr>
                        <td><strong>Subword Tokenization</strong></td>
                        <td>BPE (Sennrich et al.)</td>
                        <td>2016</td>
                        <td>è§£æ±º OOV å•é¡Œ</td>
                    </tr>
                    <tr style="background: var(--primary-light);">
                        <td><strong>512 ç¶­ Embedding</strong></td>
                        <td><strong>Transformer</strong></td>
                        <td><strong>2017</strong></td>
                        <td>çµ±ä¸€æ¶æ§‹,å¯å­¸ç¿’ embedding</td>
                    </tr>
                    <tr>
                        <td><strong>768/1024 ç¶­</strong></td>
                        <td>BERT</td>
                        <td>2018</td>
                        <td>å¢åŠ è¡¨é”èƒ½åŠ›</td>
                    </tr>
                    <tr style="background: var(--secondary-light);">
                        <td><strong>12,288 ç¶­ Embedding</strong></td>
                        <td><strong>GPT-3</strong></td>
                        <td><strong>2020</strong></td>
                        <td>æ¥µè‡´è¡¨é”èƒ½åŠ›</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="solution">
            <h4>âœ… å›ç­”ä½ çš„å•é¡Œ</h4>

            <p><strong>Q1: æ€æ¨£å°‡åŸå§‹è³‡æ–™ tokenize åˆ° 512 ç¶­åº¦ï¼Ÿ</strong></p>
            <p><strong>A1:</strong> åˆ†å…©æ­¥ï¼</p>
            <ol>
                <li><strong>Tokenization</strong>: æ–‡å­— â†’ Token IDs (è«–æ–‡: BPE, 2016)</li>
                <li><strong>Embedding</strong>: Token IDs â†’ 512 ç¶­å‘é‡ (è«–æ–‡: Transformer, 2017)</li>
            </ol>

            <p><strong>Q2: è¨“ç·´å¦‚ä½•æ“´å¤§åˆ° 12,288 ç¶­åº¦ï¼Ÿ</strong></p>
            <p><strong>A2:</strong></p>
            <ul>
                <li>ä¸æ˜¯ã€Œæ“´å¤§ã€,è€Œæ˜¯ã€Œé‡æ–°è¨“ç·´ã€</li>
                <li>GPT-3 å¾é›¶é–‹å§‹,ç”¨ [50257, 12288] çš„ Embedding Matrix</li>
                <li>ç¶“éæµ·é‡è³‡æ–™è¨“ç·´å¾Œ,æ¯å€‹ token å­¸æœƒ 12,288 ç¶­çš„è¡¨ç¤º</li>
            </ul>

            <p><strong>Q3: Tokenizer å’Œ Embedding åœ¨å“ªç¯‡è«–æ–‡ï¼Ÿ</strong></p>
            <p><strong>A3:</strong></p>
            <ul>
                <li><strong>Tokenizer (BPE)</strong>: "Neural Machine Translation of Rare Words with Subword Units" (2016)</li>
                <li><strong>Embedding</strong>: "Attention Is All You Need" (Transformer, 2017) ç¬¬ä¸€æ¬¡å®Œæ•´æ‡‰ç”¨</li>
                <li><strong>512 â†’ 12,288 æ¼”é€²</strong>: Transformer â†’ BERT â†’ GPT-3</li>
            </ul>
        </div>

        <div style="background: linear-gradient(135deg, var(--primary-light), var(--secondary-light)); padding: 30px; border-radius: var(--radius-lg); text-align: center; margin: 40px 0;">
            <h3>ğŸ¯ æ ¸å¿ƒæ´å¯Ÿ</h3>
            <p style="font-size: 1.2rem; line-height: 1.8;">
                <strong>Tokenizer</strong> åªæ˜¯ã€ŒæŸ¥å­—å…¸ã€,æŠŠæ–‡å­—è®Šæˆæ•¸å­—<br>
                <strong>Embedding</strong> æ‰æ˜¯é—œéµ,æŠŠæ•¸å­—è®Šæˆ<strong>æœ‰æ„ç¾©çš„å‘é‡</strong><br><br>
                ç¶­åº¦è¶Šé«˜ â†’ å‘é‡è¶Šç²¾ç´° â†’ AI è¶Šè°æ˜ï¼<br>
                é€™å°±æ˜¯å¾ 512 åˆ° 12,288 çš„ç§˜å¯†ï¼ğŸš€
            </p>
        </div>

        <div class="quick-links">
            <a href="../index.html" class="quick-link">â† å›åˆ°ä¸‰éƒ¨æ›²ç¸½è¦½</a>
            <a href="model-dimension-evolution.html" class="quick-link">ä¸‹ä¸€å€‹å°ˆé¡Œ â†’ ç¶­åº¦æ¼”é€²</a>
            <a href="gpt3-96-layers-architecture.html" class="quick-link">å»¶ä¼¸ â†’ 96å±¤æ¶æ§‹</a>
        </div>
        
        <div class="quick-links" style="margin-top: 20px;">
            <a href="../transformer-tutorial/index.html" class="quick-link">ğŸ“– Transformer æ•™å­¸</a>
            <a href="../bert-tutorial/index.html" class="quick-link">ğŸ“– BERT æ•™å­¸</a>
            <a href="../gpt3-tutorial/index.html" class="quick-link">ğŸ“– GPT-3 æ•™å­¸</a>
        </div>
    </div>
</body>
</html>

