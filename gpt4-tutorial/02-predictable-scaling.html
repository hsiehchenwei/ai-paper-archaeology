<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT-4 第 2 章：可預測擴展 - 從小模型預測大模型</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="hero-section" style="background-image: url('images/chapter02_hero.png'); height: 80vh;">
        <div class="hero-overlay"></div>
        <div class="hero-content">
            <h1>可預測擴展的突破</h1>
            <p class="hero-subtitle">從 1/1000 計算量預測 GPT-4 的性能</p>
            <p class="hero-meta">GPT-4 Technical Report 深度解析 · 第 2 章</p>
        </div>
    </div>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">🏠 首頁</a>
            <span>/</span>
            <a href="index.html">GPT-4 教學</a>
            <span>/</span>
            <span class="current">第 2 章</span>
        </div>

        <div class="story-container">
            <p class="story-lead drop-cap">
                訓練 GPT-4 這樣的大模型需要巨大的計算資源，不可能像訓練小模型那樣進行大量的超參數調優和實驗。
                因此，<strong>可預測擴展（Predictable Scaling）</strong>成為了 GPT-4 專案的核心挑戰：
                如何從使用 1/1000 甚至 1/10000 計算量的小模型，準確預測 GPT-4 的性能？
            </p>

            <div class="section-divider"><span>✦</span></div>

            <p>
                這不僅是技術挑戰，更是<strong>安全與責任的體現</strong>：
            </p>
            <ul>
                <li><strong>訓練前預測</strong>：在投入巨大資源訓練之前，就能預測模型的能力和風險</li>
                <li><strong>安全規劃</strong>：提前了解模型可能的能力，為安全部署做準備</li>
                <li><strong>資源優化</strong>：避免浪費計算資源在效果不佳的配置上</li>
            </ul>

            <p>
                GPT-4 報告展示了兩個關鍵的可預測擴展成果：
                <strong>損失預測 (Loss Prediction)</strong> 和 <strong>能力預測 (Capability Prediction)</strong>。
                這驗證了 Scaling Laws 論文的預言，並將其推向了新的高度。
            </p>
        </div>

        <h2>📐 損失預測 (Loss Prediction)</h2>

    <section id="codebase-definition" class="paper-section">
        <h3>什麼是「代碼庫」(codebase)？</h3>
        <p>在 GPT‑4 Technical Report 的第 2 章「Predictable Scaling」中，<em>codebase</em> 指的是我們內部收集的大規模程式碼資料集（由各種語言的 code tokens 組成），此資料集<strong>未出現在模型的訓練資料裡</strong>，因此可作為純粹測試模型在程式碼上的表現。</p>
        <p>這個代碼庫被用來評估模型的最終損失 (final loss)。報告說：「我們預測 GPT‑4 在內部代碼庫（non‑part of the training set）上的最終損失…」<cite>GPT‑4 Technical Report, §2.1</cite></p>
        <figure class="figure-original">
            <img src="../論文原文/arXiv-2303.08774v6＿GPT-4 Technical Report/assets/codebase_loss.pdf" alt="代碼庫 loss 圖表" style="max-width:100%;height:auto;" />
            <figcaption>圖 1：在內部代碼庫上測得的最終損失與預測曲線（來自原始報告）</figcaption>
        </figure>
    </section>

        <div class="glossary-box">
            <h4>📖 名詞百科</h4>
            <div style="margin-bottom: 15px;">
                <strong>1. 損失 (Loss)：衡量模型「有多笨」的指標</strong>
                <p style="margin-bottom: 10px;">
                    損失是用來衡量模型「預測正確程度」的數值。<strong>Loss 越高</strong>，代表模型預測得越不準；<strong>Loss 越低</strong>，代表模型越精準。
                </p>
            </div>
            <div>
                <strong>2. 不可約損失 (Irreducible Loss)：進步的「天花板」</strong>
                <p>
                    這指的是公式中的常數 <strong>$c$</strong>。你可以把它想像成一個「誤差底線」：就算你給模型無限的計算量和資料，有些誤差還是永遠無法消除（例如：有些問題本身就沒有標準答案，或語言本身存在的模糊性）。
                </p>
            </div>
        </div>

        <div class="paper-section">
            <div class="original-quote">
                <strong>原文</strong>
                <p>
                    The final loss of properly-trained large language models is thought to be well approximated by power laws in the amount of compute used to train the model.
                </p>
                <p>
                    To verify the scalability of our optimization infrastructure, we predicted GPT-4's final loss on our internal codebase (not part of the training set) by fitting a scaling law with an irreducible loss term (as in Henighan et al. 2020): $L(C) = aC^b + c,$ from models trained using the same methodology but using at most 10,000x less compute than GPT-4. This prediction was made shortly after the run started, without use of any partial results. The fitted scaling law predicted GPT-4's final loss with high accuracy (Figure \ref{fig:predictable_scaling_loss}).
                </p>
            </div>

            <div class="translation">
                <h4>📝 中文翻譯</h4>
                <p>
                    學界普遍有一個共識：只要訓練方式正確，大型語言模型的<strong>「最終損失 (Final Loss)」</strong>其實是可以被精準預測的。我們只需要知道訓練時投入了多少計算量，就能透過所謂的「冪律 (Power Law)」公式算出結果。
                </p>
                <p>
                    為了測試我們的系統到底有多能「預測未來」，我們套用了一個包含<strong>「不可約損失項 (Irreducible Loss Term)」</strong>的公式：$L(C) = aC^b + c$。這個公式厲害的地方在於，我們只用計算規模比 GPT-4 小了一萬倍的「超迷你模型」數據來做擬合（Fitting），就能預測出 GPT-4 在我們內部代碼庫上的最終表現。
                </p>
                <p>
                    重點是，這項預測是在 GPT-4 剛開始訓練、什麼成果都還沒看到的時候就做出的。結果令人驚艷：這套數學模型精準地預言了 GPT-4 完工後的損失表現，誤差極小。
                </p>
            </div>

            <div class="figure figure-original">
                <img src="images/original/codebase_loss.png" alt="Loss Prediction">
                <div class="caption">
                    <strong>Figure:</strong> 看看 GPT-4 到底有多好預測。
                    這張圖顯示模型在我們內部的代碼數據集（模型從來沒看過這份資料）上的<strong>最終損失 (Loss)</strong> 表現。
                    <strong>為什麼看 Loss？</strong> 因為比起準確率，Loss 在不同模型規模下更穩定，比較不會有亂跳的雜訊。
                    圖中的虛線是我們根據那些「超小型模型」的表現算出來的預測線。
                    你可以看到，當我們把計算量一路推到 GPT-4 的等級（x 軸的 1），預測結果竟然跟實際表現幾乎完全重合！
                </div>
            </div>

            <div class="key-concept">
                <h4>📐 擴展定律公式</h4>
                <div class="math-block">
                    $$L(C) = aC^b + c$$
                    <p style="margin-top: 10px; font-size: 0.9em; color: var(--text-secondary);">
                        其中：
                        <ul style="margin-top: 10px;">
                            <li>$C$：訓練計算量</li>
                            <li>$a, b$：冪律參數</li>
                            <li>$c$：不可約損失（irreducible loss）</li>
                        </ul>
                    </p>
                </div>
            </div>

            <div class="explanation">
                <h4>💡 關鍵洞察</h4>
                <p>
                    <strong>為什麼選擇代碼庫作為測試數據？</strong>
                </p>
                <ul>
                    <li><strong>數據量大且完全沒見過</strong>：我們使用了全新的代碼數據，確保模型沒辦法靠記憶來「作弊」。</li>
                    <li><strong>Loss 指標更穩定</strong>：比起容易跳動的準確率（Accuracy），Loss 在模型變大時的變化非常平滑，更容易被精準預測。</li>
                    <li><strong>實務相關性高</strong>：寫程式能力是 GPT-4 的強項，代碼數據的 Loss 直接反映了它的邏輯理解能力。</li>
                </ul>
                <p>
                    <strong>預測精準度</strong>：即使只用「萬分之一」的計算量來進行小規模測試，我們還是能精準算出 GPT-4 最終的表現。這證明了 Scaling Laws 不只是理論，更讓訓練大模型從「豪賭」變成了可以預先計算的科學。
                </p>
            </div>
        </div>

        <h2>🎯 能力預測：HumanEval 案例</h2>

        <div class="paper-section">
            <div class="original-quote">
                <strong>原文</strong>
                <p>
                    Having a sense of the capabilities of a model before training can improve decisions around alignment, safety, and deployment. In addition to predicting final loss, we developed methodology to predict more interpretable metrics of capability. One such metric is pass rate on the HumanEval dataset, which measures the ability to synthesize Python functions of varying complexity. We successfully predicted the pass rate on a subset of the HumanEval dataset by extrapolating from models trained with at most $1,000\times$ less compute (Figure \ref{fig:predictable_scaling_humaneval}).
                </p>
            </div>

            <div class="translation">
                <h4>📝 中文翻譯</h4>
                <p>
                    在正式訓練展開前即掌握模型的能力表現，有助於優化關於對齊 (Alignment)、安全性與部署相關的決策。除了預測<strong>最終損失 (Final Loss)</strong> 之外，我們還開發了預測更具可解釋性之「能力指標」的方法。
                </p>
                <p>
                    其中一項指標是 <strong>HumanEval 數據集</strong>的通過率，用以衡量模型合成不同複雜程度 Python 函數的能力。我們僅透過比 GPT-4 計算量少 1,000 倍的模型進行外推 (Extrapolation)，便成功預測了 HumanEval 子集上的通過率。
                </p>
            </div>

            <div class="figure figure-original">
                <img src="images/original/capability_pred.png" alt="Capability Prediction">
                <div class="caption">
                    <strong>Figure:</strong> 寫程式能力也能提早預測？
                    這是模型在 <strong>HumanEval</strong>（Python 程式撰寫測驗）上的表現。
                    我們用對數通過率（Log Pass Rate）來觀察，結果發現：即使只用計算量小 1,000 倍的模型來推算（虛線），
                    我們還是能精準預測出 GPT-4 在解決複雜程式題時的實力。
                </div>
            </div>

            <div class="explanation">
                <h4>💡 關鍵洞察</h4>
                <p>
                    <strong>為什麼預測「能力」比預測「損失」更難？</strong>
                </p>
                <ul>
                    <li><strong>對錯太分明 (Discrete)</strong>：答對就是 1，答錯就是 0。這不像連續的 Loss 可以慢慢下降，能力指標往往會突然躍升。</li>
                    <li><strong>反向擴展 (Inverse Scaling)</strong>：某些任務在模型變大時，表現反而會變差（例如想太多的陷阱題）。</li>
                    <li><strong>低頻信號難捕捉</strong>：當模型還很小時，通過率幾乎是 0，很難從一堆零當中預測出未來的成長曲線。</li>
                </ul>
                <p>
                    <strong>解決方案</strong>：研究團隊使用了對數通過率，並特別鎖定在那些「可以穩定預估」的問題子集上。這讓我們即使在模型還很弱的時候，就能預見它變強後的樣子。
                </p>
            </div>
        </div>

        <h2>🔄 Inverse Scaling 的逆轉</h2>

        <div class="paper-section">
            <div class="original-quote">
                <strong>原文</strong>
                <p>
                    Certain capabilities remain hard to predict. For example, the Inverse Scaling Prize proposed several tasks for which model performance decreases as a function of scale. Similarly to a recent result by Wei et al., we find that GPT-4 reverses this trend, as shown on one of the tasks called Hindsight Neglect in Figure \ref{fig:inverse_scaling}.
                </p>
            </div>

            <div class="translation">
                <h4>📝 中文翻譯</h4>
                <p>
                    某些能力仍然難以預測。例如，Inverse Scaling Prize 提出了幾個任務，
                    其中模型性能隨規模而下降。與 Wei et al. 最近的結果類似，
                    我們發現 GPT-4 逆轉了這一趨勢，如圖 \ref{fig:inverse_scaling} 中名為 Hindsight Neglect 的任務所示。
                </p>
            </div>

            <div class="figure figure-original">
                <img src="images/original/inverse_scaling.png" alt="Inverse Scaling">
                <div class="caption">
                    <strong>Figure:</strong> 那些「大器晚成」的任務：以 Hindsight Neglect 為例。
                    在某些陷阱題中，小模型（如 ada, babbage）往往會越學越糟，出現性能下滑的奇特現象。
                    但好消息是，當模型大到 GPT-4 這個等級時，它會突然「開竅」並逆轉這個趨勢，準確率直接衝上頂峰。
                </div>
            </div>

            <div class="explanation">
                <h4>💡 關鍵洞察</h4>
                <p>
                    <strong>「逆轉趨勢」的意義</strong>：
                </p>
                <ul>
                    <li><strong>大不一定好？</strong> 在某些邏輯陷阱題中，中型模型往往因為「學半套」而比小模型表現更差。</li>
                    <li><strong>GPT-4 的突破</strong>：當規模突破某個臨界點，GPT-4 展現了逆轉趨勢的能力，證明「夠大」確實能解決原本解決不了的問題。</li>
                    <li><strong>預測的極限</strong>：這種「開竅」或「湧現」的能力，目前依然很難從小模型精準預測。</li>
                </ul>
                <p>
                    <strong>實務啟發</strong>：這提醒我們「可預測性」是有邊界的。雖然我們可以算出 Loss 的下降，但模型何時會突然學會某種複雜邏輯，依然帶有一絲神祕感。
                </p>
            </div>
        </div>

        <h2>🔮 與現在 LLM 使用經驗的對照</h2>

        <div class="paper-section">
            <div class="explanation">
                <h4>💡 從論文到現實</h4>
                <p>
                    可預測擴展的技術不僅用於 GPT-4 的開發，也影響了我們今天使用的 AI 服務：
                </p>
                <ul>
                    <li><strong>模型版本迭代</strong>：當 OpenAI 發布 GPT-4 Turbo、GPT-4o 等新版本時，他們可能使用了類似的可預測擴展技術來評估性能</li>
                    <li><strong>API 定價策略</strong>：不同模型版本的定價可能基於可預測的性能差異</li>
                    <li><strong>能力邊界理解</strong>：當我們使用 ChatGPT 時，理解其能力邊界（哪些任務做得好、哪些做不好）正是可預測擴展研究的應用</li>
                    <li><strong>安全部署</strong>：在部署新模型前，通過小模型預測大模型的能力和風險，是負責任 AI 開發的體現</li>
                </ul>
                <p>
                    <strong>理解這項技術的意義</strong>：可預測擴展不僅是技術突破，更是 AI 安全與責任的體現。
                    它讓我們能夠在投入巨大資源之前，就了解模型的能力和風險，
                    為負責任的 AI 開發奠定了基礎。
                </p>
            </div>
        </div>

        <div class="quote-block">
            「可預測擴展不僅是技術突破，更是 AI 安全與責任的體現。
            它讓我們能夠在投入巨大資源之前，就了解模型的能力和風險。」
        </div>

        <div class="navigation">
            <a href="01-introduction.html" class="nav-link">← 上一章：摘要與核心能力</a>
            <a href="03-capabilities.html" class="nav-link">下一章：能力展示 →</a>
        </div>
    </div>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
</body>
</html>

