<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ViT 第 4 章：實驗結果與 SOTA 對比</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="hero-section" style="background-image: url('images/generated/chapter04_hero.png'); height: 80vh;">
        <div class="hero-overlay"></div>
        <div class="hero-content">
            <h1>數據說話</h1>
            <p class="hero-subtitle">超越 SOTA，更少的計算資源</p>
            <p class="hero-meta">Vision Transformer 深度解析 · 第 4 章</p>
        </div>
    </div>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">🏠 首頁</a>
            <span>/</span>
            <a href="index.html">ViT 教學</a>
            <span>/</span>
            <span class="current">第 4 章</span>
        </div>

        <div class="story-container">
            <p class="story-lead drop-cap">
                ViT 的實驗設計非常全面：從資料規模的影響、到與 SOTA 模型的對比、再到計算效率的分析。
                結果顯示，ViT 不僅在準確率上超越 ResNet，更重要的是，它用<strong>更少的計算資源</strong>達到了更好的結果。
            </p>

            <div class="section-divider"><span>✦</span></div>

            <!-- 1. 實驗設定 -->
            <h2>⚙️ 實驗設定</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>資料集</h4>
                    <p>
                        為了探索模型的擴展性，論文使用了三個不同規模的預訓練資料集：
                    </p>
                    <ul>
                        <li><strong>ImageNet</strong>：1,000 類，130 萬張圖片（小規模）</li>
                        <li><strong>ImageNet-21k</strong>：21,000 類，1,400 萬張圖片（中規模）</li>
                        <li><strong>JFT-300M</strong>：18,000 類，3.03 億張高解析度圖片（大規模）</li>
                    </ul>
                    
                    <p>
                        下游任務包括：
                    </p>
                    <ul>
                        <li><strong>ImageNet</strong>（原始標籤和 ReaL 標籤）</li>
                        <li><strong>CIFAR-10/100</strong></li>
                        <li><strong>Oxford-IIIT Pets</strong></li>
                        <li><strong>Oxford Flowers-102</strong></li>
                        <li><strong>VTAB</strong>：19 個任務的套件（Natural、Specialized、Structured）</li>
                    </ul>
                </div>
            </div>

            <!-- 2. SOTA 對比 -->
            <h2>🏆 與 SOTA 模型對比</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>對比基準</h4>
                    <p>
                        論文比較了兩個主要的 SOTA 模型：
                    </p>
                    <ul>
                        <li><strong>Big Transfer (BiT)</strong>：使用大型 ResNet 進行監督式遷移學習</li>
                        <li><strong>Noisy Student</strong>：大型 EfficientNet，使用半監督學習在 ImageNet 和 JFT-300M 上訓練</li>
                    </ul>
                </div>
            </div>

            <div class="figure figure-original">
                <img src="images/original/finetune_vs_compute.png" alt="ViT 計算效率對比">
                <div class="caption">
                    <strong>Figure 2:</strong> 預訓練計算成本 vs 性能。ViT 用更少的計算資源達到更好的性能。
                </div>
            </div>

            <div class="paper-section">
                <div class="explanation">
                    <h4>📊 關鍵結果</h4>
                    <p>
                        以下是 ViT-H/14 (JFT-300M) 與其他模型的對比：
                    </p>
                    
                    <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                        <thead>
                            <tr style="background: var(--mag-bg-light);">
                                <th style="padding: 12px; text-align: left; border-bottom: 2px solid var(--mag-primary);">資料集</th>
                                <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">ViT-H/14<br/>(JFT)</th>
                                <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">ViT-L/16<br/>(JFT)</th>
                                <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">BiT-L<br/>(ResNet)</th>
                                <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">Noisy Student</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td style="padding: 12px; border-bottom: 1px solid #e5e7eb;"><strong>ImageNet</strong></td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb; color: var(--mag-secondary); font-weight: 600;">88.55%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">87.76%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">87.54%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">88.4%</td>
                            </tr>
                            <tr>
                                <td style="padding: 12px; border-bottom: 1px solid #e5e7eb;"><strong>ImageNet-ReaL</strong></td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb; color: var(--mag-secondary); font-weight: 600;">90.72%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">90.54%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">90.54%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">90.55%</td>
                            </tr>
                            <tr>
                                <td style="padding: 12px; border-bottom: 1px solid #e5e7eb;"><strong>CIFAR-100</strong></td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb; color: var(--mag-secondary); font-weight: 600;">94.55%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">93.90%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">93.51%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">-</td>
                            </tr>
                            <tr>
                                <td style="padding: 12px; border-bottom: 1px solid #e5e7eb;"><strong>VTAB (19 tasks)</strong></td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb; color: var(--mag-secondary); font-weight: 600;">77.63%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">76.28%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">76.29%</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">-</td>
                            </tr>
                            <tr>
                                <td style="padding: 12px;"><strong>TPUv3-core-days</strong></td>
                                <td style="padding: 12px; text-align: center; color: var(--mag-secondary); font-weight: 600;">2.5k</td>
                                <td style="padding: 12px; text-align: center; color: var(--mag-secondary); font-weight: 600;">0.68k</td>
                                <td style="padding: 12px; text-align: center;">9.9k</td>
                                <td style="padding: 12px; text-align: center;">12.3k</td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="key-concept" style="background: linear-gradient(135deg, #10b98120 0%, #3b82f620 100%); border-left: 5px solid var(--mag-secondary);">
                        <h5>🎯 關鍵發現</h5>
                        <ul>
                            <li><strong>ViT-L/16 (JFT)</strong> 在所有任務上都超越 BiT-L，但訓練成本只有後者的 <strong>約 7%</strong></li>
                            <li><strong>ViT-H/14</strong> 進一步提升性能，特別是在更具挑戰性的資料集上</li>
                            <li><strong>ViT-L/16 (ImageNet-21k)</strong> 在公開資料集上表現也很好，可以在標準 8 核 TPUv3 上約 30 天完成訓練</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- 3. VTAB 結果 -->
            <div class="figure figure-original">
                <img src="images/original/vit-vtab.png" alt="VTAB 任務分解結果">
                <div class="caption">
                    <strong>Figure 3:</strong> VTAB 任務分解為 Natural、Specialized、Structured 三組的表現對比。
                </div>
            </div>

            <div class="paper-section">
                <div class="explanation">
                    <h4>🔍 VTAB 詳細分析</h4>
                    <p>
                        VTAB (Visual Task Adaptation Benchmark) 包含 19 個任務，分為三組：
                    </p>
                    <ul>
                        <li><strong>Natural</strong>：自然場景任務（如 Pets、CIFAR 等）</li>
                        <li><strong>Specialized</strong>：專業領域任務（醫學影像、衛星影像等）</li>
                        <li><strong>Structured</strong>：需要幾何理解的任務（如定位）</li>
                    </ul>
                    
                    <p>
                        <strong>結果</strong>：
                    </p>
                    <ul>
                        <li>ViT-H/14 在 <strong>Natural</strong> 和 <strong>Structured</strong> 任務上超越 BiT-R152x4</li>
                        <li>在 <strong>Specialized</strong> 任務上，頂尖兩個模型的表現相似</li>
                    </ul>
                </div>
            </div>

            <!-- 4. 資料規模的影響 -->
            <h2>📈 預訓練資料規模的影響</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>小資料集 vs 大資料集</h4>
                    <p>
                        這是 ViT 論文最核心的發現之一：
                    </p>
                </div>
            </div>

            <div class="figure figure-original">
                <img src="images/original/imagenet_5shot.png" alt="資料規模對性能的影響">
                <div class="caption">
                    <strong>Figure 4:</strong> ImageNet 上的線性少樣本評估 vs 預訓練資料規模。ResNet 在小資料集上表現更好，但很快達到平台期；ViT 在大資料集上表現更好。
                </div>
            </div>

            <div class="paper-section">
                <div class="explanation">
                    <div class="key-concept">
                        <h5>❌ 小資料集：ViT 表現差</h5>
                        <p>
                            當預訓練資料集較小時（如 ImageNet，130 萬張）：
                        </p>
                        <ul>
                            <li>大型 ViT 模型的表現<strong>不如 BiT ResNet</strong></li>
                            <li>需要更強的正則化才能避免過擬合</li>
                            <li>這符合預期：ViT 缺乏歸納偏置，需要更多資料來學習</li>
                        </ul>
                    </div>

                    <div class="key-concept" style="background: linear-gradient(135deg, #10b98120 0%, #3b82f620 100%); border-left: 5px solid var(--mag-secondary);">
                        <h5>✅ 大資料集：ViT 開始超越</h5>
                        <p>
                            當預訓練資料集擴大時（ImageNet-21k 或 JFT-300M）：
                        </p>
                        <ul>
                            <li>ViT 開始<strong>超越 ResNet</strong></li>
                            <li>更大的 ViT 模型（如 ViT-H）表現更好</li>
                            <li>ResNet 達到平台期，但 ViT 仍在持續提升</li>
                        </ul>
                        <p>
                            <strong>這證明了論文的核心結論：大規模訓練勝過歸納偏置。</strong>
                        </p>
                    </div>
                </div>
            </div>

            <!-- 5. 計算效率 -->
            <h2>⚡ 計算效率分析</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>為什麼 ViT 更高效？</h4>
                    <p>
                        ViT 的計算效率優勢來自幾個方面：
                    </p>
                    <ul>
                        <li><strong>並行性更好</strong>：Transformer 的 Self-Attention 機制比卷積更容易並行化</li>
                        <li><strong>標準化實現</strong>：可以直接使用 NLP 領域已經高度優化的 Transformer 實作</li>
                        <li><strong>硬體友好</strong>：Transformer 的運算模式更適合現代 TPU/GPU 的架構</li>
                    </ul>
                    
                    <div class="quote-block">
                        「ViT-L/16 的訓練成本只有 BiT-L 的約 7%，但性能更好。」<br>
                        <span style="font-size: 0.9em; opacity: 0.8;">這讓大規模視覺模型的訓練變得更加可行。</span>
                    </div>
                </div>
            </div>

            <!-- 6. 自監督學習的潛力 -->
            <h2>🔮 自監督學習的未來</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>初步實驗</h4>
                    <p>
                        論文還進行了一個小規模的自監督學習實驗，使用類似 BERT 的 Masked Patch Prediction 任務。
                        結果顯示，自監督 ViT 也很有潛力，為未來的無監督視覺學習指明了方向。
                    </p>
                </div>
            </div>

            <!-- Navigation -->
            <div class="chapter-navigation">
                <a href="03-architecture.html" class="nav-link">← 上一章：模型架構</a>
                <a href="index.html" class="nav-link">返回目錄</a>
                <a href="05-scaling-and-data.html" class="nav-link">下一章：資料規模與泛化 →</a>
            </div>
        </div>
    </div>
</body>
</html>
