<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ViT 第 3 章：模型架構全解析</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="hero-section" style="background-image: url('images/generated/chapter03_hero.png'); height: 80vh;">
        <div class="hero-overlay"></div>
        <div class="hero-content">
            <h1>Transformer 的視覺化</h1>
            <p class="hero-subtitle">標準 Transformer Encoder 的完整解析</p>
            <p class="hero-meta">Vision Transformer 深度解析 · 第 3 章</p>
        </div>
    </div>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">🏠 首頁</a>
            <span>/</span>
            <a href="index.html">ViT 教學</a>
            <span>/</span>
            <span class="current">第 3 章</span>
        </div>

        <div class="story-container">
            <p class="story-lead drop-cap">
                ViT 的架構設計遵循一個核心原則：<strong>「盡可能接近原始 Transformer」</strong>。
                這樣做的好處是，所有在 NLP 領域已經優化好的 Transformer 實作，幾乎可以直接拿來用。
                但這也意味著，ViT 必須放棄 CNN 的「歸納偏置」，完全依賴資料來學習視覺結構。
            </p>

            <div class="section-divider"><span>✦</span></div>

            <!-- 1. Transformer Encoder 結構 -->
            <h2>🏗️ Transformer Encoder 結構</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>標準架構</h4>
                    <p>
                        ViT 使用標準的 Transformer Encoder，由多個相同的層（layers）堆疊而成。
                        每一層包含兩個主要組件：
                    </p>
                    
                    <div class="key-concept">
                        <h5>1️⃣ Multi-Head Self-Attention (MSA)</h5>
                        <p>
                            讓每個 patch 都能「看到」其他所有 patches，並根據相似度決定「關注」哪些 patches。
                        </p>
                        <p>
                            <strong>公式（Eq. 2）</strong>：
                        </p>
                        <div style="background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0; font-family: 'JetBrains Mono', monospace; text-align: center;">
                            <strong>z'<sub>ℓ</sub> = MSA(LN(z<sub>ℓ-1</sub>)) + z<sub>ℓ-1</sub></strong>
                        </div>
                        <p>
                            其中：
                        </p>
                        <ul>
                            <li><strong>LN</strong>：Layer Normalization（在注意力之前）</li>
                            <li><strong>MSA</strong>：Multi-Head Self-Attention</li>
                            <li><strong>+ z<sub>ℓ-1</sub></strong>：殘差連接（Residual Connection）</li>
                        </ul>
                    </div>

                    <div class="key-concept">
                        <h5>2️⃣ MLP Block</h5>
                        <p>
                            一個兩層的前饋神經網路，使用 GELU 作為激活函數。
                        </p>
                        <p>
                            <strong>公式（Eq. 3）</strong>：
                        </p>
                        <div style="background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0; font-family: 'JetBrains Mono', monospace; text-align: center;">
                            <strong>z<sub>ℓ</sub> = MLP(LN(z'<sub>ℓ</sub>)) + z'<sub>ℓ</sub></strong>
                        </div>
                        <p>
                            同樣包含 Layer Normalization 和殘差連接。
                        </p>
                    </div>

                    <div class="key-concept">
                        <h5>🔍 Pre-LayerNorm vs Post-LayerNorm</h5>
                        <p>
                            ViT 使用 <strong>Pre-LayerNorm</strong>（在注意力/MLP 之前做 LayerNorm），而不是 Post-LayerNorm（在之後做）。
                            這是現代 Transformer 的標準做法，因為它更穩定，更容易訓練。
                        </p>
                    </div>
                </div>
            </div>

            <!-- 2. 歸納偏置對比 -->
            <h2>⚖️ 歸納偏置：CNN vs ViT</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>什麼是歸納偏置？</h4>
                    <p>
                        <strong>歸納偏置 (Inductive Bias)</strong> 是模型對資料結構的「先驗假設」。
                        它幫助模型在資料不足時也能做出合理的推斷。
                    </p>
                    
                    <div class="key-concept" style="background: linear-gradient(135deg, #3b82f620 0%, #8b5cf620 100%); border-left: 5px solid var(--mag-primary);">
                        <h5>🔵 CNN 的歸納偏置</h5>
                        <ul>
                            <li><strong>局部性 (Locality)</strong>：相近的像素更相關</li>
                            <li><strong>平移等變性 (Translation Equivariance)</strong>：物體移動位置，特徵也跟著移動</li>
                            <li><strong>二維鄰域結構</strong>：卷積核直接利用圖片的空間結構</li>
                        </ul>
                        <p>
                            這些假設「內建」在 CNN 的每一層中，貫穿整個模型。
                        </p>
                    </div>

                    <div class="key-concept" style="background: linear-gradient(135deg, #10b98120 0%, #3b82f620 100%); border-left: 5px solid var(--mag-secondary);">
                        <h5>🟢 ViT 的歸納偏置</h5>
                        <ul>
                            <li><strong>MLP 層</strong>：只有局部性和平移等變性（因為每個 patch 獨立處理）</li>
                            <li><strong>Self-Attention 層</strong>：全局性（每個 patch 都能看到所有其他 patches）</li>
                            <li><strong>二維結構</strong>：只在兩個地方使用：
                                <ul>
                                    <li>切分圖片成 patches（模型開始時）</li>
                                    <li>Fine-tuning 時調整位置編碼（不同解析度的圖片）</li>
                                </ul>
                            </li>
                        </ul>
                        <p>
                            <strong>關鍵差異</strong>：位置編碼在初始化時<strong>不包含任何 2D 位置資訊</strong>。
                            所有 patches 之間的空間關係都必須從資料中學習。
                        </p>
                    </div>

                    <div class="key-concept">
                        <h5>💡 類比：天賦 vs 努力</h5>
                        <p>
                            <strong>CNN</strong>：像是一個「有天賦」的學生，天生就理解空間結構。
                            在少量練習（小資料集）下就能表現優異。
                        </p>
                        <p>
                            <strong>ViT</strong>：像是一個「普通」學生，沒有先天的空間理解能力。
                            但透過海量練習（大規模資料），最終也能超越有天賦的學生。
                        </p>
                        <p>
                            這正是論文的核心結論：<strong>「Large scale training trumps inductive bias」</strong>。
                        </p>
                    </div>
                </div>
            </div>

            <!-- 3. 模型變體 -->
            <h2>📊 模型變體：Base, Large, Huge</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>三種規模</h4>
                    <p>
                        ViT 的模型配置直接參考 BERT，並新增了一個更大的「Huge」版本：
                    </p>
                    
                    <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                        <thead>
                            <tr style="background: var(--mag-bg-light);">
                                <th style="padding: 12px; text-align: left; border-bottom: 2px solid var(--mag-primary);">模型</th>
                                <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">層數 (L)</th>
                                <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">隱藏維度 (D)</th>
                                <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">MLP 大小</th>
                                <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">注意力頭數</th>
                                <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">參數數</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td style="padding: 12px; border-bottom: 1px solid #e5e7eb;"><strong>ViT-Base</strong></td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">12</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">768</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">3072</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">12</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">86M</td>
                            </tr>
                            <tr>
                                <td style="padding: 12px; border-bottom: 1px solid #e5e7eb;"><strong>ViT-Large</strong></td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">24</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">1024</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">4096</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">16</td>
                                <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">307M</td>
                            </tr>
                            <tr>
                                <td style="padding: 12px;"><strong>ViT-Huge</strong></td>
                                <td style="padding: 12px; text-align: center;">32</td>
                                <td style="padding: 12px; text-align: center;">1280</td>
                                <td style="padding: 12px; text-align: center;">5120</td>
                                <td style="padding: 12px; text-align: center;">16</td>
                                <td style="padding: 12px; text-align: center;">632M</td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="key-concept">
                        <h5>📏 Patch Size 的影響</h5>
                        <p>
                            論文使用 <strong>ViT-L/16</strong> 這樣的記號，其中：
                        </p>
                        <ul>
                            <li><strong>L</strong>：Large 模型（24 層）</li>
                            <li><strong>16</strong>：Patch size 是 16×16</li>
                        </ul>
                        <p>
                            <strong>Patch size 越小，序列越長，計算成本越高</strong>：
                        </p>
                        <ul>
                            <li>224×224 圖片，16×16 patches → 196 個 patches</li>
                            <li>224×224 圖片，14×14 patches → 256 個 patches（更貴）</li>
                            <li>224×224 圖片，32×32 patches → 49 個 patches（更便宜，但可能損失細節）</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- 4. Classification Head -->
            <h2>🎯 Classification Head</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>預訓練 vs Fine-tuning</h4>
                    <p>
                        最終的分類結果是從 CLS token 的輸出得到的（Eq. 4）：
                    </p>
                    <div style="background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0; font-family: 'JetBrains Mono', monospace; text-align: center;">
                        <strong>y = LN(z<sub>L</sub><sup>0</sup>)</strong>
                    </div>
                    
                    <div class="key-concept">
                        <h5>🔵 預訓練階段</h5>
                        <p>
                            使用一個 <strong>MLP（一層隱藏層）</strong>作為分類頭，在 ImageNet-21k 或 JFT-300M 上預訓練。
                        </p>
                    </div>

                    <div class="key-concept">
                        <h5>🟢 Fine-tuning 階段</h5>
                        <p>
                            移除預訓練的分類頭，換成一個<strong>零初始化的線性層</strong>（大小為 D×K，K 是下游任務的類別數）。
                        </p>
                        <p>
                            <strong>為什麼零初始化？</strong> 這樣可以確保在 Fine-tuning 初期，模型的輸出接近隨機，不會破壞預訓練的權重。
                        </p>
                    </div>
                </div>
            </div>

            <!-- 5. 訓練目標與 Loss Function -->
            <h2>🎓 訓練目標與 Loss Function</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>ViT 的訓練目標：監督式分類</h4>
                    <p>
                        這是理解 ViT 最關鍵的部分：<strong>ViT 的訓練目標與 GPT 完全不同</strong>。
                    </p>
                    
                    <div class="key-concept" style="background: linear-gradient(135deg, #10b98120 0%, #3b82f620 100%); border-left: 5px solid var(--mag-secondary);">
                        <h5>🎯 ViT 的訓練流程</h5>
                        
                        <div class="figure figure-original" style="margin: 20px 0;">
                            <img src="images/generated/vit_training_flow_diagram.png" alt="ViT 訓練流程圖解">
                            <div class="caption">
                                <strong>圖解：</strong> Vision Transformer 的完整訓練流程，從輸入圖片到輸出分類結果的每個步驟。
                            </div>
                        </div>
                        
                        <div class="figure figure-original" style="margin: 20px 0;">
                            <img src="images/original/model_scheme.png" alt="ViT 模型架構圖（論文 Figure 1）">
                            <div class="caption">
                                <strong>論文 Figure 1:</strong> Vision Transformer 的完整架構。圖片從左到右展示：1) 圖片切分成 patches，2) Linear Projection 和位置編碼，3) Transformer Encoder，4) MLP Head 輸出分類結果。
                            </div>
                        </div>
                        
                        <p>
                            <strong>輸入</strong>：圖片 patches（196 個） + CLS token（1 個） = 197 個 tokens
                        </p>
                        <p>
                            <strong>處理</strong>：經過 L 層 Transformer Encoder，每個 token 都被更新
                        </p>
                        <p>
                            <strong>輸出</strong>：CLS token 的最終狀態（Eq. 4：<code>y = LN(z<sub>L</sub><sup>0</sup>)</code>）
                        </p>
                        <p>
                            <strong>分類</strong>：CLS token 的輸出經過 Classification Head，得到 K 個類別的機率分布
                        </p>
                        <p>
                            <strong>目標</strong>：預測圖片屬於哪個類別（例如：ImageNet 有 1000 個類別）
                        </p>
                    </div>

                    <div class="key-concept">
                        <h5>🔍 CLS Token 的關鍵角色</h5>
                        <p>
                            CLS token 在 ViT 中扮演「資訊聚合器」的角色，類似 BERT 的 <code>[CLS]</code> token。
                            論文 Section 3.1 明確說明：
                        </p>
                        
                        <div class="figure figure-original" style="margin: 20px 0;">
                            <img src="images/generated/vit_cls_token_aggregation.png" alt="CLS Token 資訊聚合機制示意圖">
                            <div class="caption">
                                <strong>圖解：</strong> CLS Token 如何透過 Self-Attention 機制聚合所有 patches 的資訊。每個 patch 的資訊都透過連接線流向 CLS token，最終 CLS token 包含整張圖片的全局資訊。
                            </div>
                        </div>
                        
                        <div class="original-quote" style="margin: 20px 0;">
                            <strong>論文原文（Section 3.1）</strong>
                            <p>
                                "Similar to BERT's <code>[class]</code> token, we prepend a learnable embedding to the sequence of embedded patches (<code>z<sub>0</sub><sup>0</sup> = x<sub>class</sub></code>), whose state at the output of the Transformer encoder (<code>z<sub>L</sub><sup>0</sup></code>) serves as the image representation <code>y</code> (Eq. 4)."
                            </p>
                        </div>
                        
                        <div class="translation" style="margin-bottom: 20px;">
                            <strong>中文翻譯</strong>
                            <p>
                                類似 BERT 的 <code>[class]</code> token，我們在嵌入的 patches 序列前加上一個可學習的嵌入向量（<code>z<sub>0</sub><sup>0</sup> = x<sub>class</sub></code>），其在 Transformer encoder 輸出端的狀態（<code>z<sub>L</sub><sup>0</sup></code>）作為圖片的表徵 <code>y</code>（公式 4）。
                            </p>
                        </div>
                        
                        <ul>
                            <li><strong>初始狀態</strong>：CLS token 是一個可學習的向量，隨機初始化</li>
                            <li><strong>Self-Attention 機制</strong>：透過多層 Self-Attention，CLS token 可以「看到」所有 patches</li>
                            <li><strong>資訊聚合</strong>：每一層的 Attention 機制讓 CLS token 逐漸整合所有 patches 的資訊</li>
                            <li><strong>最終表徵</strong>：經過 L 層後，CLS token 包含了整張圖片的全局資訊</li>
                        </ul>
                        <p>
                            <strong>類比</strong>：CLS token 就像是一個「會議主持人」，在每一層的 Attention 中，它會「詢問」每個 patch：「你看到了什麼？」，然後整合所有資訊，最終做出「這張圖片是什麼」的判斷。
                        </p>
                    </div>

                    <div class="key-concept">
                        <h5>📐 Loss Function：Cross-Entropy</h5>
                        <p>
                            ViT 使用標準的 <strong>Cross-Entropy Loss（交叉熵損失）</strong>進行監督式分類訓練：
                        </p>
                        <div style="background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0; font-family: 'JetBrains Mono', monospace; text-align: center;">
                            <strong>\(\mathcal{L}_{CE} = -\sum_{i=1}^{K} y_i \log(p_i)\)</strong>
                        </div>
                        <p>
                            其中：
                        </p>
                        <ul>
                            <li><strong>K</strong>：類別數量（例如 ImageNet 有 1000 個類別）</li>
                            <li><strong>y<sub>i</sub></strong>：真實標籤（one-hot 編碼，只有正確類別是 1，其他是 0）</li>
                            <li><strong>p<sub>i</sub></strong>：模型預測的類別 i 的機率（經過 Softmax）</li>
                        </ul>
                        <p>
                            <strong>直觀理解</strong>：
                        </p>
                        <ul>
                            <li>如果模型預測正確（p<sub>正確類別</sub> 接近 1），loss 接近 0</li>
                            <li>如果模型預測錯誤（p<sub>正確類別</sub> 接近 0），loss 會很大</li>
                            <li>訓練目標：<strong>最小化 loss</strong>，讓模型更準確地預測正確類別</li>
                        </ul>
                    </div>

                    <div class="key-concept" style="background: linear-gradient(135deg, #3b82f620 0%, #8b5cf620 100%); border-left: 5px solid var(--mag-primary);">
                        <h5>🔄 ViT vs GPT vs BERT：訓練方式對比</h5>
                        <p>
                            這三個模型雖然都使用 Transformer，但訓練目標完全不同：
                        </p>
                        
                        <div class="figure figure-original" style="margin: 20px 0;">
                            <img src="images/generated/vit_gpt_bert_comparison.png" alt="ViT vs GPT vs BERT 訓練方式對比圖">
                            <div class="caption">
                                <strong>圖解：</strong> 三種模型的訓練方式對比。GPT 是自回歸語言模型，BERT 是遮蔽語言模型，ViT 是監督式分類模型。雖然都使用 Transformer 架構，但訓練目標和輸出完全不同。
                            </div>
                        </div>
                        
                        <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                            <thead>
                                <tr style="background: var(--mag-bg-light);">
                                    <th style="padding: 12px; text-align: left; border-bottom: 2px solid var(--mag-primary);">模型</th>
                                    <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">訓練目標</th>
                                    <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">輸出</th>
                                    <th style="padding: 12px; text-align: center; border-bottom: 2px solid var(--mag-primary);">Loss Function</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td style="padding: 12px; border-bottom: 1px solid #e5e7eb;"><strong>GPT</strong></td>
                                    <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">自回歸語言模型<br/>預測「下一個 token」</td>
                                    <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">詞彙表的機率分布<br/>（例如 50,000 個詞彙）</td>
                                    <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">Cross-Entropy<br/>（語言模型）</td>
                                </tr>
                                <tr>
                                    <td style="padding: 12px; border-bottom: 1px solid #e5e7eb;"><strong>BERT</strong></td>
                                    <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">遮蔽語言模型<br/>預測「被遮住的 token」</td>
                                    <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">被遮住位置的詞彙機率分布</td>
                                    <td style="padding: 12px; text-align: center; border-bottom: 1px solid #e5e7eb;">Cross-Entropy<br/>（遮蔽預測）</td>
                                </tr>
                                <tr>
                                    <td style="padding: 12px;"><strong>ViT</strong></td>
                                    <td style="padding: 12px; text-align: center; color: var(--mag-secondary); font-weight: 600;">監督式分類<br/>預測「圖片類別」</td>
                                    <td style="padding: 12px; text-align: center; color: var(--mag-secondary); font-weight: 600;">K 個類別的機率分布<br/>（例如 1000 個類別）</td>
                                    <td style="padding: 12px; text-align: center; color: var(--mag-secondary); font-weight: 600;">Cross-Entropy<br/>（分類任務）</td>
                                </tr>
                            </tbody>
                        </table>

                        <p style="margin-top: 20px;">
                            <strong>關鍵差異</strong>：
                        </p>
                        <ul>
                            <li><strong>GPT</strong>：序列生成任務，每個位置都要預測下一個 token，是「自回歸」的</li>
                            <li><strong>BERT</strong>：雙向理解任務，預測被遮住的 token，是「雙向」的</li>
                            <li><strong>ViT</strong>：分類任務，只預測一個結果（圖片類別），是「單一輸出」的</li>
                        </ul>
                        <p>
                            <strong>類比</strong>：
                        </p>
                        <ul>
                            <li><strong>GPT</strong>：像是一個「寫作助手」，根據前面的文字，預測下一個字</li>
                            <li><strong>BERT</strong>：像是一個「填空助手」，根據上下文，猜測被遮住的字</li>
                            <li><strong>ViT</strong>：像是一個「圖片辨識助手」，看完整張圖片後，判斷「這是什麼」</li>
                        </ul>
                    </div>

                    <div class="key-concept">
                        <h5>🔄 預訓練-微調範式</h5>
                        <p>
                            ViT 遵循標準的「預訓練-微調」範式，類似 BERT：
                        </p>
                        
                        <div style="background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0;">
                            <h6 style="margin-top: 0; color: var(--mag-primary);">階段 1：預訓練（Pre-training）</h6>
                            <ul>
                                <li><strong>資料集</strong>：大規模資料集（ImageNet-21k：1400 萬張，或 JFT-300M：3 億張）</li>
                                <li><strong>任務</strong>：監督式分類（每張圖片都有標籤）</li>
                                <li><strong>Classification Head</strong>：MLP（一層隱藏層）</li>
                                <li><strong>目標</strong>：學習通用的視覺表徵</li>
                            </ul>
                        </div>

                        <div style="background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0;">
                            <h6 style="margin-top: 0; color: var(--mag-secondary);">階段 2：微調（Fine-tuning）</h6>
                            <ul>
                                <li><strong>資料集</strong>：下游任務資料集（例如 ImageNet：1000 類，或 CIFAR-100：100 類）</li>
                                <li><strong>任務</strong>：同樣是監督式分類，但類別數可能不同</li>
                                <li><strong>Classification Head</strong>：替換成零初始化的線性層（D×K，K 是下游任務的類別數）</li>
                                <li><strong>目標</strong>：適應特定任務，同時保留預訓練的通用表徵</li>
                            </ul>
                        </div>

                        <p>
                            <strong>為什麼要預訓練？</strong> 在大規模資料上預訓練可以讓模型學習到通用的視覺特徵（邊緣、紋理、形狀、物體等），這些特徵對各種視覺任務都有用。然後在下游任務上微調，只需要學習任務特定的特徵即可。
                        </p>
                    </div>
                </div>
            </div>

            <!-- 6. Fine-tuning 與高解析度 -->
            <h2>🔍 Fine-tuning 與高解析度</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>為什麼要用更高解析度？</h4>
                    <p>
                        在 Fine-tuning 時，通常使用比預訓練時更高的解析度（例如預訓練用 224×224，Fine-tuning 用 384×384 或 512×512）。
                        這樣可以：
                    </p>
                    <ul>
                        <li>捕捉更細緻的細節</li>
                        <li>提升分類準確率</li>
                    </ul>
                    
                    <div class="key-concept">
                        <h5>📍 位置編碼的插值</h5>
                        <p>
                            當解析度改變時，patches 的數量也會改變（例如 224×224 → 384×384，patches 從 196 個變成 576 個）。
                            但預訓練的位置編碼只有 197 個（1 個 CLS + 196 個 patches）。
                        </p>
                        <p>
                            <strong>解決方案</strong>：使用 <strong>2D 插值</strong>，根據 patches 在原始圖片中的 2D 位置，對位置編碼進行插值。
                        </p>
                        <p>
                            <strong>注意</strong>：這是 ViT 中<strong>唯一手動注入 2D 結構資訊</strong>的地方（除了切分 patches）。
                        </p>
                    </div>
                </div>
            </div>

            <!-- 6. Hybrid Architecture -->
            <h2>🔀 Hybrid Architecture（混合架構）</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>CNN + Transformer</h4>
                    <p>
                        除了直接使用圖片 patches，ViT 也支援「混合架構」：
                        先用 CNN 提取特徵圖（feature maps），然後把特徵圖當成「patches」送入 Transformer。
                    </p>
                    
                    <div class="key-concept">
                        <h5>💡 為什麼需要混合架構？</h5>
                        <ul>
                            <li><strong>小資料集</strong>：當資料不足時，CNN 的歸納偏置可以幫助模型更快收斂</li>
                            <li><strong>遷移學習</strong>：可以利用預訓練好的 CNN（如 ResNet）作為特徵提取器</li>
                        </ul>
                        <p>
                            <strong>實驗結果</strong>：在 ImageNet 上，混合架構的表現略好於純 ViT，但差距不大。
                            當資料規模擴大時，純 ViT 的優勢會更明顯。
                        </p>
                    </div>
                </div>
            </div>

            <!-- Navigation -->
            <div class="chapter-navigation">
                <a href="02-patch-embedding.html" class="nav-link">← 上一章：Patch Embedding</a>
                <a href="index.html" class="nav-link">返回目錄</a>
                <a href="04-experiments.html" class="nav-link">下一章：實驗結果 →</a>
            </div>
        </div>
    </div>
</body>
</html>
