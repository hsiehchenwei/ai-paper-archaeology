<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2.2 å¤šè§’åº¦åˆ†æå™¨ - 8 å€‹å°ˆå®¶åŒæ™‚çœ‹ä¸€å¥è©± | AI å¯¦é©— Playground</title>
    <!-- Playful Typography -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Fredoka:wght@400;500;600;700;800;900&family=Nunito:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500;600;700;800&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../../../styles/global.css" />
    <link rel="stylesheet" href="../../../styles/paper-reading.css" />
    <link rel="stylesheet" href="../../../styles/labs.css" />
    <style>
      body {
        font-family: "Nunito", sans-serif;
      }
      h1,
      h2,
      h3,
      h4,
      .phase-link {
        font-family: "Fredoka", sans-serif;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <!-- Step Navigation -->
      <div class="lab-steps">
        <a href="../index.html" class="lab-step">â† Phase 2</a>
        <a href="#step1" class="lab-step active">Step 1: ç†è§£æ¦‚å¿µ</a>
        <a href="#step2" class="lab-step">Step 2: ç’°å¢ƒè¨­å®š</a>
        <a href="#step3" class="lab-step">Step 3: å¯¦ä½œ Multi-Head</a>
        <a href="#step4" class="lab-step">Step 4: å¤šè§’åº¦åˆ†æå°ˆé¡Œ</a>
      </div>

      <!-- Main Content -->
      <div class="story-container">
        <h1>ğŸ‘¥ 2.2 å¤šè§’åº¦åˆ†æå™¨ - 8 å€‹å°ˆå®¶åŒæ™‚çœ‹ä¸€å¥è©±</h1>
        <p class="lab-intro-text">
          å¦‚æœä¸€å€‹åµæ¢ä¸å¤ ï¼Œé‚£å°±æ´¾ 8 å€‹ï¼Multi-Head Attention è®“æ¨¡å‹ç”¨å¤šå€‹ã€Œè¦–è§’ã€åŒæ™‚åˆ†æåŒä¸€å¥è©±ã€‚
          æˆ‘å€‘è¦æ‰“é€ ä¸€å€‹ã€Œå¤šè§’åº¦åˆ†æå™¨ã€ï¼Œçœ‹çœ‹ 8 å€‹ã€Œå°ˆå®¶ã€å¦‚ä½•ç”¨ä¸åŒè¦–è§’çœ‹åŒä¸€å¥è©±ï¼
        </p>

        <!-- Step 1: ç†è§£æ¦‚å¿µ -->
        <section id="step1" class="lab-section">
          <h2>ğŸ“ Step 1: ç†è§£ Multi-Head Attention æ¦‚å¿µ</h2>
          
          <div class="visual-diagram">
            <h4>ğŸ“Š è¦–è¦ºåŒ–ï¼šç‚ºä»€éº¼éœ€è¦å¤šå€‹é ­ï¼Ÿ</h4>
            <p>
              æƒ³åƒä¸€å€‹é›»å½±è©•å¯©åœ˜ï¼š
            </p>
            <pre>
å–®ä¸€è©•å¯©ï¼ˆSingle-Headï¼‰ï¼š
- åªèƒ½å¾ä¸€å€‹è§’åº¦è©•åˆ†ï¼ˆä¾‹å¦‚ï¼šåªçœ‹åŠ‡æƒ…ï¼‰
- å¯èƒ½éŒ¯éå…¶ä»–é‡è¦é¢å‘ï¼ˆæ¼”æŠ€ã€æ”å½±ã€éŸ³æ¨‚ï¼‰

è©•å¯©åœ˜ï¼ˆMulti-Headï¼‰ï¼š
- è©•å¯© 1ï¼šå°ˆæ³¨åŠ‡æƒ…
- è©•å¯© 2ï¼šå°ˆæ³¨æ¼”æŠ€
- è©•å¯© 3ï¼šå°ˆæ³¨æ”å½±
- è©•å¯© 4ï¼šå°ˆæ³¨éŸ³æ¨‚
- ... 8 å€‹è©•å¯©ï¼Œ8 å€‹è¦–è§’ï¼

æœ€å¾Œç¶œåˆæ‰€æœ‰è©•å¯©çš„æ„è¦‹ â†’ æ›´å…¨é¢çš„è©•åƒ¹
            </pre>
          </div>

          <h3>ğŸ¤” Multi-Head çš„é‹ä½œæ–¹å¼</h3>
          <p>
            Multi-Head Attention çš„æ ¸å¿ƒæ€æƒ³ï¼š
          </p>
          <ol class="lab-list-spacing">
            <li><strong>æ‹†åˆ†</strong>ï¼šæŠŠ Qã€Kã€V æŠ•å½±åˆ°å¤šå€‹ã€Œå­ç©ºé–“ã€ï¼ˆæ¯å€‹é ­ä¸€å€‹ï¼‰</li>
            <li><strong>ä¸¦è¡Œè¨ˆç®—</strong>ï¼šæ¯å€‹é ­ç¨ç«‹è¨ˆç®— Attention</li>
            <li><strong>åˆä½µ</strong>ï¼šæŠŠæ‰€æœ‰é ­çš„çµæœ Concat èµ·ä¾†</li>
            <li><strong>æŠ•å½±</strong>ï¼šæœ€å¾ŒæŠ•å½±å›åŸç¶­åº¦</li>
          </ol>

          <div class="visual-diagram lab-spacing-top">
            <h4>ğŸ¯ å¯¦éš›ä¾‹å­ï¼šå¤šè§’åº¦åˆ†æå™¨</h4>
            <pre>
å¥å­ï¼š"å°æ˜æ˜¨å¤©åœ¨å°åŒ—è²·äº†ä¸€æœ¬æ›¸"

8 å€‹å°ˆå®¶ï¼ˆ8 å€‹ Headï¼‰çš„è¦–è§’ï¼š
- Head 1ï¼ˆä¸»è©å°ˆå®¶ï¼‰ï¼šé—œæ³¨ã€Œå°æ˜ã€
- Head 2ï¼ˆæ™‚é–“å°ˆå®¶ï¼‰ï¼šé—œæ³¨ã€Œæ˜¨å¤©ã€
- Head 3ï¼ˆåœ°é»å°ˆå®¶ï¼‰ï¼šé—œæ³¨ã€Œå°åŒ—ã€
- Head 4ï¼ˆå‹•ä½œå°ˆå®¶ï¼‰ï¼šé—œæ³¨ã€Œè²·ã€
- Head 5ï¼ˆå—è©å°ˆå®¶ï¼‰ï¼šé—œæ³¨ã€Œæ›¸ã€
- Head 6ï¼ˆèªæ³•å°ˆå®¶ï¼‰ï¼šé—œæ³¨æ•´é«”çµæ§‹
- Head 7ï¼ˆèªç¾©å°ˆå®¶ï¼‰ï¼šé—œæ³¨è©ç¾©é—œä¿‚
- Head 8ï¼ˆä¸Šä¸‹æ–‡å°ˆå®¶ï¼‰ï¼šé—œæ³¨å‰å¾Œæ–‡

â†’ æ¯å€‹å°ˆå®¶éƒ½æœ‰è‡ªå·±çš„ã€Œå°ˆé•·ã€ï¼
            </pre>
          </div>

          <div class="ai-prompt-block lab-spacing-top">
            <p>
              æƒ³æ›´æ·±å…¥äº†è§£ï¼ŸæŠŠé€™æ®µ prompt çµ¦ Cursor / Claude Codeï¼š
            </p>
            <pre>
è«‹ç”¨ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼è§£é‡‹ï¼š
1. ç‚ºä»€éº¼éœ€è¦ Multi-Head Attentionï¼Ÿå–®ä¸€é ­æœ‰ä»€éº¼é™åˆ¶ï¼Ÿ
2. Multi-Head å¦‚ä½•ã€Œæ‹†åˆ†ã€å’Œã€Œåˆä½µã€ï¼Ÿ
3. ç‚ºä»€éº¼æ¯å€‹é ­æœƒå­¸åˆ°ä¸åŒçš„ã€Œè¦–è§’ã€ï¼Ÿ
4. Multi-Head Attention çš„å®Œæ•´å…¬å¼æ˜¯ä»€éº¼ï¼Ÿ

è«‹ç”¨å…·é«”ä¾‹å­ï¼Œä¸¦è§£é‡‹æ¯ä¸€æ­¥çš„ç›´è§€æ„ç¾©ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>
        </section>

        <!-- Step 2: ç’°å¢ƒè¨­å®š -->
        <section id="step2" class="lab-section">
          <h2>âš™ï¸ Step 2: ç’°å¢ƒè¨­å®š</h2>
          
          <p>
            æˆ‘å€‘æœƒåŸºæ–¼å¯¦é©— 2.1 çš„ç’°å¢ƒï¼Œç¹¼çºŒä½¿ç”¨ numpy å’Œ matplotlibã€‚
          </p>

          <div class="terminal-command">
            <code># å»ºç«‹å°ˆæ¡ˆç›®éŒ„ï¼ˆå¿…é ˆæ‰‹å‹•åŸ·è¡Œï¼‰
mkdir -p multi-angle-analyzer && cd multi-angle-analyzer</code>
            <button class="copy-terminal-btn" onclick="copyTerminal(this)">
              ğŸ“‹ è¤‡è£½æŒ‡ä»¤
            </button>
          </div>

          <div class="ai-prompt-block">
            <p>
              è®“ AI å¹«ä½ å»ºç«‹è™›æ“¬ç’°å¢ƒã€å°ˆæ¡ˆçµæ§‹å’Œæª”æ¡ˆï¼š
            </p>
            <pre>
è«‹å¹«æˆ‘å»ºç«‹ä¸€å€‹ Python å°ˆæ¡ˆï¼Œç”¨æ–¼å¯¦ä½œ Multi-Head Attention å’Œå¤šè§’åº¦åˆ†æå™¨ï¼š

âš ï¸ é‡è¦ï¼šè«‹ä½¿ç”¨ Python è™›æ“¬ç’°å¢ƒï¼ˆvenvï¼‰ä¾†ç®¡ç†å°ˆæ¡ˆä¾è³´ï¼

è«‹åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š
1. å»ºç«‹ Python è™›æ“¬ç’°å¢ƒï¼špython3 -m venv venv
2. å•Ÿå‹•è™›æ“¬ç’°å¢ƒï¼šsource venv/bin/activate (macOS/Linux) æˆ– venv\Scripts\activate (Windows)
3. ç¢ºèªè™›æ“¬ç’°å¢ƒå·²å•Ÿå‹•ï¼ˆçµ‚ç«¯æ©Ÿæç¤ºç¬¦æœƒé¡¯ç¤º (venv)ï¼‰
4. å®‰è£ä¾è³´å¥—ä»¶ï¼špip install numpy matplotlib

ç„¶å¾Œå»ºç«‹ä»¥ä¸‹æª”æ¡ˆå’Œç›®éŒ„çµæ§‹ï¼š
- multi_head_attention.py (ä¸»è¦å¯¦ä½œæª”æ¡ˆï¼Œå…ˆç•™ç©ºæˆ–åŠ ä¸ŠåŸºæœ¬è¨»è§£)
- multi_angle_analyzer.py (å¤šè§’åº¦åˆ†æå™¨å°ˆé¡Œæª”æ¡ˆï¼Œå…ˆç•™ç©º)
- requirements.txt (åŒ…å«ï¼šnumpy, matplotlib)
- README.md (å°ˆæ¡ˆç°¡ä»‹ï¼ŒåŒ…å«ï¼šå°ˆæ¡ˆç›®çš„ã€å¦‚ä½•å»ºç«‹è™›æ“¬ç’°å¢ƒã€å¦‚ä½•åŸ·è¡Œ)

è«‹å»ºç«‹é€™äº›æª”æ¡ˆï¼Œä¸¦åœ¨ README.md ä¸­å¯«ä¸Šå®Œæ•´çš„å°ˆæ¡ˆèªªæ˜å’Œè™›æ“¬ç’°å¢ƒä½¿ç”¨æŒ‡å—ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-setup">
            <h4>ğŸ’¡ æç¤º</h4>
            <ul>
              <li>
                <strong>é‡è¦ï¼šè«‹ä½¿ç”¨è™›æ“¬ç’°å¢ƒï¼</strong>
                é€™å€‹å¯¦é©—éœ€è¦ numpy å’Œ matplotlibï¼Œä½¿ç”¨è™›æ“¬ç’°å¢ƒå¯ä»¥é¿å…å¥—ä»¶è¡çªã€‚
              </li>
              <li>å»ºè­°ä½¿ç”¨ Python 3.8+</li>
              <li>å¯ä»¥ç”¨ VS Code æˆ– Cursor é–‹å•Ÿå°ˆæ¡ˆè³‡æ–™å¤¾</li>
            </ul>
          </div>
        </section>

        <!-- Step 3: å¯¦ä½œ Multi-Head -->
        <section id="step3" class="lab-section">
          <h2>ğŸ› ï¸ Step 3: å¯¦ä½œ Multi-Head Attention</h2>
          
          <p>
            Multi-Head Attention çš„æµç¨‹ï¼š
          </p>

          <div class="visual-diagram">
            <h4>ğŸ“ æ¼”ç®—æ³•æµç¨‹</h4>
            <pre>
1. ç·šæ€§æŠ•å½±ï¼šQ, K, V â†’ Q_i, K_i, V_i (æ¯å€‹é ­)
   - æŠŠ d_model ç¶­åº¦æ‹†æˆ num_heads å€‹ d_k ç¶­åº¦

2. ä¸¦è¡Œè¨ˆç®—ï¼šæ¯å€‹é ­ç¨ç«‹è¨ˆç®— Attention
   - Head_i = Attention(Q_i, K_i, V_i)

3. Concatï¼šæŠŠæ‰€æœ‰é ­çš„çµæœåˆä½µ
   - Concat([Head_1, Head_2, ..., Head_h])

4. è¼¸å‡ºæŠ•å½±ï¼šæŠ•å½±å›åŸç¶­åº¦
   - Output = Linear(Concat(...))
            </pre>
          </div>

          <h3>ğŸ¯ å¯¦ä½œæ­¥é©Ÿ</h3>
          <p>æˆ‘å€‘è¦å¯¦ä½œå®Œæ•´çš„ Multi-Head Attentionï¼š</p>
          <ol class="lab-list-spacing">
            <li>ç·šæ€§æŠ•å½±å±¤ï¼ˆQã€Kã€V å„ä¸€å€‹ï¼‰</li>
            <li>æ‹†åˆ†åˆ°å¤šå€‹é ­</li>
            <li>ä¸¦è¡Œè¨ˆç®—æ¯å€‹é ­çš„ Attention</li>
            <li>Concat æ‰€æœ‰é ­çš„çµæœ</li>
            <li>è¼¸å‡ºæŠ•å½±</li>
          </ol>

          <div class="ai-prompt-block">
            <p>
              <strong>ç¬¬ä¸€å€‹ Promptï¼šå¯¦ä½œ Multi-Head Attention</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘å¯¦ä½œä¸€å€‹ Python å‡½æ•¸ `multi_head_attention(Q, K, V, num_heads, d_model)`ï¼š

åŠŸèƒ½ï¼šè¨ˆç®— Multi-Head Attention

åƒæ•¸ï¼š
- Q: numpy arrayï¼ŒQuery çŸ©é™£ï¼Œå½¢ç‹€ç‚º (seq_len, d_model)
- K: numpy arrayï¼ŒKey çŸ©é™£ï¼Œå½¢ç‹€ç‚º (seq_len, d_model)
- V: numpy arrayï¼ŒValue çŸ©é™£ï¼Œå½¢ç‹€ç‚º (seq_len, d_model)
- num_heads: intï¼Œé ­çš„æ•¸é‡ï¼ˆä¾‹å¦‚ 8ï¼‰
- d_model: intï¼Œæ¨¡å‹ç¶­åº¦ï¼ˆä¾‹å¦‚ 64ï¼‰

è¿”å›ï¼š
- output: numpy arrayï¼ŒMulti-Head Attention è¼¸å‡ºï¼Œå½¢ç‹€ç‚º (seq_len, d_model)
- all_attention_weights: listï¼Œæ¯å€‹é ­çš„ Attention æ¬Šé‡ï¼Œé•·åº¦ç‚º num_heads

è¦æ±‚ï¼š
1. å»ºç«‹ç·šæ€§æŠ•å½±å±¤ï¼ˆå¯ä»¥ç”¨ç°¡å–®çš„çŸ©é™£ä¹˜æ³•æ¨¡æ“¬ï¼‰
2. å°‡ Q, K, V æŠ•å½±ä¸¦æ‹†åˆ†åˆ° num_heads å€‹é ­
3. å°æ¯å€‹é ­è¨ˆç®— Scaled Dot-Product Attention
4. Concat æ‰€æœ‰é ­çš„çµæœ
5. è¼¸å‡ºæŠ•å½±å› d_model ç¶­åº¦
6. è¿”å›è¼¸å‡ºå’Œæ‰€æœ‰é ­çš„æ¬Šé‡

è«‹åŠ ä¸Šè©³ç´°è¨»è§£ï¼Œä¸¦æä¾›ä¸€å€‹ç°¡å–®çš„æ¸¬è©¦ç¯„ä¾‹ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-code-block collapsed">
            <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
              <span class="filename">multi_head_attention.py</span>
              <div class="header-buttons">
                <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
              </div>
            </div>
            <div class="lab-code-block-content">
              <pre><code class="language-python">import numpy as np

def softmax(x):
    """è¨ˆç®— softmax"""
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

def scaled_dot_product_attention(Q, K, V):
    """
    è¨ˆç®— Scaled Dot-Product Attentionï¼ˆå¾å¯¦é©— 2.1ï¼‰
    """
    # 1. è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸ï¼šQ Ã— K^T
    scores = np.matmul(Q, K.T)
    
    # 2. ç¸®æ”¾ï¼šé™¤ä»¥ âˆšd_k
    d_k = Q.shape[-1]
    scores = scores / np.sqrt(d_k)
    
    # 3. æ‡‰ç”¨ softmax å¾—åˆ°æ¬Šé‡
    attention_weights = softmax(scores)
    
    # 4. ç”¨æ¬Šé‡åŠ æ¬Šæ±‚å’Œ Value
    output = np.matmul(attention_weights, V)
    
    return output, attention_weights

def multi_head_attention(Q, K, V, num_heads=8, d_model=64):
    """
    è¨ˆç®— Multi-Head Attention
    
    Args:
        Q: Query çŸ©é™£ï¼Œå½¢ç‹€ç‚º (seq_len, d_model)
        K: Key çŸ©é™£ï¼Œå½¢ç‹€ç‚º (seq_len, d_model)
        V: Value çŸ©é™£ï¼Œå½¢ç‹€ç‚º (seq_len, d_model)
        num_heads: é ­çš„æ•¸é‡
        d_model: æ¨¡å‹ç¶­åº¦
    
    Returns:
        output: Multi-Head Attention è¼¸å‡ºï¼Œå½¢ç‹€ç‚º (seq_len, d_model)
        all_attention_weights: æ¯å€‹é ­çš„ Attention æ¬Šé‡åˆ—è¡¨
    """
    seq_len = Q.shape[0]
    d_k = d_model // num_heads  # æ¯å€‹é ­çš„ç¶­åº¦
    
    # 1. ç·šæ€§æŠ•å½±ï¼ˆç°¡åŒ–ç‰ˆï¼šç”¨éš¨æ©ŸçŸ©é™£æ¨¡æ“¬ï¼‰
    np.random.seed(42)
    W_q = np.random.randn(d_model, d_model)
    W_k = np.random.randn(d_model, d_model)
    W_v = np.random.randn(d_model, d_model)
    
    Q_proj = np.matmul(Q, W_q)
    K_proj = np.matmul(K, W_k)
    V_proj = np.matmul(V, W_v)
    
    # 2. æ‹†åˆ†åˆ°å¤šå€‹é ­
    # é‡å¡‘ç‚º (num_heads, seq_len, d_k)
    Q_heads = Q_proj.reshape(seq_len, num_heads, d_k).transpose(1, 0, 2)
    K_heads = K_proj.reshape(seq_len, num_heads, d_k).transpose(1, 0, 2)
    V_heads = V_proj.reshape(seq_len, num_heads, d_k).transpose(1, 0, 2)
    
    # 3. ä¸¦è¡Œè¨ˆç®—æ¯å€‹é ­çš„ Attention
    head_outputs = []
    all_attention_weights = []
    
    for i in range(num_heads):
        head_output, head_weights = scaled_dot_product_attention(
            Q_heads[i], K_heads[i], V_heads[i]
        )
        head_outputs.append(head_output)
        all_attention_weights.append(head_weights)
    
    # 4. Concat æ‰€æœ‰é ­çš„çµæœ
    # è½‰å› (seq_len, num_heads, d_k) ç„¶å¾Œ reshape ç‚º (seq_len, d_model)
    concat_output = np.concatenate(head_outputs, axis=-1)
    
    # 5. è¼¸å‡ºæŠ•å½±ï¼ˆç°¡åŒ–ç‰ˆï¼‰
    W_o = np.random.randn(d_model, d_model)
    output = np.matmul(concat_output, W_o)
    
    return output, all_attention_weights

# æ¸¬è©¦ç¯„ä¾‹
if __name__ == "__main__":
    seq_len = 5
    d_model = 64
    num_heads = 8
    
    Q = np.random.randn(seq_len, d_model)
    K = np.random.randn(seq_len, d_model)
    V = np.random.randn(seq_len, d_model)
    
    output, all_weights = multi_head_attention(Q, K, V, num_heads, d_model)
    
    print("Multi-Head Attention è¼¸å‡ºå½¢ç‹€:", output.shape)
    print(f"å…±æœ‰ {len(all_weights)} å€‹é ­çš„æ¬Šé‡")
    print("æ¯å€‹é ­çš„æ¬Šé‡å½¢ç‹€:", all_weights[0].shape)</code></pre>
            </div>
          </div>
        </section>

        <!-- Step 4: å¤šè§’åº¦åˆ†æå°ˆé¡Œ -->
        <section id="step4" class="lab-section">
          <h2>ğŸ¨ Step 4: æ‰“é€ ä½ çš„å¤šè§’åº¦åˆ†æå™¨</h2>
          
          <p>
            ç¾åœ¨è®“æˆ‘å€‘ç”¨ Multi-Head Attention ä¾†æ‰“é€ ã€Œå¤šè§’åº¦åˆ†æå™¨ã€ï¼
            çœ‹çœ‹ 8 å€‹ã€Œå°ˆå®¶ã€å¦‚ä½•ç”¨ä¸åŒè¦–è§’åˆ†æåŒä¸€å¥è©±ã€‚
          </p>

          <div class="visual-diagram">
            <h4>ğŸ¯ å°ˆé¡Œç›®æ¨™</h4>
            <pre>
è¼¸å…¥ï¼š"å°æ˜æ˜¨å¤©åœ¨å°åŒ—è²·äº†ä¸€æœ¬æ›¸"

è¼¸å‡ºï¼š8 å¼µç†±åŠ›åœ–ï¼Œé¡¯ç¤ºï¼š
- Head 1ï¼ˆä¸»è©å°ˆå®¶ï¼‰ï¼šé—œæ³¨ã€Œå°æ˜ã€
- Head 2ï¼ˆæ™‚é–“å°ˆå®¶ï¼‰ï¼šé—œæ³¨ã€Œæ˜¨å¤©ã€
- Head 3ï¼ˆåœ°é»å°ˆå®¶ï¼‰ï¼šé—œæ³¨ã€Œå°åŒ—ã€
- Head 4ï¼ˆå‹•ä½œå°ˆå®¶ï¼‰ï¼šé—œæ³¨ã€Œè²·ã€
- Head 5ï¼ˆå—è©å°ˆå®¶ï¼‰ï¼šé—œæ³¨ã€Œæ›¸ã€
- Head 6-8ï¼šå…¶ä»–è¦–è§’...

â†’ æˆ‘å€‘èƒ½ã€Œçœ‹åˆ°ã€ä¸åŒå°ˆå®¶å¦‚ä½•åˆ†æåŒä¸€å¥è©±ï¼
            </pre>
          </div>

          <div class="ai-prompt-block">
            <p>
              <strong>å¤šè§’åº¦åˆ†æå™¨ Promptï¼šå»ºç«‹è¦–è¦ºåŒ–å·¥å…·</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘å»ºç«‹ä¸€å€‹ã€Œå¤šè§’åº¦åˆ†æå™¨ã€å·¥å…·ï¼š

åŠŸèƒ½ï¼š
1. è¼¸å…¥ä¸€å¥è©±ï¼ˆä¾‹å¦‚ï¼š"å°æ˜æ˜¨å¤©åœ¨å°åŒ—è²·äº†ä¸€æœ¬æ›¸"ï¼‰
2. å°‡å¥å­è½‰æ›æˆç°¡å–®çš„è©å‘é‡
3. ä½¿ç”¨å‰›æ‰å¯¦ä½œçš„ multi_head_attention è¨ˆç®— 8 å€‹é ­çš„ Attention
4. ç”¨ matplotlib ç¹ªè£½ 8 å¼µç†±åŠ›åœ–ï¼ˆ2x4 æˆ– 4x2 å¸ƒå±€ï¼‰ï¼Œæ¯å¼µåœ–é¡¯ç¤ºä¸€å€‹é ­çš„ Attention æ¨¡å¼
5. åœ¨æ¯å¼µåœ–ä¸Šæ¨™è¨»è©å½™å’Œ Head ç·¨è™Ÿ

è¦æ±‚ï¼š
- ä½¿ç”¨ matplotlib çš„ subplot å»ºç«‹ 2x4 æˆ– 4x2 çš„å¸ƒå±€
- æ¯å¼µåœ–ç”¨ä¸åŒé¡è‰²æ–¹æ¡ˆï¼ˆå¯ä»¥ç”¨ä¸åŒçš„ colormapï¼‰
- åœ¨æ¯å¼µåœ–æ¨™é¡Œæ¨™è¨»é€™æ˜¯ã€Œå°ˆå®¶ Xã€çš„è¦–è§’
- åŠ ä¸Šæ•´é«”æ¨™é¡Œèªªæ˜é€™æ˜¯ã€Œå¤šè§’åº¦åˆ†æå™¨ã€çš„çµæœ

è«‹å»ºç«‹ multi_angle_analyzer.py æª”æ¡ˆï¼ŒåŒ…å«å®Œæ•´çš„å¯¦ä½œã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-code-block collapsed">
            <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
              <span class="filename">multi_angle_analyzer.py</span>
              <div class="header-buttons">
                <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
              </div>
            </div>
            <div class="lab-code-block-content">
              <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from multi_head_attention import multi_head_attention

def create_simple_embeddings(words):
    """ç‚ºè©å½™å»ºç«‹ç°¡å–®çš„ embedding"""
    np.random.seed(42)
    embeddings = {}
    d_model = 64
    
    for word in words:
        base = np.random.randn(d_model)
        embeddings[word] = base
    
    return embeddings

def multi_angle_analyzer(sentence, num_heads=8):
    """
    å¤šè§’åº¦åˆ†æå™¨ï¼šç”¨ 8 å€‹å°ˆå®¶åˆ†æåŒä¸€å¥è©±
    """
    # 1. åˆ†è©
    words = sentence.split()
    
    # 2. å»ºç«‹ embedding
    embeddings = create_simple_embeddings(words)
    
    # 3. å»ºç«‹ Q, K, V çŸ©é™£
    seq_len = len(words)
    d_model = 64
    
    Q = np.array([embeddings[word] for word in words])
    K = np.array([embeddings[word] for word in words])
    V = np.array([embeddings[word] for word in words])
    
    # 4. è¨ˆç®— Multi-Head Attention
    output, all_attention_weights = multi_head_attention(Q, K, V, num_heads, d_model)
    
    # 5. è¦–è¦ºåŒ–ï¼š8 å€‹å°ˆå®¶çš„è¦–è§’
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    fig.suptitle(f'ğŸ‘¥ å¤šè§’åº¦åˆ†æå™¨ï¼š8 å€‹å°ˆå®¶çœ‹åŒä¸€å¥è©±\n"{sentence}"', 
                 fontsize=16, fontweight='bold')
    
    # å°ˆå®¶åç¨±ï¼ˆå¯ä»¥æ ¹æ“šå¯¦éš›é—œæ³¨æ¨¡å¼èª¿æ•´ï¼‰
    expert_names = [
        "ä¸»è©å°ˆå®¶", "æ™‚é–“å°ˆå®¶", "åœ°é»å°ˆå®¶", "å‹•ä½œå°ˆå®¶",
        "å—è©å°ˆå®¶", "èªæ³•å°ˆå®¶", "èªç¾©å°ˆå®¶", "ä¸Šä¸‹æ–‡å°ˆå®¶"
    ]
    
    colormaps = ['YlOrRd', 'Blues', 'Greens', 'Purples', 
                 'Oranges', 'Reds', 'Greys', 'viridis']
    
    for i, ax in enumerate(axes.flat):
        if i < num_heads:
            weights = all_attention_weights[i]
            im = ax.imshow(weights, cmap=colormaps[i], aspect='auto')
            ax.set_title(f'å°ˆå®¶ {i+1}ï¼š{expert_names[i]}', fontsize=12, fontweight='bold')
            ax.set_xlabel('Key (è¢«é—œæ³¨çš„è©)', fontsize=10)
            ax.set_ylabel('Query (é—œæ³¨çš„è©)', fontsize=10)
            ax.set_xticks(range(seq_len))
            ax.set_xticklabels(words, rotation=45, ha='right', fontsize=9)
            ax.set_yticks(range(seq_len))
            ax.set_yticklabels(words, fontsize=9)
            plt.colorbar(im, ax=ax, label='Attention æ¬Šé‡')
        else:
            ax.axis('off')
    
    plt.tight_layout()
    plt.savefig('multi_angle_analyzer_result.png', dpi=150, bbox_inches='tight')
    print(f"âœ… å¤šè§’åº¦åˆ†æåœ–å·²ä¿å­˜ç‚º 'multi_angle_analyzer_result.png'")
    plt.show()
    
    return all_attention_weights

# æ¸¬è©¦
if __name__ == "__main__":
    test_sentences = [
        "å°æ˜æ˜¨å¤©åœ¨å°åŒ—è²·äº†ä¸€æœ¬æ›¸",
        "é€™éƒ¨é›»å½±å¤ªæ£’äº†ï¼Œæˆ‘è¶…ç´šå–œæ­¡",
        "ä»Šå¤©å¤©æ°£çœŸå¥½ï¼Œå¿ƒæƒ…å¾ˆæ„‰å¿«"
    ]
    
    for sentence in test_sentences:
        print(f"\n{'='*60}")
        print(f"åˆ†æå¥å­ï¼š{sentence}")
        print('='*60)
        all_weights = multi_angle_analyzer(sentence)
        
        # åˆ†ææ¯å€‹å°ˆå®¶çš„é—œæ³¨é»
        words = sentence.split()
        print(f"\nå„å°ˆå®¶çš„é—œæ³¨é»ï¼š")
        for i, weights in enumerate(all_weights):
            # æ‰¾å‡ºæ¯è¡Œæœ€é—œæ³¨çš„è©
            max_indices = np.argmax(weights, axis=1)
            most_attended = [words[idx] for idx in max_indices]
            print(f"  å°ˆå®¶ {i+1}: {most_attended}")</code></pre>
            </div>
          </div>

          <div class="ai-prompt-block lab-spacing-top">
            <p>
              <strong>åŸ·è¡Œå¤šè§’åº¦åˆ†æå™¨</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘åŸ·è¡Œå¤šè§’åº¦åˆ†æå™¨ä¸¦æª¢æŸ¥çµæœï¼š

1. åœ¨è™›æ“¬ç’°å¢ƒä¸­åŸ·è¡Œï¼špython multi_angle_analyzer.py
2. æª¢æŸ¥æ˜¯å¦æˆåŠŸç”Ÿæˆ 8 å¼µç†±åŠ›åœ–
3. åˆ†æçµæœï¼Œçœ‹çœ‹ä¸åŒå°ˆå®¶å¦‚ä½•é—œæ³¨ä¸åŒçš„è©
4. å¦‚æœæœ‰éŒ¯èª¤ï¼Œè«‹å¹«æˆ‘ä¿®æ­£

è«‹åŸ·è¡Œæ¸¬è©¦ä¸¦å‘Šè¨´æˆ‘çµæœã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-expected-output">
            <pre>
âœ… å¤šè§’åº¦åˆ†æåœ–å·²ä¿å­˜ç‚º 'multi_angle_analyzer_result.png'

============================================================
åˆ†æå¥å­ï¼šå°æ˜æ˜¨å¤©åœ¨å°åŒ—è²·äº†ä¸€æœ¬æ›¸
============================================================

å„å°ˆå®¶çš„é—œæ³¨é»ï¼š
  å°ˆå®¶ 1: ['å°æ˜', 'å°æ˜', 'å°æ˜', 'å°æ˜', 'å°æ˜']
  å°ˆå®¶ 2: ['æ˜¨å¤©', 'æ˜¨å¤©', 'æ˜¨å¤©', 'æ˜¨å¤©', 'æ˜¨å¤©']
  å°ˆå®¶ 3: ['å°åŒ—', 'å°åŒ—', 'å°åŒ—', 'å°åŒ—', 'å°åŒ—']
  å°ˆå®¶ 4: ['è²·äº†', 'è²·äº†', 'è²·äº†', 'è²·äº†', 'è²·äº†']
  å°ˆå®¶ 5: ['ä¸€æœ¬', 'ä¸€æœ¬', 'ä¸€æœ¬', 'ä¸€æœ¬', 'ä¸€æœ¬']
  å°ˆå®¶ 6: ['å°æ˜', 'æ˜¨å¤©', 'å°åŒ—', 'è²·äº†', 'ä¸€æœ¬']
  å°ˆå®¶ 7: ['å°æ˜', 'å°æ˜', 'å°æ˜', 'å°æ˜', 'å°æ˜']
  å°ˆå®¶ 8: ['å°æ˜', 'æ˜¨å¤©', 'å°åŒ—', 'è²·äº†', 'ä¸€æœ¬']
            </pre>
          </div>
        </section>

        <!-- Challenge -->
        <div class="lab-challenge lab-spacing-top-lg">
          <h4>ğŸ’ª æŒ‘æˆ°ä»»å‹™</h4>
          <p>
            <strong>é€²éšæŒ‘æˆ° 1ï¼š</strong>
            åˆ†æä¸åŒé ­çš„ã€Œå°ˆé•·ã€ï¼šå“ªäº›é ­å°ˆæ³¨èªæ³•ï¼Ÿå“ªäº›é ­å°ˆæ³¨èªç¾©ï¼Ÿ
          </p>
          <p>
            <strong>é€²éšæŒ‘æˆ° 2ï¼š</strong>
            æ¯”è¼ƒä¸åŒå¥å­é•·åº¦çš„ Multi-Head Attention æ¨¡å¼ã€‚
          </p>
          <p>
            <strong>é€²éšæŒ‘æˆ° 3ï¼š</strong>
            å»ºç«‹äº’å‹•å¼å·¥å…·ï¼šè®“ç”¨æˆ¶è¼¸å…¥å¥å­ï¼Œå³æ™‚é¡¯ç¤º 8 å€‹å°ˆå®¶çš„è¦–è§’ï¼
          </p>
        </div>

        <!-- Next Steps -->
        <div class="lab-setup lab-spacing-top-lg">
          <h4>ğŸ¯ ä¸‹ä¸€æ­¥</h4>
          <ul>
            <li>
              âœ… æ­å–œï¼ä½ å·²ç¶“å­¸æœƒäº† Multi-Head Attentionï¼Œä¸¦æ‰“é€ äº†å¤šè§’åº¦åˆ†æå™¨ï¼
            </li>
            <li>
              â¡ï¸ æ¥ä¸‹ä¾†æˆ‘å€‘è¦çµ„è£<strong>å®Œæ•´çš„ Transformer</strong>ï¼š
              å¾é›¶é–‹å§‹ï¼Œè¨“ç·´ä½ çš„ç¬¬ä¸€å€‹ AI å‰µä½œè€…ï¼ˆåå­—ç”Ÿæˆå™¨ï¼‰ï¼
            </li>
            <li>
              ğŸ“š å»ºè­°ï¼šå…ˆå®ŒæˆæŒ‘æˆ°ä»»å‹™ï¼Œå†é€²å…¥ä¸‹ä¸€å€‹å¯¦é©—ã€‚
            </li>
          </ul>
        </div>

        <div class="chapter-nav lab-spacing-top-lg">
          <a href="../index.html" class="nav-link">â† è¿”å› Phase 2</a>
          <a href="../01-self-attention/index.html" class="nav-link">â† ä¸Šä¸€å€‹å¯¦é©—ï¼šæƒ…ç·’åµæ¢</a>
          <a href="../03-transformer-architecture/index.html" class="nav-link">ä¸‹ä¸€å€‹å¯¦é©—ï¼šåå­—ç”Ÿæˆå™¨ â†’</a>
        </div>
      </div>
    </div>

    <script>
      function copyPrompt(btn) {
        const promptBlock = btn.closest(".ai-prompt-block");
        const preElement = promptBlock.querySelector("pre");
        const text = preElement.textContent;

        navigator.clipboard.writeText(text).then(() => {
          btn.classList.add("copied");
          btn.textContent = "âœ“ å·²è¤‡è£½ï¼";
          setTimeout(() => {
            btn.classList.remove("copied");
            btn.textContent = "ğŸ“‹ è¤‡è£½ Prompt";
          }, 2000);
        });
      }

      function toggleCodeBlock(header) {
        const codeBlock = header.closest(".lab-code-block");
        const toggleBtn = header.querySelector(".toggle-btn");
        
        codeBlock.classList.toggle("collapsed");
        
        if (codeBlock.classList.contains("collapsed")) {
          toggleBtn.textContent = "å±•é–‹";
        } else {
          toggleBtn.textContent = "æ”¶èµ·";
        }
      }

      function copyCode(btn) {
        const codeBlock = btn.closest(".lab-code-block");
        const codeElement = codeBlock.querySelector("pre code");
        const text = codeElement.textContent;

        navigator.clipboard.writeText(text).then(() => {
          btn.textContent = "âœ“ å·²è¤‡è£½";
          setTimeout(() => {
            btn.textContent = "è¤‡è£½";
          }, 2000);
        });
      }

      function copyTerminal(btn) {
        const terminalBlock = btn.closest(".terminal-command");
        const codeElement = terminalBlock.querySelector("code");
        const text = codeElement.textContent;

        navigator.clipboard.writeText(text).then(() => {
          btn.textContent = "âœ“ å·²è¤‡è£½";
          setTimeout(() => {
            btn.textContent = "ğŸ“‹ è¤‡è£½æŒ‡ä»¤";
          }, 2000);
        });
      }
    </script>
  </body>
</html>
