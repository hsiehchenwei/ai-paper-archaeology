<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2.3 åå­—ç”Ÿæˆå™¨ - è¨“ç·´ä½ çš„ç¬¬ä¸€å€‹ AI å‰µä½œè€… | AI å¯¦é©— Playground</title>
    <!-- Playful Typography -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Fredoka:wght@400;500;600;700;800;900&family=Nunito:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500;600;700;800&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../../../styles/global.css" />
    <link rel="stylesheet" href="../../../styles/paper-reading.css" />
    <link rel="stylesheet" href="../../../styles/labs.css" />
    <style>
      body {
        font-family: "Nunito", sans-serif;
      }
      h1,
      h2,
      h3,
      h4,
      .phase-link {
        font-family: "Fredoka", sans-serif;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <!-- Step Navigation -->
      <div class="lab-steps">
        <a href="../index.html" class="lab-step">â† Phase 2</a>
        <a href="#step1" class="lab-step active">Step 1: ç†è§£æ¶æ§‹</a>
        <a href="#step2" class="lab-step">Step 2: ç’°å¢ƒè¨­å®š</a>
        <a href="#step3" class="lab-step">Step 3: æº–å‚™è³‡æ–™</a>
        <a href="#step4" class="lab-step">Step 4: å¯¦ä½œçµ„ä»¶</a>
        <a href="#step5" class="lab-step">Step 5: çµ„è£ Transformer</a>
        <a href="#step6" class="lab-step">Step 6: è¨“ç·´èˆ‡ç”Ÿæˆ</a>
      </div>

      <!-- Main Content -->
      <div class="story-container">
        <h1>ğŸ¨ 2.3 åå­—ç”Ÿæˆå™¨ - è¨“ç·´ä½ çš„ç¬¬ä¸€å€‹ AI å‰µä½œè€…</h1>
        <p class="lab-intro-text">
          å¾é›¶é–‹å§‹ï¼Œçµ„è£å®Œæ•´çš„ Transformerï¼Œè¨“ç·´ä¸€å€‹èƒ½å‰µé€ å¾æœªå­˜åœ¨çš„åå­—çš„ AIï¼
          é€™æ˜¯ä½ ç¬¬ä¸€æ¬¡ã€Œè¨“ç·´ã€ä¸€å€‹çœŸæ­£çš„èªè¨€æ¨¡å‹ï¼Œçœ‹è‘—å®ƒå¾äº‚ç¢¼åˆ°èƒ½ç”Ÿæˆæœ‰æ„ç¾©çš„åå­—ï¼Œé‚£ç¨®æˆå°±æ„Ÿç„¡èˆ‡å€«æ¯”ï¼ğŸš€
        </p>

        <!-- Step 1: ç†è§£æ¶æ§‹ -->
        <section id="step1" class="lab-section">
          <h2>ğŸ“ Step 1: ç†è§£ Transformer æ¶æ§‹</h2>
          
          <div class="visual-diagram">
            <h4>ğŸ“Š è¦–è¦ºåŒ–ï¼šTransformer å…¨æ™¯åœ–</h4>
            <p>
              Transformer å°±åƒä¸€å€‹ã€Œæ–‡å­—è™•ç†å·¥å» ã€ï¼š
            </p>
            <pre>
è¼¸å…¥ï¼š"Emma" â†’ Tokenization â†’ [E, m, m, a]

Transformer Block (é‡è¤‡ N æ¬¡):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Multi-Head Attentionâ”‚  â† è®“æ¯å€‹å­—ã€Œçœ‹ã€å…¶ä»–å­—
  â”‚   + Residual        â”‚
  â”‚   + LayerNorm       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Feed-Forward Network â”‚  â† è™•ç†è³‡è¨Š
  â”‚   + Residual        â”‚
  â”‚   + LayerNorm       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¼¸å‡ºï¼šä¸‹ä¸€å€‹å­—çš„æ©Ÿç‡åˆ†å¸ƒ â†’ ç”Ÿæˆ "Liam" çš„ç¬¬ä¸€å€‹å­—
            </pre>
          </div>

          <h3>ğŸ¤” ç‚ºä»€éº¼èƒ½ã€Œç”Ÿæˆã€æ–‡å­—ï¼Ÿ</h3>
          <p>
            Transformer æ˜¯ä¸€å€‹<strong>è‡ªå›æ­¸æ¨¡å‹</strong>ï¼š
          </p>
          <ol class="lab-list-spacing">
            <li><strong>è¼¸å…¥</strong>ï¼šå·²æœ‰çš„æ–‡å­—ï¼ˆä¾‹å¦‚ï¼š"Em"ï¼‰</li>
            <li><strong>è™•ç†</strong>ï¼šTransformer åˆ†æé€™å€‹åºåˆ—</li>
            <li><strong>è¼¸å‡º</strong>ï¼šä¸‹ä¸€å€‹å­—çš„æ©Ÿç‡åˆ†å¸ƒï¼ˆä¾‹å¦‚ï¼šm çš„æ©Ÿç‡ 0.3ï¼Œa çš„æ©Ÿç‡ 0.5...ï¼‰</li>
            <li><strong>ç”Ÿæˆ</strong>ï¼šæ ¹æ“šæ©Ÿç‡é¸æ“‡ä¸‹ä¸€å€‹å­—ï¼ˆä¾‹å¦‚ï¼šé¸ "m"ï¼‰</li>
            <li><strong>é‡è¤‡</strong>ï¼šæŠŠæ–°å­—åŠ å…¥åºåˆ—ï¼Œç¹¼çºŒç”Ÿæˆ</li>
          </ol>

          <div class="visual-diagram lab-spacing-top">
            <h4>ğŸ¯ åå­—ç”Ÿæˆå™¨çš„é‹ä½œæ–¹å¼</h4>
            <pre>
è¨“ç·´éšæ®µï¼š
  è¼¸å…¥ï¼š"Emma" â†’ æ¨¡å‹å­¸ç¿’ï¼šE å¾Œé¢é€šå¸¸æ˜¯ mï¼Œm å¾Œé¢é€šå¸¸æ˜¯ mï¼Œm å¾Œé¢é€šå¸¸æ˜¯ a

ç”Ÿæˆéšæ®µï¼š
  è¼¸å…¥ï¼š""ï¼ˆç©ºå­—ä¸²ï¼‰
  Step 1: æ¨¡å‹é æ¸¬ç¬¬ä¸€å€‹å­— â†’ "E"
  Step 2: æ¨¡å‹çœ‹åˆ° "E"ï¼Œé æ¸¬ç¬¬äºŒå€‹å­— â†’ "m"
  Step 3: æ¨¡å‹çœ‹åˆ° "Em"ï¼Œé æ¸¬ç¬¬ä¸‰å€‹å­— â†’ "m"
  Step 4: æ¨¡å‹çœ‹åˆ° "Emm"ï¼Œé æ¸¬ç¬¬å››å€‹å­— â†’ "a"
  Step 5: é‡åˆ°çµæŸç¬¦è™Ÿ â†’ åœæ­¢

çµæœï¼š"Emma"ï¼ˆæˆ–å¯èƒ½æ˜¯ "Emlia"ã€"Emmar" ç­‰æ–°åå­—ï¼ï¼‰
            </pre>
          </div>

          <div class="ai-prompt-block lab-spacing-top">
            <p>
              æƒ³æ›´æ·±å…¥äº†è§£ï¼ŸæŠŠé€™æ®µ prompt çµ¦ Cursor / Claude Codeï¼š
            </p>
            <pre>
è«‹ç”¨ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼è§£é‡‹ï¼š
1. Transformer çš„å®Œæ•´æ¶æ§‹åŒ…å«å“ªäº›çµ„ä»¶ï¼Ÿ
2. ç‚ºä»€éº¼éœ€è¦ Residual Connection å’Œ LayerNormï¼Ÿ
3. ä»€éº¼æ˜¯è‡ªå›æ­¸ç”Ÿæˆï¼Ÿå¦‚ä½•ä¸€æ­¥æ­¥ç”Ÿæˆæ–‡å­—ï¼Ÿ
4. Character-Level Language Model å’Œ Word-Level æœ‰ä»€éº¼ä¸åŒï¼Ÿ

è«‹ç”¨å…·é«”ä¾‹å­ï¼Œä¸¦è§£é‡‹æ¯ä¸€æ­¥çš„ç›´è§€æ„ç¾©ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>
        </section>

        <!-- Step 2: ç’°å¢ƒè¨­å®š -->
        <section id="step2" class="lab-section">
          <h2>âš™ï¸ Step 2: ç’°å¢ƒè¨­å®š</h2>
          
          <p>
            æˆ‘å€‘éœ€è¦ PyTorch ä¾†å»ºç«‹å’Œè¨“ç·´ Transformerã€‚åˆ¥æ“”å¿ƒï¼Œå®Œå…¨å¯ä»¥åœ¨ Mac æœ¬æ©Ÿé‹è¡Œï¼
          </p>

          <div class="terminal-command">
            <code># å»ºç«‹å°ˆæ¡ˆç›®éŒ„ï¼ˆå¿…é ˆæ‰‹å‹•åŸ·è¡Œï¼‰
mkdir -p name-generator && cd name-generator</code>
            <button class="copy-terminal-btn" onclick="copyTerminal(this)">
              ğŸ“‹ è¤‡è£½æŒ‡ä»¤
            </button>
          </div>

          <div class="ai-prompt-block">
            <p>
              è®“ AI å¹«ä½ å»ºç«‹è™›æ“¬ç’°å¢ƒã€å°ˆæ¡ˆçµæ§‹å’Œæª”æ¡ˆï¼š
            </p>
            <pre>
è«‹å¹«æˆ‘å»ºç«‹ä¸€å€‹ Python å°ˆæ¡ˆï¼Œç”¨æ–¼è¨“ç·´åå­—ç”Ÿæˆå™¨ï¼š

âš ï¸ é‡è¦ï¼šè«‹ä½¿ç”¨ Python è™›æ“¬ç’°å¢ƒï¼ˆvenvï¼‰ä¾†ç®¡ç†å°ˆæ¡ˆä¾è³´ï¼

è«‹åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š
1. å»ºç«‹ Python è™›æ“¬ç’°å¢ƒï¼špython3 -m venv venv
2. å•Ÿå‹•è™›æ“¬ç’°å¢ƒï¼šsource venv/bin/activate (macOS/Linux) æˆ– venv\Scripts\activate (Windows)
3. ç¢ºèªè™›æ“¬ç’°å¢ƒå·²å•Ÿå‹•ï¼ˆçµ‚ç«¯æ©Ÿæç¤ºç¬¦æœƒé¡¯ç¤º (venv)ï¼‰
4. å®‰è£ä¾è³´å¥—ä»¶ï¼špip install torch numpy matplotlib

ç„¶å¾Œå»ºç«‹ä»¥ä¸‹æª”æ¡ˆå’Œç›®éŒ„çµæ§‹ï¼š
- transformer.py (ä¸»è¦æ¨¡å‹æª”æ¡ˆï¼Œå…ˆç•™ç©ºæˆ–åŠ ä¸ŠåŸºæœ¬è¨»è§£)
- train.py (è¨“ç·´è…³æœ¬ï¼Œå…ˆç•™ç©º)
- generate.py (ç”Ÿæˆè…³æœ¬ï¼Œå…ˆç•™ç©º)
- data.py (è³‡æ–™è™•ç†ï¼Œå…ˆç•™ç©º)
- requirements.txt (åŒ…å«ï¼štorch, numpy, matplotlib)
- README.md (å°ˆæ¡ˆç°¡ä»‹ï¼ŒåŒ…å«ï¼šå°ˆæ¡ˆç›®çš„ã€å¦‚ä½•å»ºç«‹è™›æ“¬ç’°å¢ƒã€å¦‚ä½•åŸ·è¡Œ)

è«‹å»ºç«‹é€™äº›æª”æ¡ˆï¼Œä¸¦åœ¨ README.md ä¸­å¯«ä¸Šå®Œæ•´çš„å°ˆæ¡ˆèªªæ˜å’Œè™›æ“¬ç’°å¢ƒä½¿ç”¨æŒ‡å—ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-setup">
            <h4>ğŸ’¡ æç¤º</h4>
            <ul>
              <li>
                <strong>é‡è¦ï¼šè«‹ä½¿ç”¨è™›æ“¬ç’°å¢ƒï¼</strong>
                PyTorch æ˜¯ä¸€å€‹å¤§å¥—ä»¶ï¼Œä½¿ç”¨è™›æ“¬ç’°å¢ƒå¯ä»¥é¿å…å¥—ä»¶è¡çªã€‚
              </li>
              <li>å»ºè­°ä½¿ç”¨ Python 3.8+</li>
              <li>PyTorch æœƒè‡ªå‹•åµæ¸¬ä½ çš„ç³»çµ±ï¼ˆMac æœƒä½¿ç”¨ MPSï¼Œå¦‚æœæœ‰ GPU æœƒç”¨ CUDAï¼‰</li>
              <li>å®Œå…¨å¯ä»¥åœ¨ Mac 128GB æœ¬æ©Ÿé‹è¡Œï¼Œç„¡éœ€ GPU</li>
            </ul>
          </div>
        </section>

        <!-- Step 3: æº–å‚™è³‡æ–™ -->
        <section id="step3" class="lab-section">
          <h2>ğŸ“š Step 3: æº–å‚™åå­—è³‡æ–™é›†</h2>
          
          <p>
            æˆ‘å€‘éœ€è¦ä¸€å€‹åå­—è³‡æ–™é›†ã€‚æˆ‘å€‘æœƒç”¨ Character-Level çš„æ–¹å¼è™•ç†ï¼Œè®“æ¨¡å‹å­¸ç¿’ã€Œå­—æ¯çš„çµ„åˆè¦å¾‹ã€ã€‚
          </p>

          <div class="visual-diagram">
            <h4>ğŸ“Š è³‡æ–™æ ¼å¼</h4>
            <pre>
åå­—åˆ—è¡¨ï¼š
  ["Emma", "Liam", "Olivia", "Noah", "Ava", "Ethan", ...]

Character-Level Tokenizationï¼š
  "Emma" â†’ ['E', 'm', 'm', 'a', '<END>']
  "Liam" â†’ ['L', 'i', 'a', 'm', '<END>']

è©å½™è¡¨ï¼ˆVocabï¼‰ï¼š
  {'<START>': 0, '<END>': 1, 'E': 2, 'm': 3, 'a': 4, 'L': 5, 'i': 6, ...}
            </pre>
          </div>

          <div class="ai-prompt-block">
            <p>
              <strong>æº–å‚™è³‡æ–™ Prompt</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘å»ºç«‹è³‡æ–™è™•ç†æ¨¡çµ„ `data.py`ï¼š

åŠŸèƒ½ï¼š
1. å®šç¾©ä¸€å€‹åå­—åˆ—è¡¨ï¼ˆè‡³å°‘ 100 å€‹è‹±æ–‡åå­—ï¼Œä¾‹å¦‚ï¼šEmma, Liam, Olivia, Noah, Ava, Ethan, Sophia, Mason, Isabella, William, Mia, James...ï¼‰
2. å»ºç«‹ Character-Level Tokenizerï¼š
   - å°‡æ¯å€‹åå­—è½‰æ›æˆå­—å…ƒåˆ—è¡¨
   - å»ºç«‹è©å½™è¡¨ï¼ˆvocabï¼‰ï¼šåŒ…å«æ‰€æœ‰å‡ºç¾çš„å­—å…ƒ + '<START>' + '<END>'
   - æä¾› encode() å’Œ decode() å‡½æ•¸
3. å»ºç«‹è³‡æ–™é›†é¡åˆ¥ï¼ˆDatasetï¼‰ï¼š
   - å°‡åå­—è½‰æ›æˆè¨“ç·´ç”¨çš„åºåˆ—å° (input, target)
   - ä¾‹å¦‚ï¼š"Emma" â†’ input: [<START>, E, m, m, a], target: [E, m, m, a, <END>]
4. æä¾›ä¸€å€‹å‡½æ•¸ä¾†ç²å–è³‡æ–™è¼‰å…¥å™¨ï¼ˆDataLoaderï¼‰

è¦æ±‚ï¼š
- ä½¿ç”¨ PyTorch çš„ Dataset å’Œ DataLoader
- è™•ç†ä¸åŒé•·åº¦çš„åå­—ï¼ˆå¯ä»¥ç”¨ padding æˆ–å‹•æ…‹é•·åº¦ï¼‰
- åŠ ä¸Šè©³ç´°è¨»è§£

è«‹å»ºç«‹ data.py æª”æ¡ˆï¼ŒåŒ…å«å®Œæ•´çš„å¯¦ä½œã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-code-block collapsed">
            <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
              <span class="filename">data.py</span>
              <div class="header-buttons">
                <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
              </div>
            </div>
            <div class="lab-code-block-content">
              <pre><code class="language-python">import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter

# åå­—è³‡æ–™é›†ï¼ˆå¯ä»¥æ“´å……æ›´å¤šï¼‰
NAMES = [
    "Emma", "Liam", "Olivia", "Noah", "Ava", "Ethan", "Sophia", "Mason",
    "Isabella", "William", "Mia", "James", "Charlotte", "Benjamin", "Amelia",
    "Lucas", "Harper", "Henry", "Evelyn", "Alexander", "Abigail", "Michael",
    "Emily", "Daniel", "Elizabeth", "Matthew", "Sofia", "Jackson", "Avery",
    "David", "Ella", "Joseph", "Madison", "Samuel", "Scarlett", "Sebastian",
    "Victoria", "Carter", "Aria", "Owen", "Grace", "Wyatt", "Chloe",
    "John", "Penelope", "Jack", "Riley", "Luke", "Layla", "Jayden", "Nora",
    "Dylan", "Zoey", "Grayson", "Mila", "Levi", "Aubrey", "Isaac", "Hannah",
    "Gabriel", "Addison", "Julian", "Eleanor", "Mateo", "Natalie", "Anthony",
    "Lily", "Jaxon", "Lillian", "Lincoln", "Aurora", "Joshua", "Savannah",
    "Christopher", "Leah", "Andrew", "Audrey", "Theodore", "Brooklyn",
    "Caleb", "Bella", "Ryan", "Claire", "Asher", "Skylar", "Nathan", "Lucy",
    "Thomas", "Paisley", "Leo", "Everly", "Isaiah", "Anna", "Charles", "Caroline",
    "Josiah", "Nova", "Hudson", "Genesis", "Christian", "Aaliyah", "Hunter",
    "Kennedy", "Connor", "Kinsley", "Eli", "Allison", "Ezra", "Maya", "Aaron",
    "Willow", "Landon", "Naomi", "Adrian", "Elena", "Jonathan", "Sarah",
    "Nolan", "Ariana", "Jeremiah", "Allison", "Easton", "Gabriella", "Elias",
    "Alice", "Colton", "Madelyn", "Cameron", "Cora", "Carson", "Ruby",
    "Robert", "Eva", "Angel", "Serenity", "Maverick", "Autumn", "Nicholas",
    "Adeline", "Dominic", "Hailey", "Jaxson", "Gianna", "Greyson", "Valentina",
    "Adam", "Isla", "Ian", "Eliana", "Austin", "Quinn", "Santiago", "Nevaeh",
    "Jordan", "Ivy", "Cooper", "Sadie", "Brayden", "Piper", "Leo", "Lydia",
    "Jason", "Alexa", "Xavier", "Josephine", "Jose", "Emilia", "Chase", "Delilah",
    "Kevin", "Arianna", "Zachary", "Violet", "Parker", "Ariella", "Blake", "Willow",
    "Ayden", "Maya", "Carlos", "Liliana", "Justin", "Natalia", "Bentley", "Katherine"
]

class CharTokenizer:
    """Character-Level Tokenizer"""
    
    def __init__(self, names):
        # å»ºç«‹è©å½™è¡¨
        all_chars = set()
        for name in names:
            all_chars.update(name)
        
        # ç‰¹æ®Š token
        self.vocab = {'<START>': 0, '<END>': 1}
        # å­—å…ƒ token
        for i, char in enumerate(sorted(all_chars), start=2):
            self.vocab[char] = i
        
        # åå‘æ˜ å°„
        self.idx_to_char = {idx: char for char, idx in self.vocab.items()}
        self.vocab_size = len(self.vocab)
    
    def encode(self, text):
        """å°‡æ–‡å­—è½‰æ›æˆ token IDs"""
        return [self.vocab[char] for char in text]
    
    def decode(self, ids):
        """å°‡ token IDs è½‰æ›æˆæ–‡å­—"""
        return ''.join([self.idx_to_char[idx] for idx in ids if idx != self.vocab['<END>']])

class NameDataset(Dataset):
    """åå­—è³‡æ–™é›†"""
    
    def __init__(self, names, tokenizer):
        self.names = names
        self.tokenizer = tokenizer
        self.max_len = max(len(name) for name in names) + 2  # +2 for START and END
    
    def __len__(self):
        return len(self.names)
    
    def __getitem__(self, idx):
        name = self.names[idx]
        # input: [<START>, E, m, m, a]
        # target: [E, m, m, a, <END>]
        input_seq = [self.tokenizer.vocab['<START>']] + self.tokenizer.encode(name)
        target_seq = self.tokenizer.encode(name) + [self.tokenizer.vocab['<END>']]
        
        # Padding åˆ°ç›¸åŒé•·åº¦
        input_seq = input_seq + [self.tokenizer.vocab['<END>']] * (self.max_len - len(input_seq))
        target_seq = target_seq + [self.tokenizer.vocab['<END>']] * (self.max_len - len(target_seq))
        
        return torch.tensor(input_seq[:self.max_len]), torch.tensor(target_seq[:self.max_len])

def get_data_loader(names, tokenizer, batch_size=32, shuffle=True):
    """å»ºç«‹è³‡æ–™è¼‰å…¥å™¨"""
    dataset = NameDataset(names, tokenizer)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

if __name__ == "__main__":
    # æ¸¬è©¦
    tokenizer = CharTokenizer(NAMES)
    print(f"è©å½™è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"è©å½™è¡¨: {tokenizer.vocab}")
    
    # æ¸¬è©¦ encode/decode
    test_name = "Emma"
    encoded = tokenizer.encode(test_name)
    decoded = tokenizer.decode(encoded)
    print(f"\næ¸¬è©¦: '{test_name}'")
    print(f"Encoded: {encoded}")
    print(f"Decoded: '{decoded}'")
    
    # æ¸¬è©¦ Dataset
    dataset = NameDataset(NAMES[:5], tokenizer)
    print(f"\nè³‡æ–™é›†å¤§å°: {len(dataset)}")
    input_seq, target_seq = dataset[0]
    print(f"ç¬¬ä¸€å€‹æ¨£æœ¬:")
    print(f"  Input: {input_seq}")
    print(f"  Target: {target_seq}")</code></pre>
            </div>
          </div>
        </section>

        <!-- Step 4: å¯¦ä½œçµ„ä»¶ -->
        <section id="step4" class="lab-section">
          <h2>ğŸ› ï¸ Step 4: å¯¦ä½œ Transformer çµ„ä»¶</h2>
          
          <p>
            æˆ‘å€‘éœ€è¦å¯¦ä½œå¹¾å€‹æ ¸å¿ƒçµ„ä»¶ï¼šLayerNormã€Feed-Forward Networkã€Transformer Blockã€‚
          </p>

          <div class="visual-diagram">
            <h4>ğŸ“ Transformer Block çµæ§‹</h4>
            <pre>
Transformer Block:
  è¼¸å…¥ x
    â†“
  Multi-Head Attention
    â†“
  Add & Norm (Residual + LayerNorm)
    â†“
  Feed-Forward Network
    â†“
  Add & Norm (Residual + LayerNorm)
    â†“
  è¼¸å‡º
            </pre>
          </div>

          <h3>ğŸ¯ å¯¦ä½œæ­¥é©Ÿ</h3>
          <p>æˆ‘å€‘è¦å¯¦ä½œï¼š</p>
          <ol class="lab-list-spacing">
            <li>LayerNormï¼ˆå±¤æ¨™æº–åŒ–ï¼‰</li>
            <li>Feed-Forward Networkï¼ˆå‰é¥‹ç¶²è·¯ï¼‰</li>
            <li>Transformer Blockï¼ˆçµ„åˆæ‰€æœ‰çµ„ä»¶ï¼‰</li>
          </ol>

          <div class="ai-prompt-block">
            <p>
              <strong>ç¬¬ä¸€å€‹ Promptï¼šå¯¦ä½œ LayerNorm å’Œ FFN</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘å¯¦ä½œ Transformer çš„æ ¸å¿ƒçµ„ä»¶ï¼š

1. LayerNormï¼ˆå±¤æ¨™æº–åŒ–ï¼‰ï¼š
   - ä½¿ç”¨ PyTorch çš„ nn.LayerNorm
   - æˆ–æ‰‹å‹•å¯¦ä½œï¼šå°æœ€å¾Œä¸€å€‹ç¶­åº¦é€²è¡Œæ¨™æº–åŒ–

2. Feed-Forward Networkï¼ˆå‰é¥‹ç¶²è·¯ï¼‰ï¼š
   - å…©å€‹ç·šæ€§å±¤ï¼šd_model â†’ d_ff â†’ d_model
   - ä¸­é–“åŠ ä¸Š ReLU æ¿€æ´»å‡½æ•¸
   - å¯ä»¥åŠ ä¸Š Dropoutï¼ˆå¯é¸ï¼‰

3. å»ºç«‹ä¸€å€‹ç°¡å–®çš„æ¸¬è©¦ï¼Œé©—è­‰çµ„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ

è«‹å»ºç«‹ transformer.py æª”æ¡ˆï¼ŒåŒ…å«é€™äº›çµ„ä»¶çš„å¯¦ä½œã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-code-block collapsed">
            <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
              <span class="filename">transformer.py (éƒ¨åˆ†)</span>
              <div class="header-buttons">
                <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
              </div>
            </div>
            <div class="lab-code-block-content">
              <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class FeedForward(nn.Module):
    """Feed-Forward Network"""
    
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # d_model â†’ d_ff â†’ d_model
        return self.linear2(self.dropout(F.relu(self.linear1(x))))

class TransformerBlock(nn.Module):
    """Transformer Block: Attention + FFN + Residual + LayerNorm"""
    
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        # æ³¨æ„ï¼šé€™è£¡ç°¡åŒ–ï¼Œå¯¦éš›éœ€è¦å¯¦ä½œ Multi-Head Attention
        # å¯ä»¥å…ˆä½¿ç”¨ PyTorch çš„ nn.MultiheadAttention
        self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # Self-Attention + Residual + LayerNorm
        attn_output, _ = self.attention(x, x, x)
        x = self.norm1(x + self.dropout(attn_output))
        
        # FFN + Residual + LayerNorm
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))
        
        return x</code></pre>
            </div>
          </div>
        </section>

        <!-- Step 5: çµ„è£ Transformer -->
        <section id="step5" class="lab-section">
          <h2>ğŸ”§ Step 5: çµ„è£å®Œæ•´ Transformer</h2>
          
          <p>
            ç¾åœ¨æˆ‘å€‘è¦æŠŠæ‰€æœ‰çµ„ä»¶çµ„è£æˆå®Œæ•´çš„æ¨¡å‹ï¼
          </p>

          <div class="ai-prompt-block">
            <p>
              <strong>çµ„è£ Transformer Prompt</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘çµ„è£å®Œæ•´çš„ Transformer æ¨¡å‹ï¼ˆMiniGPTï¼‰ï¼š

æ¶æ§‹ï¼š
1. Embedding Layerï¼šå°‡ token IDs è½‰æ›æˆå‘é‡
2. Positional Encodingï¼šåŠ ä¸Šä½ç½®è³‡è¨Šï¼ˆå¯ä»¥ç”¨å­¸ç¿’å¼æˆ–å›ºå®šå¼ï¼‰
3. N å€‹ Transformer Blocksï¼ˆä¾‹å¦‚ 2-4 å€‹ï¼‰
4. Output Layerï¼šå°‡å‘é‡æŠ•å½±åˆ°è©å½™è¡¨å¤§å°ï¼Œç”¨æ–¼é æ¸¬ä¸‹ä¸€å€‹å­—

åƒæ•¸å»ºè­°ï¼š
- d_model: 64 æˆ– 128ï¼ˆæ¨¡å‹ç¶­åº¦ï¼‰
- num_heads: 4 æˆ– 8ï¼ˆæ³¨æ„åŠ›é ­æ•¸ï¼‰
- num_layers: 2 æˆ– 4ï¼ˆTransformer Block æ•¸é‡ï¼‰
- d_ff: d_model * 4ï¼ˆå‰é¥‹ç¶²è·¯ç¶­åº¦ï¼‰

è«‹åœ¨ transformer.py ä¸­å»ºç«‹å®Œæ•´çš„ MiniGPT é¡åˆ¥ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-code-block collapsed">
            <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
              <span class="filename">transformer.py (å®Œæ•´æ¨¡å‹)</span>
              <div class="header-buttons">
                <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
              </div>
            </div>
            <div class="lab-code-block-content">
              <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class PositionalEncoding(nn.Module):
    """ä½ç½®ç·¨ç¢¼ï¼ˆå­¸ç¿’å¼ï¼‰"""
    
    def __init__(self, d_model, max_len=100):
        super().__init__()
        self.pos_encoding = nn.Parameter(torch.randn(max_len, d_model))
    
    def forward(self, x):
        seq_len = x.size(1)
        return x + self.pos_encoding[:seq_len, :]

class FeedForward(nn.Module):
    """Feed-Forward Network"""
    
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        return self.linear2(self.dropout(F.relu(self.linear1(x))))

class TransformerBlock(nn.Module):
    """Transformer Block"""
    
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # Self-Attention + Residual + LayerNorm
        attn_output, _ = self.attention(x, x, x)
        x = self.norm1(x + self.dropout(attn_output))
        
        # FFN + Residual + LayerNorm
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))
        
        return x

class MiniGPT(nn.Module):
    """è¿·ä½  GPTï¼šCharacter-Level Language Model"""
    
    def __init__(self, vocab_size, d_model=64, num_heads=4, num_layers=2, d_ff=256, dropout=0.1, max_len=20):
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # Embedding
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        # Positional Encoding
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        
        # Transformer Blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        
        # Output Layer
        self.output = nn.Linear(d_model, vocab_size)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # x: (batch_size, seq_len)
        # Embedding
        x = self.embedding(x) * math.sqrt(self.d_model)  # ç¸®æ”¾
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        # Transformer Blocks
        for block in self.blocks:
            x = block(x)
        
        # Output
        logits = self.output(x)  # (batch_size, seq_len, vocab_size)
        return logits
    
    def generate(self, tokenizer, start_char='<START>', max_len=10, temperature=1.0):
        """ç”Ÿæˆåå­—"""
        self.eval()
        generated = [tokenizer.vocab[start_char]]
        
        with torch.no_grad():
            for _ in range(max_len):
                # æº–å‚™è¼¸å…¥
                x = torch.tensor([generated]).long()
                
                # å‰å‘å‚³æ’­
                logits = self.forward(x)
                
                # å–æœ€å¾Œä¸€å€‹ä½ç½®çš„ logits
                next_logits = logits[0, -1, :] / temperature
                
                # å–æ¨£
                probs = F.softmax(next_logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()
                
                # å¦‚æœé‡åˆ° <END>ï¼Œåœæ­¢ç”Ÿæˆ
                if next_token == tokenizer.vocab['<END>']:
                    break
                
                generated.append(next_token)
        
        # ç§»é™¤ <START>
        generated = generated[1:] if generated[0] == tokenizer.vocab['<START>'] else generated
        return tokenizer.decode(generated)</code></pre>
            </div>
          </div>
        </section>

        <!-- Step 6: è¨“ç·´èˆ‡ç”Ÿæˆ -->
        <section id="step6" class="lab-section">
          <h2>ğŸš€ Step 6: è¨“ç·´èˆ‡ç”Ÿæˆåå­—</h2>
          
          <p>
            æœ€æ¿€å‹•äººå¿ƒçš„æ™‚åˆ»ï¼è¨“ç·´ä½ çš„ç¬¬ä¸€å€‹ AI å‰µä½œè€…ï¼Œçœ‹è‘—å®ƒå¾äº‚ç¢¼åˆ°èƒ½ç”Ÿæˆæœ‰æ„ç¾©çš„åå­—ï¼
          </p>

          <div class="ai-prompt-block">
            <p>
              <strong>è¨“ç·´ Prompt</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘å»ºç«‹è¨“ç·´è…³æœ¬ `train.py`ï¼š

åŠŸèƒ½ï¼š
1. è¼‰å…¥è³‡æ–™å’Œå»ºç«‹è³‡æ–™è¼‰å…¥å™¨
2. åˆå§‹åŒ–æ¨¡å‹ï¼ˆMiniGPTï¼‰
3. è¨­å®šå„ªåŒ–å™¨ï¼ˆAdamï¼‰å’Œæå¤±å‡½æ•¸ï¼ˆCrossEntropyLossï¼‰
4. è¨“ç·´è¿´åœˆï¼š
   - å°æ¯å€‹ epoch
   - å°æ¯å€‹ batch
   - å‰å‘å‚³æ’­ã€è¨ˆç®—æå¤±ã€åå‘å‚³æ’­ã€æ›´æ–°åƒæ•¸
   - å®šæœŸæ‰“å°æå¤±å’Œé€²åº¦
5. ä¿å­˜æ¨¡å‹

è¦æ±‚ï¼š
- è¨“ç·´ç´„ 50-100 å€‹ epochs
- æ¯ 10 å€‹ epochs æ‰“å°ä¸€æ¬¡æå¤±
- ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆloss æœ€ä½çš„ï¼‰
- åŠ ä¸Šè©³ç´°è¨»è§£

è«‹å»ºç«‹ train.py æª”æ¡ˆï¼ŒåŒ…å«å®Œæ•´çš„è¨“ç·´æµç¨‹ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-code-block collapsed">
            <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
              <span class="filename">train.py</span>
              <div class="header-buttons">
                <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
              </div>
            </div>
            <div class="lab-code-block-content">
              <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from data import NAMES, CharTokenizer, get_data_loader
from transformer import MiniGPT

# è¨­å®š
BATCH_SIZE = 32
LEARNING_RATE = 0.001
NUM_EPOCHS = 100
D_MODEL = 64
NUM_HEADS = 4
NUM_LAYERS = 2
D_FF = 256

# å»ºç«‹ tokenizer å’Œè³‡æ–™è¼‰å…¥å™¨
tokenizer = CharTokenizer(NAMES)
train_loader = get_data_loader(NAMES, tokenizer, batch_size=BATCH_SIZE)

# å»ºç«‹æ¨¡å‹
model = MiniGPT(
    vocab_size=tokenizer.vocab_size,
    d_model=D_MODEL,
    num_heads=NUM_HEADS,
    num_layers=NUM_LAYERS,
    d_ff=D_FF
)

# å„ªåŒ–å™¨å’Œæå¤±å‡½æ•¸
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.vocab['<END>'])

# è¨“ç·´
print("é–‹å§‹è¨“ç·´åå­—ç”Ÿæˆå™¨...")
print(f"æ¨¡å‹åƒæ•¸æ•¸é‡: {sum(p.numel() for p in model.parameters()):,}")
print(f"è¨“ç·´è³‡æ–™: {len(NAMES)} å€‹åå­—")
print("-" * 60)

best_loss = float('inf')

for epoch in range(NUM_EPOCHS):
    model.train()
    total_loss = 0
    
    for batch_idx, (input_seq, target_seq) in enumerate(train_loader):
        # å‰å‘å‚³æ’­
        logits = model(input_seq)  # (batch_size, seq_len, vocab_size)
        
        # è¨ˆç®—æå¤±ï¼ˆéœ€è¦ reshapeï¼‰
        logits = logits.view(-1, logits.size(-1))  # (batch_size * seq_len, vocab_size)
        targets = target_seq.view(-1)  # (batch_size * seq_len)
        
        loss = criterion(logits, targets)
        
        # åå‘å‚³æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    avg_loss = total_loss / len(train_loader)
    
    # æ¯ 10 å€‹ epochs æ‰“å°ä¸€æ¬¡
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}")
        
        # ç”Ÿæˆå¹¾å€‹åå­—çœ‹çœ‹æ•ˆæœ
        model.eval()
        print("  ç”Ÿæˆçš„åå­—:")
        for _ in range(5):
            name = model.generate(tokenizer, max_len=10)
            print(f"    - {name}")
        print()
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if avg_loss < best_loss:
        best_loss = avg_loss
        torch.save(model.state_dict(), 'best_model.pth')

print("è¨“ç·´å®Œæˆï¼")
print(f"æœ€ä½³æå¤±: {best_loss:.4f}")
print("æ¨¡å‹å·²ä¿å­˜ç‚º 'best_model.pth'")</code></pre>
            </div>
          </div>

          <div class="ai-prompt-block lab-spacing-top">
            <p>
              <strong>ç”Ÿæˆåå­— Prompt</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘å»ºç«‹ç”Ÿæˆè…³æœ¬ `generate.py`ï¼š

åŠŸèƒ½ï¼š
1. è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
2. ä½¿ç”¨æ¨¡å‹çš„ generate() æ–¹æ³•ç”Ÿæˆåå­—
3. ç”Ÿæˆå¤šå€‹åå­—ï¼ˆä¾‹å¦‚ 20 å€‹ï¼‰
4. å¯ä»¥èª¿æ•´ temperature åƒæ•¸ï¼ˆæ§åˆ¶å‰µé€ æ€§ï¼‰

è¦æ±‚ï¼š
- è¼‰å…¥ best_model.pth
- ç”Ÿæˆ 20 å€‹åå­—
- å¯ä»¥å˜—è©¦ä¸åŒçš„ temperatureï¼ˆ0.5, 1.0, 1.5ï¼‰
- é¡¯ç¤ºç”Ÿæˆçµæœ

è«‹å»ºç«‹ generate.py æª”æ¡ˆã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-code-block collapsed">
            <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
              <span class="filename">generate.py</span>
              <div class="header-buttons">
                <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
              </div>
            </div>
            <div class="lab-code-block-content">
              <pre><code class="language-python">import torch
from data import NAMES, CharTokenizer
from transformer import MiniGPT

# è¼‰å…¥ tokenizer
tokenizer = CharTokenizer(NAMES)

# å»ºç«‹æ¨¡å‹ï¼ˆåƒæ•¸è¦å’Œè¨“ç·´æ™‚ä¸€æ¨£ï¼‰
model = MiniGPT(
    vocab_size=tokenizer.vocab_size,
    d_model=64,
    num_heads=4,
    num_layers=2,
    d_ff=256
)

# è¼‰å…¥è¨“ç·´å¥½çš„æ¬Šé‡
model.load_state_dict(torch.load('best_model.pth'))
model.eval()

print("ğŸ¨ åå­—ç”Ÿæˆå™¨")
print("=" * 60)

# ç”Ÿæˆåå­—
num_names = 20
temperatures = [0.5, 1.0, 1.5]

for temp in temperatures:
    print(f"\nğŸŒ¡ï¸ Temperature = {temp} (å‰µé€ æ€§: {'ä½' if temp < 1 else 'ä¸­' if temp == 1 else 'é«˜'})")
    print("-" * 60)
    
    generated_names = []
    for _ in range(num_names):
        name = model.generate(tokenizer, max_len=10, temperature=temp)
        if name and name not in generated_names:  # é¿å…é‡è¤‡
            generated_names.append(name)
            print(f"  âœ¨ {name}")
    
    print(f"\n  å…±ç”Ÿæˆ {len(generated_names)} å€‹ä¸é‡è¤‡çš„åå­—")

print("\n" + "=" * 60)
print("âœ… ç”Ÿæˆå®Œæˆï¼")</code></pre>
            </div>
          </div>

          <div class="ai-prompt-block lab-spacing-top">
            <p>
              <strong>åŸ·è¡Œè¨“ç·´</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘åŸ·è¡Œè¨“ç·´ä¸¦æª¢æŸ¥çµæœï¼š

1. åœ¨è™›æ“¬ç’°å¢ƒä¸­åŸ·è¡Œï¼špython train.py
2. è§€å¯Ÿè¨“ç·´éç¨‹ï¼Œæª¢æŸ¥æå¤±æ˜¯å¦ä¸‹é™
3. æŸ¥çœ‹ç”Ÿæˆçš„åå­—æ˜¯å¦è¶Šä¾†è¶Šåˆç†
4. å¦‚æœæœ‰éŒ¯èª¤ï¼Œè«‹å¹«æˆ‘ä¿®æ­£

è¨“ç·´å¯èƒ½éœ€è¦å¹¾åˆ†é˜åˆ°åå¹¾åˆ†é˜ï¼Œè«‹è€å¿ƒç­‰å¾…ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-expected-output">
            <pre>
é–‹å§‹è¨“ç·´åå­—ç”Ÿæˆå™¨...
æ¨¡å‹åƒæ•¸æ•¸é‡: 45,234
è¨“ç·´è³‡æ–™: 100 å€‹åå­—
------------------------------------------------------------
Epoch 10/100, Loss: 2.3456
  ç”Ÿæˆçš„åå­—:
    - Emlia
    - Noaven
    - Sophiam
    - Livianna
    - Jamesly

Epoch 20/100, Loss: 1.8923
  ç”Ÿæˆçš„åå­—:
    - Emma
    - Liam
    - Olivia
    - Noah
    - Ava

Epoch 30/100, Loss: 1.4567
  ç”Ÿæˆçš„åå­—:
    - Emmar
    - Liama
    - Olivian
    - Noahan
    - Avara

...

è¨“ç·´å®Œæˆï¼
æœ€ä½³æå¤±: 0.8234
æ¨¡å‹å·²ä¿å­˜ç‚º 'best_model.pth'
            </pre>
          </div>

          <div class="ai-prompt-block lab-spacing-top">
            <p>
              <strong>åŸ·è¡Œç”Ÿæˆ</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘åŸ·è¡Œç”Ÿæˆè…³æœ¬ä¸¦æª¢æŸ¥çµæœï¼š

1. åœ¨è™›æ“¬ç’°å¢ƒä¸­åŸ·è¡Œï¼špython generate.py
2. æŸ¥çœ‹ç”Ÿæˆçš„åå­—
3. æ¯”è¼ƒä¸åŒ temperature çš„æ•ˆæœ
4. å¦‚æœæœ‰éŒ¯èª¤ï¼Œè«‹å¹«æˆ‘ä¿®æ­£

è«‹åŸ·è¡Œç”Ÿæˆä¸¦å‘Šè¨´æˆ‘çµæœã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>
        </section>

        <!-- Challenge -->
        <div class="lab-challenge lab-spacing-top-lg">
          <h4>ğŸ’ª æŒ‘æˆ°ä»»å‹™</h4>
          <p>
            <strong>é€²éšæŒ‘æˆ° 1ï¼š</strong>
            ç”¨ä¸­æ–‡åå­—è³‡æ–™é›†è¨“ç·´ï¼æ”¶é›† 100+ å€‹ä¸­æ–‡åå­—ï¼Œçœ‹çœ‹ AI èƒ½ç”Ÿæˆä»€éº¼ã€‚
          </p>
          <p>
            <strong>é€²éšæŒ‘æˆ° 2ï¼š</strong>
            è¨“ç·´ä¸€å€‹ã€Œè©©è©ç”Ÿæˆå™¨ã€ï¼šç”¨å¤è©©è©è³‡æ–™é›†ï¼Œè®“ AI å­¸æœƒå¯«è©©ã€‚
          </p>
          <p>
            <strong>é€²éšæŒ‘æˆ° 3ï¼š</strong>
            èª¿æ•´æ¨¡å‹å¤§å°ï¼šè©¦è©¦æ›´å¤§çš„æ¨¡å‹ï¼ˆd_model=128, num_layers=4ï¼‰ï¼Œçœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚
          </p>
        </div>

        <!-- Next Steps -->
        <div class="lab-setup lab-spacing-top-lg">
          <h4>ğŸ¯ ä¸‹ä¸€æ­¥</h4>
          <ul>
            <li>
              âœ… æ­å–œï¼ä½ å·²ç¶“å¾é›¶é–‹å§‹çµ„è£äº†å®Œæ•´çš„ Transformerï¼Œä¸¦è¨“ç·´äº†ä½ çš„ç¬¬ä¸€å€‹ AI å‰µä½œè€…ï¼
            </li>
            <li>
              â¡ï¸ æ¥ä¸‹ä¾†æˆ‘å€‘è¦é€²å…¥ <strong>Phase 3: èªè¨€æ¨¡å‹è¨“ç·´</strong>ï¼š
              ç”¨çœŸå¯¦çš„å¤§è³‡æ–™é›†è¨“ç·´ GPT-2 é¢¨æ ¼çš„æ¨¡å‹ï¼
            </li>
            <li>
              ğŸ“š å»ºè­°ï¼šå…ˆå®ŒæˆæŒ‘æˆ°ä»»å‹™ï¼Œå†é€²å…¥ä¸‹ä¸€å€‹ Phaseã€‚
            </li>
          </ul>
        </div>

        <div class="chapter-nav lab-spacing-top-lg">
          <a href="../index.html" class="nav-link">â† è¿”å› Phase 2</a>
          <a href="../02-multihead-attention/index.html" class="nav-link">â† ä¸Šä¸€å€‹å¯¦é©—ï¼šå¤šè§’åº¦åˆ†æå™¨</a>
        </div>
      </div>
    </div>

    <script>
      function copyPrompt(btn) {
        const promptBlock = btn.closest(".ai-prompt-block");
        const preElement = promptBlock.querySelector("pre");
        const text = preElement.textContent;

        navigator.clipboard.writeText(text).then(() => {
          btn.classList.add("copied");
          btn.textContent = "âœ“ å·²è¤‡è£½ï¼";
          setTimeout(() => {
            btn.classList.remove("copied");
            btn.textContent = "ğŸ“‹ è¤‡è£½ Prompt";
          }, 2000);
        });
      }

      function toggleCodeBlock(header) {
        const codeBlock = header.closest(".lab-code-block");
        const toggleBtn = header.querySelector(".toggle-btn");
        
        codeBlock.classList.toggle("collapsed");
        
        if (codeBlock.classList.contains("collapsed")) {
          toggleBtn.textContent = "å±•é–‹";
        } else {
          toggleBtn.textContent = "æ”¶èµ·";
        }
      }

      function copyCode(btn) {
        const codeBlock = btn.closest(".lab-code-block");
        const codeElement = codeBlock.querySelector("pre code");
        const text = codeElement.textContent;

        navigator.clipboard.writeText(text).then(() => {
          btn.textContent = "âœ“ å·²è¤‡è£½";
          setTimeout(() => {
            btn.textContent = "è¤‡è£½";
          }, 2000);
        });
      }

      function copyTerminal(btn) {
        const terminalBlock = btn.closest(".terminal-command");
        const codeElement = terminalBlock.querySelector("code");
        const text = codeElement.textContent;

        navigator.clipboard.writeText(text).then(() => {
          btn.textContent = "âœ“ å·²è¤‡è£½";
          setTimeout(() => {
            btn.textContent = "ğŸ“‹ è¤‡è£½æŒ‡ä»¤";
          }, 2000);
        });
      }
    </script>
  </body>
</html>
