<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1.2 Embedding - æ–‡å­—è®Šå‘é‡ | AI å¯¦é©— Playground</title>
    <!-- Playful Typography -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Fredoka:wght@400;500;600;700;800;900&family=Nunito:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500;600;700;800&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../../../styles/global.css" />
    <link rel="stylesheet" href="../../../styles/paper-reading.css" />
    <link rel="stylesheet" href="../../../styles/labs.css" />
    <style>
      body {
        font-family: "Nunito", sans-serif;
      }
      h1,
      h2,
      h3,
      h4 {
        font-family: "Fredoka", sans-serif;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <!-- Step Navigation -->
      <div class="lab-steps">
        <a href="../index.html" class="lab-step">â† Phase 1</a>
        <a href="#step1" class="lab-step active">Step 1: ç†è§£æ¦‚å¿µ</a>
        <a href="#step2" class="lab-step">Step 2: ç’°å¢ƒè¨­å®š</a>
        <a href="#step3" class="lab-step">Step 3: å¯¦ä½œ Word2Vec</a>
        <a href="#step4" class="lab-step">Step 4: è¦–è¦ºåŒ– Embedding</a>
      </div>

      <!-- Main Content -->
      <div class="story-container">
        <h1>ğŸ”¢ 1.2 Embedding - æ–‡å­—è®Šå‘é‡</h1>
        <p class="lab-intro-text">
          é‚„è¨˜å¾— Tokenization å—ï¼Ÿæˆ‘å€‘æŠŠæ–‡å­—ã€Œåˆ‡ã€æˆäº† tokensã€‚
          ç¾åœ¨è¦æ›´é€²ä¸€æ­¥ï¼šæŠŠé€™äº› tokens è®Šæˆã€Œå‘é‡ã€ï¼ˆä¸€çµ„æ•¸å­—ï¼‰ï¼
          é€™å°±æ˜¯ Embeddingï¼Œå®ƒæ˜¯ AIã€Œç†è§£ã€æ–‡å­—çš„é—œéµã€‚
        </p>

        <!-- Step 1: ç†è§£æ¦‚å¿µ -->
        <section id="step1" class="lab-section">
          <h2>ğŸ“ Step 1: ç†è§£ Embedding æ¦‚å¿µ</h2>
          
          <div class="visual-diagram">
            <h4>ğŸ“Š è¦–è¦ºåŒ–ï¼šä»€éº¼æ˜¯ Embeddingï¼Ÿ</h4>
            <p>
              æŠŠæ–‡å­—è½‰æ›æˆå‘é‡çš„éç¨‹ï¼š
            </p>
            <pre>
æ–‡å­—: "è²“"
  â†“ Tokenization
Token: "è²“" (ID: 1234)
  â†“ Embedding
å‘é‡: [0.2, -0.5, 0.8, 0.1, ...]  (ä¾‹å¦‚ 128 ç¶­)

æ–‡å­—: "ç‹—"
  â†“ Tokenization
Token: "ç‹—" (ID: 5678)
  â†“ Embedding
å‘é‡: [0.3, -0.4, 0.7, 0.2, ...]  (ç›¸ä¼¼çš„æ•¸å­—ï¼)

âœ¨ é‡é»ï¼šç›¸ä¼¼çš„è©å½™æœƒæœ‰ç›¸ä¼¼çš„å‘é‡ï¼
   "è²“" å’Œ "ç‹—" éƒ½æ˜¯å‹•ç‰©ï¼Œæ‰€ä»¥å‘é‡å¾ˆæ¥è¿‘
            </pre>
          </div>

          <div class="explanation">
            <h4>ğŸ” é‡è¦æ¦‚å¿µï¼šå¤š Token è©å½™çš„ Embedding è™•ç†</h4>
            <p>
              ç•¶ä¸€å€‹ä¸­æ–‡è©è¢«æ‹†æˆå¤šå€‹ tokens æ™‚ï¼Œembedding æ˜¯å¦‚ä½•è™•ç†çš„å‘¢ï¼Ÿ
            </p>
            
            <div class="visual-diagram">
              <h4>ğŸ“Š ç¯„ä¾‹ï¼šå¤š Token è©å½™çš„ Embedding</h4>
              <pre>
<strong>æƒ…æ³ 1ï¼šå–®ä¸€ Tokenï¼ˆç†æƒ³æƒ…æ³ï¼‰</strong>
æ–‡å­—: "è²“"
  â†“ Tokenization
Token: ["è²“"] â†’ 1 å€‹ token
  â†“ Embeddingï¼ˆæ¯å€‹ token å°æ‡‰ä¸€å€‹ vectorï¼‰
Embedding: [0.2, -0.5, 0.8, ...]  (1 å€‹ vectorï¼Œä¾‹å¦‚ 768 ç¶­)

<strong>æƒ…æ³ 2ï¼šå¤šå€‹ Tokensï¼ˆå¸¸è¦‹æƒ…æ³ï¼‰</strong>
æ–‡å­—: "è²“å’ª"
  â†“ Tokenization (GPT-3)
Tokens: ["è²“", "å’ª"] â†’ 2 å€‹ tokens
  â†“ Embeddingï¼ˆæ¯å€‹ token å°æ‡‰ä¸€å€‹ vectorï¼‰
Embedding: [
  [0.2, -0.5, 0.8, ...],  â† "è²“" çš„ vector
  [0.1, -0.3, 0.6, ...]   â† "å’ª" çš„ vector
]  (2 å€‹ vectorsï¼Œæ¯å€‹éƒ½æ˜¯ 768 ç¶­)

<strong>æƒ…æ³ 3ï¼šè¤‡é›œè©å½™ï¼ˆå¤šå€‹ Tokensï¼‰</strong>
æ–‡å­—: "äººå·¥æ™ºæ…§"
  â†“ Tokenization (GPT-3)
Tokens: ["äººå·¥", "æ™º", "æ…§"] â†’ 3 å€‹ tokens
  â†“ Embedding
Embedding: [
  [0.3, -0.4, 0.7, ...],  â† "äººå·¥" çš„ vector
  [0.2, -0.2, 0.5, ...],  â† "æ™º" çš„ vector
  [0.1, -0.1, 0.4, ...]   â† "æ…§" çš„ vector
]  (3 å€‹ vectorsï¼Œæ¯å€‹éƒ½æ˜¯ 768 ç¶­)

âœ¨ é—œéµç†è§£ï¼š
- æ¯å€‹ token éƒ½æœ‰è‡ªå·±ç¨ç«‹çš„ embedding vector
- å¤šå€‹ tokens = å¤šå€‹ vectorsï¼ˆä¸æ˜¯åˆä½µæˆä¸€å€‹ï¼‰
- æ¨¡å‹æœƒè™•ç†é€™å¤šå€‹ vectorsï¼Œå­¸ç¿’å®ƒå€‘çš„çµ„åˆæ„ç¾©
              </pre>
            </div>

            <div class="visual-diagram lab-spacing-top">
              <h4>ğŸ”„ æ¨¡å‹å¦‚ä½•è™•ç†å¤š Token Embeddingsï¼Ÿ</h4>
              <pre>
<strong>æ­¥é©Ÿ 1ï¼šTokenization</strong>
"è²“å’ª" â†’ ["è²“", "å’ª"] (2 tokens)

<strong>æ­¥é©Ÿ 2ï¼šEmbedding Lookupï¼ˆæŸ¥è¡¨ï¼‰</strong>
æ¯å€‹ token å¾ embedding è¡¨ä¸­æŸ¥æ‰¾å°æ‡‰çš„ vectorï¼š
- "è²“" (ID: 1234) â†’ vector_1: [0.2, -0.5, 0.8, ...]
- "å’ª" (ID: 5678) â†’ vector_2: [0.1, -0.3, 0.6, ...]

<strong>æ­¥é©Ÿ 3ï¼šTransformer è™•ç†</strong>
Transformer æ¥æ”¶å¤šå€‹ vectors ä½œç‚ºè¼¸å…¥ï¼š
Input: [vector_1, vector_2]  (2 å€‹ vectors)

Transformer æœƒï¼š
1. ä½¿ç”¨ Self-Attention è®“ tokens äº’ç›¸ã€Œäº¤æµã€
2. "è²“" å’Œ "å’ª" çš„ vectors æœƒäº’ç›¸å½±éŸ¿
3. æœ€çµ‚æ¯å€‹ token çš„è¡¨ç¤ºæœƒåŒ…å«ä¸Šä¸‹æ–‡è³‡è¨Š

<strong>æ­¥é©Ÿ 4ï¼šè¼¸å‡ºè¡¨ç¤º</strong>
æ¯å€‹ token ä½ç½®éƒ½æœ‰æœ€çµ‚çš„è¡¨ç¤ºï¼š
- Position 0: "è²“" çš„è¡¨ç¤ºï¼ˆå·²è€ƒæ…® "å’ª" çš„ä¸Šä¸‹æ–‡ï¼‰
- Position 1: "å’ª" çš„è¡¨ç¤ºï¼ˆå·²è€ƒæ…® "è²“" çš„ä¸Šä¸‹æ–‡ï¼‰

<strong>æ­¥é©Ÿ 5ï¼šå¦‚ä½•å¾—åˆ°æ•´å€‹è©çš„è¡¨ç¤ºï¼Ÿ</strong>
å¦‚æœéœ€è¦æ•´å€‹è© "è²“å’ª" çš„å–®ä¸€è¡¨ç¤ºï¼Œå¯ä»¥ï¼š
- æ–¹æ³• 1ï¼šå¹³å‡æ± åŒ–ï¼ˆAverage Poolingï¼‰
  è©è¡¨ç¤º = (vector_1 + vector_2) / 2
  
- æ–¹æ³• 2ï¼šä½¿ç”¨ç¬¬ä¸€å€‹ token çš„è¡¨ç¤º
  è©è¡¨ç¤º = vector_1ï¼ˆ"è²“" çš„è¡¨ç¤ºï¼‰
  
- æ–¹æ³• 3ï¼šä½¿ç”¨æœ€å¾Œä¸€å€‹ token çš„è¡¨ç¤º
  è©è¡¨ç¤º = vector_2ï¼ˆ"å’ª" çš„è¡¨ç¤ºï¼‰
  
- æ–¹æ³• 4ï¼šä½¿ç”¨ [CLS] tokenï¼ˆå¦‚æœæœ‰ï¼‰
  è©è¡¨ç¤º = [CLS] token çš„è¡¨ç¤º
              </pre>
            </div>

            <div class="visual-diagram lab-spacing-top">
              <h4>ğŸ“Š å¯¦éš›ä¾‹å­ï¼šæ¯”è¼ƒä¸åŒæ¨¡å‹çš„è™•ç†æ–¹å¼</h4>
              <pre>
<strong>ç¯„ä¾‹ï¼šè™•ç† "æˆ‘æ„›äººå·¥æ™ºæ…§"</strong>

<strong>GPT-3 çš„è™•ç†ï¼š</strong>
Tokenization: ["æˆ‘", "æ„›", "äººå·¥", "æ™º", "æ…§"] â†’ 5 tokens
Embedding: 5 å€‹ vectorsï¼Œæ¯å€‹ 768 ç¶­
  - "æˆ‘": [0.1, -0.2, 0.3, ...]
  - "æ„›": [0.2, -0.1, 0.4, ...]
  - "äººå·¥": [0.3, -0.3, 0.5, ...]
  - "æ™º": [0.1, -0.1, 0.2, ...]
  - "æ…§": [0.1, -0.1, 0.2, ...]

Transformer è™•ç†ï¼š5 å€‹ vectors â†’ 5 å€‹ä¸Šä¸‹æ–‡è¡¨ç¤º
æœ€çµ‚è¼¸å‡ºï¼š5 å€‹è¡¨ç¤ºï¼ˆæ¯å€‹ token ä¸€å€‹ï¼‰

<strong>Qwen çš„è™•ç†ï¼š</strong>
Tokenization: ["æˆ‘", "æ„›", "äººå·¥æ™ºæ…§"] â†’ 3 tokens
Embedding: 3 å€‹ vectorsï¼Œæ¯å€‹ 768 ç¶­
  - "æˆ‘": [0.1, -0.2, 0.3, ...]
  - "æ„›": [0.2, -0.1, 0.4, ...]
  - "äººå·¥æ™ºæ…§": [0.4, -0.4, 0.7, ...]  â† å®Œæ•´çš„è©è¡¨ç¤ºï¼

Transformer è™•ç†ï¼š3 å€‹ vectors â†’ 3 å€‹ä¸Šä¸‹æ–‡è¡¨ç¤º
æœ€çµ‚è¼¸å‡ºï¼š3 å€‹è¡¨ç¤ºï¼ˆæ›´é«˜æ•ˆï¼ï¼‰

âœ¨ é—œéµå·®ç•°ï¼š
- GPT-3: 5 tokens â†’ 5 vectors â†’ éœ€è¦æ›´å¤šè¨ˆç®—
- Qwen: 3 tokens â†’ 3 vectors â†’ æ›´é«˜æ•ˆï¼Œèªç¾©æ›´å®Œæ•´
              </pre>
            </div>

            <div class="explanation lab-spacing-top">
              <h4>ğŸ’¡ å¸¸è¦‹å•é¡Œè§£ç­”</h4>
              <p>
                <strong>Q1: ä¸€å€‹ token å°æ‡‰ä¸€å€‹ vector å—ï¼Ÿ</strong>
              </p>
              <p>
                <strong>A:</strong> æ˜¯çš„ï¼æ¯å€‹ token éƒ½æœ‰è‡ªå·±ç¨ç«‹çš„ embedding vectorã€‚
                å¦‚æœä¸€å€‹è©è¢«æ‹†æˆ 3 å€‹ tokensï¼Œå°±æœƒæœ‰ 3 å€‹ vectorsã€‚
              </p>
              
              <p>
                <strong>Q2: å¤šå€‹ tokens æœƒåˆä½µæˆä¸€å€‹ vector å—ï¼Ÿ</strong>
              </p>
              <p>
                <strong>A:</strong> ä¸æœƒè‡ªå‹•åˆä½µã€‚æ¯å€‹ token ä¿æŒç¨ç«‹çš„ vectorã€‚
                ä½† Transformer æœƒé€šé Self-Attention è®“é€™äº› vectors äº’ç›¸å½±éŸ¿ï¼Œ
                æœ€çµ‚æ¯å€‹ token çš„è¡¨ç¤ºæœƒåŒ…å«ä¸Šä¸‹æ–‡è³‡è¨Šã€‚
              </p>
              
              <p>
                <strong>Q3: å¦‚ä½•å¾—åˆ°æ•´å€‹è©çš„å–®ä¸€è¡¨ç¤ºï¼Ÿ</strong>
              </p>
              <p>
                <strong>A:</strong> å¯ä»¥ä½¿ç”¨å¤šç¨®æ–¹æ³•ï¼š
              </p>
              <ul>
                <li><strong>å¹³å‡æ± åŒ–</strong>ï¼šå°‡å¤šå€‹ token vectors å¹³å‡</li>
                <li><strong>ç¬¬ä¸€å€‹ token</strong>ï¼šä½¿ç”¨ç¬¬ä¸€å€‹ token çš„è¡¨ç¤º</li>
                <li><strong>æœ€å¾Œä¸€å€‹ token</strong>ï¼šä½¿ç”¨æœ€å¾Œä¸€å€‹ token çš„è¡¨ç¤º</li>
                <li><strong>ç‰¹æ®Š token</strong>ï¼šä½¿ç”¨ [CLS] æˆ– [SEP] token çš„è¡¨ç¤º</li>
              </ul>
              
              <p>
                <strong>Q4: ç‚ºä»€éº¼ä¸­æ–‡å„ªåŒ–æ¨¡å‹æ›´å¥½ï¼Ÿ</strong>
              </p>
              <p>
                <strong>A:</strong> å› ç‚ºå®ƒå€‘èƒ½å°‡å®Œæ•´çš„ä¸­æ–‡è©ä½œç‚ºå–®ä¸€ tokenï¼Œ
                é€™æ¨£ï¼š
              </p>
              <ul>
                <li>æ›´å°‘çš„ tokens â†’ æ›´å°‘çš„ vectors â†’ æ›´é«˜æ•ˆ</li>
                <li>å®Œæ•´çš„è©è¡¨ç¤º â†’ æ›´å¥½çš„èªç¾©ç†è§£</li>
                <li>æ›´ä½çš„æˆæœ¬ï¼ˆæ›´å°‘çš„ tokensï¼‰</li>
              </ul>
            </div>

            <div class="explanation lab-spacing-top">
              <h4>ğŸ” ä»€éº¼æ˜¯ [CLS] Tokenï¼Ÿ</h4>
              <p>
                <strong>[CLS]</strong> æ˜¯ "Classification" çš„ç¸®å¯«ï¼Œæ˜¯ä¸€å€‹ç‰¹æ®Šçš„ tokenï¼Œ
                ä¸»è¦ç”¨æ–¼ BERT ç­‰é›™å‘ Transformer æ¨¡å‹ä¸­ã€‚
              </p>
              
              <div class="visual-diagram">
                <h4>ğŸ“Š [CLS] Token çš„ä½œç”¨</h4>
                <pre>
<strong>1. ä½ç½®ï¼š</strong>
[CLS] token ç¸½æ˜¯æ”¾åœ¨å¥å­çš„æœ€å‰é¢

<strong>ç¯„ä¾‹ï¼š</strong>
åŸå§‹å¥å­: "æˆ‘æ„›äººå·¥æ™ºæ…§"
  â†“ åŠ ä¸Š [CLS] token
Tokenization: ["[CLS]", "æˆ‘", "æ„›", "äººå·¥", "æ™ºæ…§"]
                â†‘
            ç‰¹æ®Š token

<strong>2. ä½œç”¨ï¼š</strong>
[CLS] token çš„ embedding ç¶“é Transformer è™•ç†å¾Œï¼Œ
æœƒåŒ…å«æ•´å€‹å¥å­çš„èªç¾©è³‡è¨Šï¼

<strong>è™•ç†éç¨‹ï¼š</strong>
è¼¸å…¥: ["[CLS]", "æˆ‘", "æ„›", "äººå·¥", "æ™ºæ…§"]
  â†“ Embedding
Embeddings: [
  [CLS_embedding],    â† ç‰¹æ®Š embedding
  [æˆ‘_embedding],
  [æ„›_embedding],
  [äººå·¥_embedding],
  [æ™ºæ…§_embedding]
]
  â†“ Transformer è™•ç†ï¼ˆSelf-Attentionï¼‰
  â†“ [CLS] token å¯ä»¥ã€Œçœ‹åˆ°ã€æ‰€æœ‰å…¶ä»– tokens
è¼¸å‡º: [
  [CLS_output],       â† åŒ…å«æ•´å€‹å¥å­çš„èªç¾©ï¼
  [æˆ‘_output],
  [æ„›_output],
  [äººå·¥_output],
  [æ™ºæ…§_output]
]

âœ¨ é—œéµï¼šä½¿ç”¨ [CLS_output] å°±å¯ä»¥ä»£è¡¨æ•´å€‹å¥å­ï¼
                </pre>
              </div>

              <div class="visual-diagram lab-spacing-top">
                <h4>ğŸ“Š ç‚ºä»€éº¼éœ€è¦ [CLS] Tokenï¼Ÿ</h4>
                <pre>
<strong>å•é¡Œï¼š</strong>
å¦‚ä½•ç”¨ä¸€å€‹å‘é‡è¡¨ç¤ºæ•´å€‹å¥å­æˆ–è©å½™ï¼Ÿ

<strong>è§£æ±ºæ–¹æ¡ˆï¼š</strong>
1. å¹³å‡æ± åŒ–ï¼šè¨ˆç®—æ‰€æœ‰ token embeddings çš„å¹³å‡
   â†’ ç°¡å–®ä½†å¯èƒ½ç¨€é‡‹é‡è¦è³‡è¨Š

2. ç¬¬ä¸€å€‹ tokenï¼šä½¿ç”¨ç¬¬ä¸€å€‹ token çš„è¡¨ç¤º
   â†’ å¯èƒ½ä¸åŒ…å«å®Œæ•´èªç¾©

3. æœ€å¾Œä¸€å€‹ tokenï¼šä½¿ç”¨æœ€å¾Œä¸€å€‹ token çš„è¡¨ç¤º
   â†’ å¯èƒ½ä¸åŒ…å«å®Œæ•´èªç¾©

4. [CLS] tokenï¼šå°ˆé–€è¨­è¨ˆä¾†ä»£è¡¨æ•´å€‹å¥å­ï¼
   â†’ ç¶“éè¨“ç·´ï¼Œå°ˆé–€å­¸ç¿’ã€Œç¸½çµã€æ•´å€‹å¥å­çš„èªç¾©
   â†’ æœ€æº–ç¢ºï¼

<strong>å¯¦éš›æ‡‰ç”¨ï¼š</strong>
- æ–‡æœ¬åˆ†é¡ï¼šä½¿ç”¨ [CLS] token çš„è¡¨ç¤ºä¾†åˆ†é¡
- å¥å­ç›¸ä¼¼åº¦ï¼šæ¯”è¼ƒå…©å€‹å¥å­çš„ [CLS] token è¡¨ç¤º
- å•ç­”ç³»çµ±ï¼šä½¿ç”¨ [CLS] token ä¾†ç†è§£å•é¡Œ
- æƒ…æ„Ÿåˆ†æï¼šä½¿ç”¨ [CLS] token ä¾†åˆ¤æ–·æƒ…æ„Ÿ
                </pre>
              </div>

              <div class="visual-diagram lab-spacing-top">
                <h4>ğŸ“Š å“ªäº›æ¨¡å‹ä½¿ç”¨ [CLS] Tokenï¼Ÿ</h4>
                <pre>
<strong>ä½¿ç”¨ [CLS] Token çš„æ¨¡å‹ï¼š</strong>
âœ… BERT (Bidirectional Encoder Representations from Transformers)
âœ… RoBERTa (Robustly Optimized BERT)
âœ… ALBERT (A Lite BERT)
âœ… DistilBERT (Distilled BERT)
âœ… å¤§å¤šæ•¸åŸºæ–¼ BERT çš„æ¨¡å‹

<strong>ä¸ä½¿ç”¨ [CLS] Token çš„æ¨¡å‹ï¼š</strong>
âŒ GPT ç³»åˆ—ï¼ˆGPT-2, GPT-3, GPT-4ï¼‰
   â†’ ä½¿ç”¨æœ€å¾Œä¸€å€‹ token çš„è¡¨ç¤º
   
âŒ T5 (Text-to-Text Transfer Transformer)
   â†’ ä½¿ç”¨ç‰¹æ®Šçš„ decoder token

âŒ Qwenã€ChatGLM ç­‰ä¸­æ–‡æ¨¡å‹
   â†’ é€šå¸¸ä¸ä½¿ç”¨ [CLS]ï¼Œä½¿ç”¨å…¶ä»–æ–¹æ³•

<strong>ç‚ºä»€éº¼ GPT ä¸ä½¿ç”¨ [CLS]ï¼Ÿ</strong>
- GPT æ˜¯å–®å‘ï¼ˆå¾å·¦åˆ°å³ï¼‰çš„æ¨¡å‹
- [CLS] éœ€è¦é›™å‘æ³¨æ„åŠ›æ‰èƒ½æœ‰æ•ˆå·¥ä½œ
- GPT ä½¿ç”¨æœ€å¾Œä¸€å€‹ token çš„è¡¨ç¤ºä¾†ä»£è¡¨æ•´å€‹åºåˆ—
                </pre>
              </div>

              <div class="visual-diagram lab-spacing-top">
                <h4>ğŸ“Š å¯¦éš›ä¾‹å­ï¼šä½¿ç”¨ [CLS] Token</h4>
                <pre>
<strong>ç¯„ä¾‹ 1ï¼šæ–‡æœ¬åˆ†é¡</strong>
å¥å­: "é€™éƒ¨é›»å½±å¾ˆå¥½çœ‹"
  â†“ Tokenization
Tokens: ["[CLS]", "é€™", "éƒ¨", "é›»", "å½±", "å¾ˆ", "å¥½", "çœ‹"]
  â†“ BERT è™•ç†
  â†“ å–å¾— [CLS] token çš„è¼¸å‡º
[CLS] è¡¨ç¤º: [0.2, -0.5, 0.8, ...]  (åŒ…å«æ•´å€‹å¥å­çš„èªç¾©)
  â†“ åˆ†é¡å±¤
æƒ…æ„Ÿ: æ­£é¢ (0.95)

<strong>ç¯„ä¾‹ 2ï¼šå¥å­ç›¸ä¼¼åº¦</strong>
å¥å­ 1: "æˆ‘æ„›äººå·¥æ™ºæ…§"
å¥å­ 2: "æˆ‘å–œæ­¡ AI"
  â†“ åˆ†åˆ¥è™•ç†
[CLS]_1: [0.3, -0.4, 0.7, ...]  (å¥å­ 1 çš„è¡¨ç¤º)
[CLS]_2: [0.28, -0.38, 0.68, ...]  (å¥å­ 2 çš„è¡¨ç¤º)
  â†“ è¨ˆç®—ç›¸ä¼¼åº¦
ç›¸ä¼¼åº¦ = cosine_similarity([CLS]_1, [CLS]_2) = 0.92
â†’ å…©å€‹å¥å­æ„æ€å¾ˆç›¸ä¼¼ï¼

<strong>ç¯„ä¾‹ 3ï¼šå¤š Token è©å½™çš„è¡¨ç¤º</strong>
è©å½™: "äººå·¥æ™ºæ…§"
  â†“ Tokenization (BERT)
Tokens: ["[CLS]", "äººå·¥", "æ™ºæ…§"]
  â†“ BERT è™•ç†
  â†“ ä½¿ç”¨ [CLS] token çš„è¡¨ç¤º
è©å½™è¡¨ç¤º = [CLS] çš„è¼¸å‡º
â†’ é€™æ¯”å¹³å‡æ± åŒ–æ›´æº–ç¢ºï¼
                </pre>
              </div>

              <div class="lab-code-block collapsed">
                <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
                  <span class="filename">use_cls_token.py</span>
                  <div class="header-buttons">
                    <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                    <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
                  </div>
                </div>
                <div class="lab-code-block-content">
                  <pre><code class="language-python">from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# è¼‰å…¥ BERT æ¨¡å‹ï¼ˆä½¿ç”¨ [CLS] tokenï¼‰
print("è¼‰å…¥ BERT æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
model = AutoModel.from_pretrained("bert-base-chinese")

# æ¸¬è©¦å¥å­
sentences = [
    "æˆ‘æ„›äººå·¥æ™ºæ…§",
    "æˆ‘å–œæ­¡ AI",
    "è²“å’ªå¾ˆå¯æ„›",
    "å°ç‹—å¾ˆå¯æ„›"
]

print("\n" + "="*70)
print("ä½¿ç”¨ [CLS] Token å–å¾—å¥å­è¡¨ç¤º")
print("="*70)

# å–å¾—æ¯å€‹å¥å­çš„ [CLS] token è¡¨ç¤º
cls_embeddings = {}

for sentence in sentences:
    # Tokenizationï¼ˆBERT æœƒè‡ªå‹•åŠ ä¸Š [CLS] tokenï¼‰
    inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True)
    
    # æŸ¥çœ‹ tokenization çµæœ
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    print(f"\nå¥å­: '{sentence}'")
    print(f"Tokens: {tokens[:5]}...")  # é¡¯ç¤ºå‰ 5 å€‹ tokens
    
    # å–å¾—æ¨¡å‹è¼¸å‡º
    with torch.no_grad():
        outputs = model(**inputs)
        # [CLS] token ç¸½æ˜¯åœ¨ä½ç½® 0
        cls_embedding = outputs.last_hidden_state[0][0].numpy()  # [CLS] token çš„è¡¨ç¤º
        cls_embeddings[sentence] = cls_embedding
        print(f"[CLS] embedding å½¢ç‹€: {cls_embedding.shape}")

# è¨ˆç®—å¥å­ç›¸ä¼¼åº¦
print("\n" + "="*70)
print("å¥å­ç›¸ä¼¼åº¦çŸ©é™£ï¼ˆä½¿ç”¨ [CLS] Tokenï¼‰")
print("="*70)

print(f"{'å¥å­':<20}", end="")
for s in sentences:
    print(f"{s[:10]:>12}", end="")
print()

for s1 in sentences:
    print(f"{s1:<20}", end="")
    for s2 in sentences:
        sim = cosine_similarity(
            [cls_embeddings[s1]], 
            [cls_embeddings[s2]]
        )[0][0]
        print(f"{sim:>12.4f}", end="")
    print()

# æ¯”è¼ƒ [CLS] å’Œå¹³å‡æ± åŒ–çš„å·®ç•°
print("\n" + "="*70)
print("æ¯”è¼ƒ [CLS] Token å’Œå¹³å‡æ± åŒ–")
print("="*70)

test_sentence = "æˆ‘æ„›äººå·¥æ™ºæ…§"
inputs = tokenizer(test_sentence, return_tensors="pt", padding=True, truncation=True)

with torch.no_grad():
    outputs = model(**inputs)
    # æ–¹æ³• 1: ä½¿ç”¨ [CLS] token
    cls_embedding = outputs.last_hidden_state[0][0].numpy()
    
    # æ–¹æ³• 2: å¹³å‡æ± åŒ–ï¼ˆæ’é™¤ [CLS] tokenï¼‰
    all_embeddings = outputs.last_hidden_state[0][1:]  # æ’é™¤ [CLS]
    avg_embedding = all_embeddings.mean(dim=0).numpy()

print(f"\nå¥å­: '{test_sentence}'")
print(f"[CLS] embedding å½¢ç‹€: {cls_embedding.shape}")
print(f"å¹³å‡æ± åŒ– embedding å½¢ç‹€: {avg_embedding.shape}")

# è¨ˆç®—å…©ç¨®æ–¹æ³•çš„å·®ç•°
diff = np.linalg.norm(cls_embedding - avg_embedding)
print(f"\nå…©ç¨®æ–¹æ³•çš„å·®ç•°ï¼ˆL2 è·é›¢ï¼‰: {diff:.4f}")
print("â†’ [CLS] token å’Œå¹³å‡æ± åŒ–æœƒç”¢ç”Ÿä¸åŒçš„è¡¨ç¤ºï¼")
print("â†’ [CLS] token é€šå¸¸æ›´æº–ç¢ºï¼Œå› ç‚ºå®ƒç¶“éå°ˆé–€è¨“ç·´")

print("\n" + "="*70)
print("ğŸ“Š ç¸½çµ")
print("="*70)
print("1. [CLS] token æ˜¯ BERT ç­‰æ¨¡å‹ä¸­çš„ç‰¹æ®Š token")
print("2. å®ƒæ”¾åœ¨å¥å­æœ€å‰é¢ï¼Œç”¨ä¾†ä»£è¡¨æ•´å€‹å¥å­çš„èªç¾©")
print("3. ä½¿ç”¨ [CLS] token çš„è¡¨ç¤ºæ¯”å¹³å‡æ± åŒ–æ›´æº–ç¢º")
print("4. é©ç”¨æ–¼æ–‡æœ¬åˆ†é¡ã€å¥å­ç›¸ä¼¼åº¦ç­‰ä»»å‹™")</code></pre>
                </div>
              </div>

              <div class="explanation lab-spacing-top">
                <h4>ğŸ’¡ [CLS] Token vs å…¶ä»–æ–¹æ³•</h4>
                <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                  <thead>
                    <tr style="background-color: #f0f0f0;">
                      <th style="border: 1px solid #ddd; padding: 8px;">æ–¹æ³•</th>
                      <th style="border: 1px solid #ddd; padding: 8px;">å„ªé»</th>
                      <th style="border: 1px solid #ddd; padding: 8px;">ç¼ºé»</th>
                      <th style="border: 1px solid #ddd; padding: 8px;">é©ç”¨æ¨¡å‹</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td style="border: 1px solid #ddd; padding: 8px;"><strong>[CLS] Token</strong></td>
                      <td style="border: 1px solid #ddd; padding: 8px;">å°ˆé–€è¨“ç·´ä¾†ä»£è¡¨æ•´å€‹å¥å­ï¼Œæœ€æº–ç¢º</td>
                      <td style="border: 1px solid #ddd; padding: 8px;">åªåœ¨ BERT ç­‰é›™å‘æ¨¡å‹ä¸­ä½¿ç”¨</td>
                      <td style="border: 1px solid #ddd; padding: 8px;">BERT, RoBERTa, ALBERT</td>
                    </tr>
                    <tr>
                      <td style="border: 1px solid #ddd; padding: 8px;"><strong>å¹³å‡æ± åŒ–</strong></td>
                      <td style="border: 1px solid #ddd; padding: 8px;">ç°¡å–®ï¼Œé©ç”¨æ–¼æ‰€æœ‰æ¨¡å‹</td>
                      <td style="border: 1px solid #ddd; padding: 8px;">å¯èƒ½ç¨€é‡‹é‡è¦è³‡è¨Š</td>
                      <td style="border: 1px solid #ddd; padding: 8px;">æ‰€æœ‰æ¨¡å‹</td>
                    </tr>
                    <tr>
                      <td style="border: 1px solid #ddd; padding: 8px;"><strong>æœ€å¾Œä¸€å€‹ Token</strong></td>
                      <td style="border: 1px solid #ddd; padding: 8px;">ç°¡å–®å¿«é€Ÿ</td>
                      <td style="border: 1px solid #ddd; padding: 8px;">å¯èƒ½ä¸åŒ…å«å®Œæ•´èªç¾©</td>
                      <td style="border: 1px solid #ddd; padding: 8px;">GPT ç³»åˆ—</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>

            <div class="explanation lab-spacing-top">
              <h4>ğŸ” å¦‚ä½•æ¯”è¼ƒå¤š Token è©å½™çš„ç›¸ä¼¼åº¦ï¼Ÿ</h4>
              <p>
                ç•¶æˆ‘å€‘è¦æ¯”è¼ƒå¤š token è©å½™ï¼ˆå¦‚ "è²“å’ª"ã€"å°ç‹—"ã€"å¤§ç‹—"ã€"ç‹¼çŠ¬"ï¼‰çš„ç›¸ä¼¼åº¦æ™‚ï¼Œ
                éœ€è¦å…ˆå°‡å¤šå€‹ token embeddings è½‰æ›æˆå–®ä¸€è¡¨ç¤ºï¼Œç„¶å¾Œå†æ¯”è¼ƒã€‚
              </p>
              
              <div class="visual-diagram">
                <h4>ğŸ“Š æ–¹æ³• 1ï¼šå¹³å‡æ± åŒ–ï¼ˆAverage Poolingï¼‰</h4>
                <pre>
<strong>åŸç†ï¼š</strong>å°‡æ‰€æœ‰ token embeddings å¹³å‡ï¼Œå¾—åˆ°è©çš„è¡¨ç¤º

<strong>ç¯„ä¾‹ï¼š</strong>
"è²“å’ª" â†’ ["è²“", "å’ª"] â†’ 2 å€‹ vectors
  - vector_è²“: [0.2, -0.5, 0.8, ...]
  - vector_å’ª: [0.1, -0.3, 0.6, ...]
  â†“ å¹³å‡æ± åŒ–
è©è¡¨ç¤º: [(0.2+0.1)/2, (-0.5-0.3)/2, (0.8+0.6)/2, ...]
      = [0.15, -0.4, 0.7, ...]

"å°ç‹—" â†’ ["å°", "ç‹—"] â†’ 2 å€‹ vectors
  - vector_å°: [0.3, -0.4, 0.7, ...]
  - vector_ç‹—: [0.4, -0.3, 0.8, ...]
  â†“ å¹³å‡æ± åŒ–
è©è¡¨ç¤º: [(0.3+0.4)/2, (-0.4-0.3)/2, (0.7+0.8)/2, ...]
      = [0.35, -0.35, 0.75, ...]

<strong>æ¯”è¼ƒç›¸ä¼¼åº¦ï¼š</strong>
ç›¸ä¼¼åº¦ = cosine_similarity(è©è¡¨ç¤º_è²“å’ª, è©è¡¨ç¤º_å°ç‹—)
       = cosine_similarity([0.15, -0.4, 0.7, ...], [0.35, -0.35, 0.75, ...])
       = 0.85 (é«˜ç›¸ä¼¼åº¦ï¼å› ç‚ºéƒ½æ˜¯å‹•ç‰©)

<strong>å„ªé»ï¼š</strong>
- ç°¡å–®ç›´è§€
- è€ƒæ…®æ‰€æœ‰ token çš„è³‡è¨Š
- é©åˆå¤§å¤šæ•¸æƒ…æ³

<strong>ç¼ºé»ï¼š</strong>
- å¯èƒ½ç¨€é‡‹é‡è¦ token çš„è³‡è¨Š
- å° token é †åºä¸æ•æ„Ÿ
                </pre>
              </div>

              <div class="visual-diagram lab-spacing-top">
                <h4>ğŸ“Š æ–¹æ³• 2ï¼šä½¿ç”¨ Transformer è¼¸å‡ºå¾Œçš„è¡¨ç¤º</h4>
                <pre>
<strong>åŸç†ï¼š</strong>ä½¿ç”¨ Transformer è™•ç†å¾Œçš„ token è¡¨ç¤ºï¼ˆå·²åŒ…å«ä¸Šä¸‹æ–‡ï¼‰

<strong>ç¯„ä¾‹ï¼š</strong>
"è²“å’ª" â†’ ["è²“", "å’ª"] â†’ 2 å€‹ token embeddings
  â†“ Transformer è™•ç†ï¼ˆSelf-Attentionï¼‰
  â†“ è¼¸å‡ºè¡¨ç¤ºï¼ˆå·²è€ƒæ…®ä¸Šä¸‹æ–‡ï¼‰
  - output_è²“: [0.25, -0.45, 0.75, ...]  (å·²è€ƒæ…® "å’ª" çš„ä¸Šä¸‹æ–‡)
  - output_å’ª: [0.12, -0.28, 0.65, ...]  (å·²è€ƒæ…® "è²“" çš„ä¸Šä¸‹æ–‡)
  â†“ é¸æ“‡è¡¨ç¤ºæ–¹æ³•
  æ–¹æ³• A: ä½¿ç”¨æœ€å¾Œä¸€å€‹ token â†’ output_å’ª
  æ–¹æ³• B: å¹³å‡æ‰€æœ‰ tokens â†’ (output_è²“ + output_å’ª) / 2

<strong>æ¯”è¼ƒç›¸ä¼¼åº¦ï¼š</strong>
ä½¿ç”¨æœ€å¾Œä¸€å€‹ token çš„è¡¨ç¤ºï¼š
ç›¸ä¼¼åº¦ = cosine_similarity(output_å’ª, output_ç‹—)
       = 0.88 (æ¯”å¹³å‡æ± åŒ–æ›´é«˜ï¼å› ç‚ºè€ƒæ…®äº†ä¸Šä¸‹æ–‡)

<strong>å„ªé»ï¼š</strong>
- è€ƒæ…®äº†ä¸Šä¸‹æ–‡è³‡è¨Š
- æ›´æº–ç¢ºçš„èªç¾©è¡¨ç¤º
- é€™æ˜¯ Transformer æ¨¡å‹çš„æ¨™æº–åšæ³•

<strong>ç¼ºé»ï¼š</strong>
- éœ€è¦å®Œæ•´çš„æ¨¡å‹å‰å‘å‚³æ’­
- è¨ˆç®—æˆæœ¬è¼ƒé«˜
                </pre>
              </div>

              <div class="visual-diagram lab-spacing-top">
                <h4>ğŸ“Š æ–¹æ³• 3ï¼šä½¿ç”¨ç¬¬ä¸€å€‹æˆ–æœ€å¾Œä¸€å€‹ Token</h4>
                <pre>
<strong>åŸç†ï¼š</strong>åªä½¿ç”¨ç¬¬ä¸€å€‹æˆ–æœ€å¾Œä¸€å€‹ token çš„ embedding

<strong>ç¯„ä¾‹ï¼š</strong>
"è²“å’ª" â†’ ["è²“", "å’ª"]
  ä½¿ç”¨ç¬¬ä¸€å€‹ token: "è²“" çš„ embedding
  æˆ–ä½¿ç”¨æœ€å¾Œä¸€å€‹ token: "å’ª" çš„ embedding

<strong>æ¯”è¼ƒç›¸ä¼¼åº¦ï¼š</strong>
ä½¿ç”¨æœ€å¾Œä¸€å€‹ tokenï¼š
ç›¸ä¼¼åº¦ = cosine_similarity(embedding_å’ª, embedding_ç‹—)
       = 0.72 (ä¸­ç­‰ç›¸ä¼¼åº¦)

<strong>å„ªé»ï¼š</strong>
- ç°¡å–®å¿«é€Ÿ
- ä¸éœ€è¦é¡å¤–è¨ˆç®—

<strong>ç¼ºé»ï¼š</strong>
- å¯èƒ½ä¸Ÿå¤±é‡è¦è³‡è¨Š
- å° token é †åºæ•æ„Ÿ
- ä¸é©åˆæ‰€æœ‰æƒ…æ³
                </pre>
              </div>

              <div class="visual-diagram lab-spacing-top">
                <h4>ğŸ“Š å¯¦éš›ä¾‹å­ï¼šæ¯”è¼ƒ "è²“å’ª"ã€"å°ç‹—"ã€"å¤§ç‹—"ã€"ç‹¼çŠ¬"</h4>
                <pre>
<strong>å‡è¨­ Tokenization çµæœï¼š</strong>
"è²“å’ª" â†’ ["è²“", "å’ª"] (2 tokens)
"å°ç‹—" â†’ ["å°", "ç‹—"] (2 tokens)
"å¤§ç‹—" â†’ ["å¤§", "ç‹—"] (2 tokens)
"ç‹¼çŠ¬" â†’ ["ç‹¼", "çŠ¬"] (2 tokens)

<strong>æ–¹æ³• 1ï¼šå¹³å‡æ± åŒ–</strong>
1. å–å¾—æ¯å€‹è©çš„æ‰€æœ‰ token embeddings
2. è¨ˆç®—æ¯å€‹è©çš„å¹³å‡ embedding
3. æ¯”è¼ƒå¹³å‡ embeddings çš„ç›¸ä¼¼åº¦

çµæœï¼ˆå‡è¨­ï¼‰ï¼š
- "è²“å’ª" â†” "å°ç‹—": 0.65 (éƒ½æ˜¯å°å‹•ç‰©)
- "å°ç‹—" â†” "å¤§ç‹—": 0.85 (éƒ½æ˜¯ç‹—ï¼Œåªæ˜¯å¤§å°ä¸åŒ)
- "å¤§ç‹—" â†” "ç‹¼çŠ¬": 0.80 (éƒ½æ˜¯å¤§å‹çŠ¬é¡)
- "è²“å’ª" â†” "ç‹¼çŠ¬": 0.35 (å·®ç•°è¼ƒå¤§)

<strong>æ–¹æ³• 2ï¼šTransformer è¼¸å‡ºå¾Œå¹³å‡</strong>
1. å°‡æ¯å€‹è©è¼¸å…¥ Transformer
2. å–å¾—æ‰€æœ‰ token çš„è¼¸å‡ºè¡¨ç¤º
3. è¨ˆç®—å¹³å‡
4. æ¯”è¼ƒç›¸ä¼¼åº¦

çµæœï¼ˆå‡è¨­ï¼Œé€šå¸¸æ›´æº–ç¢ºï¼‰ï¼š
- "è²“å’ª" â†” "å°ç‹—": 0.70 (è€ƒæ…®ä¸Šä¸‹æ–‡å¾Œæ›´æº–ç¢º)
- "å°ç‹—" â†” "å¤§ç‹—": 0.88 (éƒ½æ˜¯ç‹—ï¼Œç›¸ä¼¼åº¦æ›´é«˜)
- "å¤§ç‹—" â†” "ç‹¼çŠ¬": 0.82 (éƒ½æ˜¯å¤§å‹çŠ¬é¡)
- "è²“å’ª" â†” "ç‹¼çŠ¬": 0.30 (å·®ç•°æ›´å¤§ï¼Œæ›´æº–ç¢º)

âœ¨ è§€å¯Ÿï¼š
- æ–¹æ³• 2 é€šå¸¸æ›´æº–ç¢ºï¼Œå› ç‚ºè€ƒæ…®äº†ä¸Šä¸‹æ–‡
- "å°ç‹—" å’Œ "å¤§ç‹—" ç›¸ä¼¼åº¦é«˜ï¼ˆéƒ½æ˜¯ç‹—ï¼‰
- "è²“å’ª" å’Œå…¶ä»–è©ç›¸ä¼¼åº¦è¼ƒä½ï¼ˆä¸åŒå‹•ç‰©ï¼‰
                </pre>
              </div>

              <div class="lab-code-block collapsed">
                <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
                  <span class="filename">compare_multi_token_words.py</span>
                  <div class="header-buttons">
                    <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                    <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
                  </div>
                </div>
                <div class="lab-code-block-content">
                  <pre><code class="language-python">from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def get_word_embedding_average(text, tokenizer, model):
    """æ–¹æ³• 1ï¼šå¹³å‡æ± åŒ– - å–å¾—è©çš„å¹³å‡ embedding"""
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    
    with torch.no_grad():
        outputs = model(**inputs)
        # å–å¾—æ‰€æœ‰ token çš„ embeddings
        embeddings = outputs.last_hidden_state[0]  # [seq_len, hidden_size]
        # å¹³å‡æ± åŒ–
        avg_embedding = embeddings.mean(dim=0).numpy()
    
    return avg_embedding

def get_word_embedding_transformer(text, tokenizer, model):
    """æ–¹æ³• 2ï¼šTransformer è¼¸å‡ºå¾Œå¹³å‡ - ä½¿ç”¨è™•ç†å¾Œçš„è¡¨ç¤º"""
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    
    with torch.no_grad():
        outputs = model(**inputs)
        # å–å¾— Transformer è¼¸å‡ºå¾Œçš„è¡¨ç¤ºï¼ˆå·²åŒ…å«ä¸Šä¸‹æ–‡ï¼‰
        output_embeddings = outputs.last_hidden_state[0]  # [seq_len, hidden_size]
        # å¹³å‡æ‰€æœ‰ token çš„è¼¸å‡ºè¡¨ç¤º
        avg_embedding = output_embeddings.mean(dim=0).numpy()
    
    return avg_embedding

def get_word_embedding_last_token(text, tokenizer, model):
    """æ–¹æ³• 3ï¼šä½¿ç”¨æœ€å¾Œä¸€å€‹ token"""
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    
    with torch.no_grad():
        outputs = model(**inputs)
        # å–å¾—æœ€å¾Œä¸€å€‹ token çš„è¼¸å‡ºè¡¨ç¤º
        last_token_embedding = outputs.last_hidden_state[0][-1].numpy()
    
    return last_token_embedding

def compare_words(words, tokenizer, model, method_name, method_func):
    """æ¯”è¼ƒå¤šå€‹è©å½™çš„ç›¸ä¼¼åº¦"""
    print(f"\n{'='*70}")
    print(f"æ–¹æ³•: {method_name}")
    print(f"{'='*70}")
    
    # å–å¾—æ¯å€‹è©çš„è¡¨ç¤º
    word_embeddings = {}
    for word in words:
        embedding = method_func(word, tokenizer, model)
        word_embeddings[word] = embedding
        print(f"  '{word}': embedding å½¢ç‹€ {embedding.shape}")
    
    # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
    print(f"\nç›¸ä¼¼åº¦çŸ©é™£:")
    print(f"{'è©å½™':<10}", end="")
    for word in words:
        print(f"{word:>12}", end="")
    print()
    
    similarity_matrix = []
    for word1 in words:
        print(f"{word1:<10}", end="")
        row = []
        for word2 in words:
            sim = cosine_similarity(
                [word_embeddings[word1]], 
                [word_embeddings[word2]]
            )[0][0]
            row.append(sim)
            print(f"{sim:>12.4f}", end="")
        similarity_matrix.append(row)
        print()
    
    return similarity_matrix

# æ¸¬è©¦è©å½™
test_words = ["è²“å’ª", "å°ç‹—", "å¤§ç‹—", "ç‹¼çŠ¬"]

print("=" * 70)
print("æ¯”è¼ƒå¤š Token è©å½™çš„ç›¸ä¼¼åº¦")
print("=" * 70)
print(f"\næ¸¬è©¦è©å½™: {test_words}")

# è¼‰å…¥æ¨¡å‹
try:
    print("\nè¼‰å…¥ Qwen æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B")
    model = AutoModel.from_pretrained("Qwen/Qwen2-0.5B")
    
    # é¡¯ç¤ºæ¯å€‹è©çš„ tokenization
    print("\nğŸ“Š Tokenization çµæœ:")
    for word in test_words:
        tokens = tokenizer.encode(word, add_special_tokens=False)
        token_texts = [tokenizer.decode([tid]) for tid in tokens]
        print(f"  '{word}': {len(tokens)} tokens â†’ {token_texts}")
    
    # æ–¹æ³• 1ï¼šå¹³å‡æ± åŒ–
    compare_words(
        test_words, 
        tokenizer, 
        model, 
        "æ–¹æ³• 1: å¹³å‡æ± åŒ– (Token Embeddings)",
        get_word_embedding_average
    )
    
    # æ–¹æ³• 2ï¼šTransformer è¼¸å‡ºå¾Œå¹³å‡
    compare_words(
        test_words, 
        tokenizer, 
        model, 
        "æ–¹æ³• 2: Transformer è¼¸å‡ºå¾Œå¹³å‡ (è€ƒæ…®ä¸Šä¸‹æ–‡)",
        get_word_embedding_transformer
    )
    
    # æ–¹æ³• 3ï¼šæœ€å¾Œä¸€å€‹ token
    compare_words(
        test_words, 
        tokenizer, 
        model, 
        "æ–¹æ³• 3: æœ€å¾Œä¸€å€‹ Token",
        get_word_embedding_last_token
    )
    
    print("\n" + "="*70)
    print("ğŸ“Š ç¸½çµ")
    print("="*70)
    print("1. æ–¹æ³• 2 (Transformer è¼¸å‡ºå¾Œå¹³å‡) é€šå¸¸æœ€æº–ç¢º")
    print("2. æ–¹æ³• 1 (å¹³å‡æ± åŒ–) ç°¡å–®ä½†å¯èƒ½ä¸å¤ æº–ç¢º")
    print("3. æ–¹æ³• 3 (æœ€å¾Œä¸€å€‹ token) å¿«é€Ÿä½†å¯èƒ½ä¸Ÿå¤±è³‡è¨Š")
    print("4. å»ºè­°ï¼šæ ¹æ“šä»»å‹™éœ€æ±‚é¸æ“‡åˆé©çš„æ–¹æ³•")
    
except Exception as e:
    print(f"éŒ¯èª¤: {e}")
    print("è«‹ç¢ºä¿å·²å®‰è£ transformers å’Œ torch")</code></pre>
                </div>
              </div>

              <div class="explanation lab-spacing-top">
                <h4>ğŸ’¡ å¯¦å‹™å»ºè­°</h4>
                <p>
                  <strong>ä½•æ™‚ä½¿ç”¨å“ªç¨®æ–¹æ³•ï¼Ÿ</strong>
                </p>
                <ul>
                  <li>
                    <strong>æ–¹æ³• 1ï¼ˆå¹³å‡æ± åŒ–ï¼‰</strong>ï¼š
                    <ul>
                      <li>é©åˆï¼šå¿«é€Ÿæ¯”è¼ƒã€ä¸éœ€è¦å®Œæ•´æ¨¡å‹æ™‚</li>
                      <li>ä¸é©åˆï¼šéœ€è¦ç²¾ç¢ºèªç¾©ç†è§£æ™‚</li>
                    </ul>
                  </li>
                  <li>
                    <strong>æ–¹æ³• 2ï¼ˆTransformer è¼¸å‡ºå¾Œå¹³å‡ï¼‰</strong>ï¼š
                    <ul>
                      <li>é©åˆï¼šéœ€è¦æº–ç¢ºèªç¾©æ¯”è¼ƒæ™‚ï¼ˆæ¨è–¦ï¼‰</li>
                      <li>ä¸é©åˆï¼šè¨ˆç®—è³‡æºæœ‰é™æ™‚</li>
                    </ul>
                  </li>
                  <li>
                    <strong>æ–¹æ³• 3ï¼ˆæœ€å¾Œä¸€å€‹ tokenï¼‰</strong>ï¼š
                    <ul>
                      <li>é©åˆï¼šå¿«é€Ÿæ¯”è¼ƒã€token é †åºæœ‰æ„ç¾©æ™‚</li>
                      <li>ä¸é©åˆï¼šéœ€è¦å®Œæ•´èªç¾©è³‡è¨Šæ™‚</li>
                    </ul>
                  </li>
                </ul>
                <p>
                  <strong>å¯¦éš›æ‡‰ç”¨ä¸­çš„æœ€ä½³å¯¦è¸ï¼š</strong>
                </p>
                <ul>
                  <li>å¤§å¤šæ•¸æƒ…æ³ä¸‹ï¼Œä½¿ç”¨<strong>æ–¹æ³• 2ï¼ˆTransformer è¼¸å‡ºå¾Œå¹³å‡ï¼‰</strong>æœ€æº–ç¢º</li>
                  <li>å¦‚æœæ¨¡å‹æœ‰ [CLS] tokenï¼Œå¯ä»¥ä½¿ç”¨ [CLS] token çš„è¡¨ç¤º</li>
                  <li>å°æ–¼ä¸­æ–‡å„ªåŒ–æ¨¡å‹ï¼ˆå¦‚ Qwenï¼‰ï¼Œé€šå¸¸èƒ½å°‡å®Œæ•´è©å½™ä½œç‚ºå–®ä¸€ tokenï¼Œæ¯”è¼ƒæ›´ç°¡å–®</li>
                </ul>
              </div>
            </div>

            <div class="ai-prompt-block lab-spacing-top">
              <p>
                <strong>å‹•æ‰‹è©¦è©¦ï¼šè§€å¯Ÿå¤š Token è©å½™çš„ Embedding</strong>
              </p>
              <pre>
è«‹å¹«æˆ‘å¯«ä¸€å€‹è…³æœ¬ï¼Œè§€å¯Ÿå¤š token è©å½™çš„ embeddingï¼š

1. ä½¿ç”¨ tiktoken å’Œ transformers åº«
2. æ¸¬è©¦ä»¥ä¸‹è©å½™ï¼š
   - "è²“" (å¯èƒ½æ˜¯ 1 å€‹ token)
   - "è²“å’ª" (å¯èƒ½æ˜¯ 2 å€‹ tokens)
   - "äººå·¥æ™ºæ…§" (å¯èƒ½æ˜¯ 3-4 å€‹ tokens)
3. å°æ–¼æ¯å€‹è©å½™ï¼š
   a. é¡¯ç¤º tokenization çµæœï¼ˆæœ‰å¤šå°‘ tokensï¼‰
   b. é¡¯ç¤ºæ¯å€‹ token çš„ ID
   c. å–å¾—æ¯å€‹ token çš„ embedding vector
   d. é¡¯ç¤ºæ¯å€‹ vector çš„ç¶­åº¦
   e. è¨ˆç®—æ‰€æœ‰ token vectors çš„å¹³å‡ï¼ˆæ¨¡æ“¬ã€Œè©çš„è¡¨ç¤ºã€ï¼‰
4. æ¯”è¼ƒä¸åŒæ¨¡å‹çš„çµæœï¼ˆGPT-3 vs Qwenï¼‰

è«‹å¹«æˆ‘å¯«ä¸€å€‹å®Œæ•´çš„è…³æœ¬ä¸¦åŸ·è¡Œã€‚
              </pre>
              <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
                ğŸ“‹ è¤‡è£½ Prompt
              </button>
            </div>

            <div class="lab-code-block collapsed">
              <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
                <span class="filename">observe_multi_token_embedding.py</span>
                <div class="header-buttons">
                  <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                  <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
                </div>
              </div>
              <div class="lab-code-block-content">
                <pre><code class="language-python">import tiktoken
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np

def analyze_token_embedding(text, tokenizer, model, model_name):
    """åˆ†æä¸€å€‹è©å½™çš„ tokenization å’Œ embedding"""
    print(f"\n{'='*70}")
    print(f"åˆ†æè©å½™: '{text}' (ä½¿ç”¨ {model_name})")
    print(f"{'='*70}")
    
    # 1. Tokenization
    tokens = tokenizer.encode(text, add_special_tokens=False)
    token_count = len(tokens)
    print(f"\nğŸ“Š Tokenization çµæœ:")
    print(f"  Token æ•¸é‡: {token_count}")
    print(f"  Token IDs: {tokens}")
    
    # å–å¾—æ¯å€‹ token çš„æ–‡å­—ï¼ˆå¦‚æœå¯ä»¥è§£ç¢¼ï¼‰
    try:
        token_texts = [tokenizer.decode([tid]) for tid in tokens]
        print(f"  Token æ–‡å­—: {token_texts}")
    except:
        print(f"  (ç„¡æ³•è§£ç¢¼ token æ–‡å­—)")
    
    # 2. Embedding
    print(f"\nğŸ”¢ Embedding çµæœ:")
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    
    with torch.no_grad():
        outputs = model(**inputs)
        # å–å¾—æ¯å€‹ token çš„ embedding
        embeddings = outputs.last_hidden_state[0]  # [seq_len, hidden_size]
    
    print(f"  Embedding å½¢ç‹€: {embeddings.shape}")  # [token_count, hidden_size]
    print(f"  æ¯å€‹ token çš„ vector ç¶­åº¦: {embeddings.shape[1]}")
    
    # é¡¯ç¤ºæ¯å€‹ token çš„ vectorï¼ˆå‰ 5 å€‹æ•¸å­—ï¼‰
    for i, (token_id, embedding) in enumerate(zip(tokens, embeddings)):
        vec_preview = embedding[:5].numpy()
        print(f"  Token {i+1} (ID: {token_id}): {vec_preview} ...")
    
    # 3. è¨ˆç®—å¹³å‡ï¼ˆæ¨¡æ“¬ã€Œè©çš„è¡¨ç¤ºã€ï¼‰
    avg_embedding = embeddings.mean(dim=0)
    print(f"\nğŸ“ˆ å¹³å‡ Embeddingï¼ˆæ¨¡æ“¬æ•´å€‹è©çš„è¡¨ç¤ºï¼‰:")
    print(f"  å½¢ç‹€: {avg_embedding.shape}")
    print(f"  å‰ 5 å€‹æ•¸å­—: {avg_embedding[:5].numpy()}")
    
    return tokens, embeddings, avg_embedding

# æ¸¬è©¦è©å½™
test_words = ["è²“", "è²“å’ª", "äººå·¥æ™ºæ…§"]

print("=" * 70)
print("è§€å¯Ÿå¤š Token è©å½™çš„ Embedding")
print("=" * 70)

# 1. GPT-2 (è¥¿æ–¹æ¨¡å‹ä»£è¡¨)
print("\n" + "="*70)
print("1. GPT-2 (è¥¿æ–¹æ¨¡å‹)")
print("="*70)
try:
    gpt2_tokenizer = AutoTokenizer.from_pretrained("gpt2")
    gpt2_model = AutoModel.from_pretrained("gpt2")
    
    for word in test_words:
        analyze_token_embedding(word, gpt2_tokenizer, gpt2_model, "GPT-2")
except Exception as e:
    print(f"ç„¡æ³•è¼‰å…¥ GPT-2: {e}")

# 2. Qwen (ä¸­æ–‡å„ªåŒ–æ¨¡å‹)
print("\n" + "="*70)
print("2. Qwen (ä¸­æ–‡å„ªåŒ–æ¨¡å‹)")
print("="*70)
try:
    qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B")
    qwen_model = AutoModel.from_pretrained("Qwen/Qwen2-0.5B")
    
    for word in test_words:
        analyze_token_embedding(word, qwen_tokenizer, qwen_model, "Qwen")
except Exception as e:
    print(f"ç„¡æ³•è¼‰å…¥ Qwen: {e}")

# 3. ä½¿ç”¨ tiktoken æŸ¥çœ‹ GPT-3.5/4 çš„ tokenization
print("\n" + "="*70)
print("3. GPT-3.5/4 Tokenization (ä½¿ç”¨ tiktoken)")
print("="*70)
try:
    enc = tiktoken.get_encoding("cl100k_base")
    for word in test_words:
        tokens = enc.encode(word)
        print(f"\nè©å½™: '{word}'")
        print(f"  Token æ•¸é‡: {len(tokens)}")
        print(f"  Token IDs: {tokens}")
        # è§£ç¢¼æ¯å€‹ token çœ‹çœ‹å…§å®¹
        print(f"  Token å…§å®¹:")
        for i, tid in enumerate(tokens):
            token_text = enc.decode([tid])
            print(f"    [{i}] ID {tid}: '{token_text}'")
except Exception as e:
    print(f"ç„¡æ³•ä½¿ç”¨ tiktoken: {e}")

print("\n" + "="*70)
print("ğŸ“Š ç¸½çµ")
print("="*70)
print("1. æ¯å€‹ token éƒ½æœ‰è‡ªå·±ç¨ç«‹çš„ embedding vector")
print("2. å¤šå€‹ tokens = å¤šå€‹ vectorsï¼ˆä¸æ˜¯åˆä½µæˆä¸€å€‹ï¼‰")
print("3. Transformer æœƒè™•ç†é€™äº› vectorsï¼Œè®“å®ƒå€‘äº’ç›¸å½±éŸ¿")
print("4. å¦‚æœéœ€è¦æ•´å€‹è©çš„è¡¨ç¤ºï¼Œå¯ä»¥è¨ˆç®—å¹³å‡æˆ–å…¶ä»–æ–¹æ³•")
print("5. ä¸­æ–‡å„ªåŒ–æ¨¡å‹é€šå¸¸èƒ½å°‡å®Œæ•´è©å½™ä½œç‚ºå–®ä¸€ tokenï¼Œæ›´é«˜æ•ˆ")</code></pre>
              </div>
            </div>
          </div>

          <h3>ğŸ¤” ç‚ºä»€éº¼éœ€è¦ Embeddingï¼Ÿ</h3>
          <p>
            AI æ¨¡å‹æœ¬è³ªä¸Šæ˜¯ã€Œæ•¸å­¸å‡½æ•¸ã€ï¼Œå®ƒå€‘åªèƒ½è¨ˆç®—æ•¸å­—ã€‚
            Embedding è®“æˆ‘å€‘å¯ä»¥ç”¨ã€Œæ•¸å­¸ã€ä¾†è™•ç†ã€Œèªç¾©ã€ï¼š
          </p>
          <ul class="lab-list-spacing">
            <li><strong>èªç¾©ç›¸ä¼¼æ€§</strong>ï¼šè¨ˆç®—å…©å€‹è©çš„ã€Œè·é›¢ã€å°±çŸ¥é“å®ƒå€‘æ˜¯å¦ç›¸ä¼¼</li>
            <li><strong>æ•¸å­¸é‹ç®—</strong>ï¼šå¯ä»¥åšã€Œåœ‹ç‹ - ç”·äºº + å¥³äºº = å¥³ç‹ã€é€™ç¨®ç¥å¥‡é‹ç®—</li>
            <li><strong>æ¨¡å‹è¼¸å…¥</strong>ï¼šTransformer éœ€è¦å‘é‡ä½œç‚ºè¼¸å…¥</li>
          </ul>

          <div class="ai-prompt-block">
            <p>
              æƒ³æ›´æ·±å…¥äº†è§£ï¼ŸæŠŠé€™æ®µ prompt çµ¦ Cursor / Claude Codeï¼š
            </p>
            <pre>
è«‹ç”¨ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼è§£é‡‹ï¼š
1. ä»€éº¼æ˜¯ Embeddingï¼Ÿç‚ºä»€éº¼ AI éœ€è¦å®ƒï¼Ÿ
2. Word2Vec çš„åŸºæœ¬åŸç†æ˜¯ä»€éº¼ï¼Ÿï¼ˆSkip-gram å’Œ CBOWï¼‰
3. Embedding å’Œå‚³çµ±çš„ one-hot encoding æœ‰ä»€éº¼ä¸åŒï¼Ÿ
4. ç‚ºä»€éº¼ç›¸ä¼¼çš„è©å½™æœƒæœ‰ç›¸ä¼¼çš„å‘é‡ï¼Ÿ
5. å¦‚ä½•ç”¨ Embedding åšã€Œåœ‹ç‹ - ç”·äºº + å¥³äºº = å¥³ç‹ã€é€™ç¨®é‹ç®—ï¼Ÿ

è«‹ç”¨ç”Ÿæ´»åŒ–çš„é¡æ¯”ï¼Œä¸¦æä¾›å…·é«”ä¾‹å­ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>
        </section>

        <!-- Step 2: ç’°å¢ƒè¨­å®š -->
        <section id="step2" class="lab-section">
          <h2>âš™ï¸ Step 2: ç’°å¢ƒè¨­å®š</h2>
          
          <p>
            æˆ‘å€‘æœƒç”¨ <code>gensim</code> åº«ä¾†å¯¦ä½œ Word2Vecï¼Œé€™æ˜¯ Python æœ€æµè¡Œçš„ Embedding åº«ã€‚
          </p>

          <div class="terminal-command">
            <code># å»ºç«‹å°ˆæ¡ˆç›®éŒ„ï¼ˆå¿…é ˆæ‰‹å‹•åŸ·è¡Œï¼‰
mkdir -p word2vec-lab && cd word2vec-lab</code>
            <button class="copy-terminal-btn" onclick="copyTerminal(this)">
              ğŸ“‹ è¤‡è£½æŒ‡ä»¤
            </button>
          </div>

          <div class="ai-prompt-block">
            <p>
              è®“ AI å¹«ä½ å»ºç«‹è™›æ“¬ç’°å¢ƒã€å°ˆæ¡ˆçµæ§‹å’Œæª”æ¡ˆï¼š
            </p>
            <pre>
è«‹å¹«æˆ‘å»ºç«‹ä¸€å€‹ Python å°ˆæ¡ˆï¼Œç”¨æ–¼å¯¦ä½œ Word2Vec Embeddingï¼š

âš ï¸ é‡è¦ï¼šè«‹ä½¿ç”¨ Python è™›æ“¬ç’°å¢ƒï¼ˆvenvï¼‰ä¾†ç®¡ç†å°ˆæ¡ˆä¾è³´ï¼

è«‹åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š
1. å»ºç«‹ Python è™›æ“¬ç’°å¢ƒï¼špython3 -m venv venv
2. å•Ÿå‹•è™›æ“¬ç’°å¢ƒï¼šsource venv/bin/activate (macOS/Linux) æˆ– venv\Scripts\activate (Windows)
3. ç¢ºèªè™›æ“¬ç’°å¢ƒå·²å•Ÿå‹•ï¼ˆçµ‚ç«¯æ©Ÿæç¤ºç¬¦æœƒé¡¯ç¤º (venv)ï¼‰
4. åœ¨è™›æ“¬ç’°å¢ƒä¸­å®‰è£ä¾è³´ï¼špip install gensim matplotlib numpy scikit-learn

ç„¶å¾Œå»ºç«‹ä»¥ä¸‹æª”æ¡ˆå’Œç›®éŒ„çµæ§‹ï¼š
- train_word2vec.py (è¨“ç·´ Word2Vec æ¨¡å‹ï¼Œå…ˆç•™ç©ºæˆ–åŠ ä¸ŠåŸºæœ¬è¨»è§£)
- visualize_embeddings.py (è¦–è¦ºåŒ– Embeddingï¼Œå…ˆç•™ç©º)
- requirements.txt (åˆ—å‡ºæ‰€æœ‰ä¾è³´ï¼šgensim, matplotlib, numpy, scikit-learn)
- README.md (å°ˆæ¡ˆç°¡ä»‹ï¼ŒåŒ…å«ï¼šå°ˆæ¡ˆç›®çš„ã€å¦‚ä½•å»ºç«‹è™›æ“¬ç’°å¢ƒã€å¦‚ä½•å®‰è£ä¾è³´ã€å¦‚ä½•åŸ·è¡Œ)
- .gitignore (å¿½ç•¥ venv/ å’Œ __pycache__/ ç›®éŒ„)

è«‹å»ºç«‹é€™äº›æª”æ¡ˆï¼Œä¸¦åœ¨ requirements.txt ä¸­æ­£ç¢ºåˆ—å‡ºæ‰€æœ‰ä¾è³´å¥—ä»¶å’Œç‰ˆæœ¬ã€‚
åœ¨ README.md ä¸­åŠ å…¥å®Œæ•´çš„å°ˆæ¡ˆèªªæ˜å’Œè™›æ“¬ç’°å¢ƒä½¿ç”¨æŒ‡å—ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-setup">
            <h4>ğŸ’¡ æç¤º</h4>
            <ul>
              <li>
                <strong>é‡è¦ï¼šè«‹ä½¿ç”¨è™›æ“¬ç’°å¢ƒï¼</strong>
                æ‰€æœ‰ä¾è³´å¥—ä»¶éƒ½æ‡‰è©²å®‰è£åœ¨è™›æ“¬ç’°å¢ƒä¸­ï¼Œé¿å…æ±¡æŸ“ç³»çµ± Pythonã€‚
              </li>
              <li>æˆ‘å€‘æœƒç”¨ä¸€å€‹ç°¡å–®çš„æ–‡æœ¬è³‡æ–™é›†ä¾†è¨“ç·´</li>
              <li>Word2Vec ä¸éœ€è¦ GPUï¼Œå¯ä»¥åœ¨ Mac æœ¬æ©Ÿé‹è¡Œ</li>
              <li>è¨“ç·´æ™‚é–“ç´„ 1-2 åˆ†é˜ï¼ˆå–æ±ºæ–¼è³‡æ–™å¤§å°ï¼‰</li>
              <li>è¨˜å¾—åœ¨ .gitignore ä¸­åŠ å…¥ <code>venv/</code> å’Œ <code>__pycache__/</code></li>
            </ul>
          </div>
        </section>

        <!-- Step 3: å¯¦ä½œ Word2Vec -->
        <section id="step3" class="lab-section">
          <h2>ğŸ› ï¸ Step 3: è¨“ç·´ Word2Vec æ¨¡å‹</h2>
          
          <p>
            Word2Vec æœ‰å…©ç¨®æ–¹æ³•ï¼š<strong>Skip-gram</strong> å’Œ <strong>CBOW</strong>ã€‚
            æˆ‘å€‘æœƒç”¨ Skip-gramï¼Œå®ƒæ›´å®¹æ˜“ç†è§£ã€‚
          </p>

          <div class="visual-diagram">
            <h4>ğŸ“Š Skip-gram åŸç†</h4>
            <pre>
ç›®æ¨™ï¼šå¾ä¸­å¿ƒè©é æ¸¬å‘¨åœçš„è©

ç¯„ä¾‹å¥å­: "the quick brown fox"

è¨“ç·´æ¨£æœ¬ï¼š
  ä¸­å¿ƒè©: "quick"
  å‘¨åœè©: ["the", "brown"]
  
  æ¨¡å‹å­¸ç¿’ï¼šçœ‹åˆ° "quick" æ™‚ï¼Œæ‡‰è©²é æ¸¬åˆ° "the" å’Œ "brown"

çµæœï¼šç›¸ä¼¼çš„è©ï¼ˆå¦‚ "quick" å’Œ "fast"ï¼‰æœƒæœ‰ç›¸ä¼¼çš„å‘é‡ï¼
            </pre>
          </div>

          <h3>ğŸ¯ å¯¦ä½œæ­¥é©Ÿ</h3>
          <p>æˆ‘å€‘è¦ï¼š</p>
          <ol class="lab-list-spacing">
            <li>æº–å‚™è¨“ç·´è³‡æ–™ï¼ˆæ–‡æœ¬èªæ–™ï¼‰</li>
            <li>ç”¨ gensim è¨“ç·´ Word2Vec æ¨¡å‹</li>
            <li>æŸ¥çœ‹è©å½™è¡¨å’Œå‘é‡</li>
            <li>æ¸¬è©¦èªç¾©ç›¸ä¼¼æ€§</li>
          </ol>

          <div class="ai-prompt-block">
            <p>
              <strong>ç¬¬ä¸€å€‹ Promptï¼šæº–å‚™è¨“ç·´è³‡æ–™</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘å¯«ä¸€å€‹ Python è…³æœ¬ï¼Œæº–å‚™ Word2Vec çš„è¨“ç·´è³‡æ–™ï¼š

è¦æ±‚ï¼š
1. å»ºç«‹ä¸€å€‹ç°¡å–®çš„æ–‡æœ¬èªæ–™ï¼ˆå¯ä»¥ç”¨å¹¾å€‹å¥å­ï¼Œä¾‹å¦‚é—œæ–¼å‹•ç‰©çš„ï¼‰
2. å°‡æ–‡æœ¬åˆ†å‰²æˆè©å½™åˆ—è¡¨ï¼ˆtokenizationï¼‰
3. æ¸…ç†è³‡æ–™ï¼ˆç§»é™¤æ¨™é»ã€è½‰å°å¯«ç­‰ï¼‰
4. è¿”å›ä¸€å€‹åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ æ˜¯ä¸€å€‹å¥å­çš„è©å½™åˆ—è¡¨

ç¯„ä¾‹è¼¸å‡ºï¼š
  [
    ["the", "quick", "brown", "fox"],
    ["the", "lazy", "dog"],
    ["a", "cat", "jumps", "over", "the", "dog"]
  ]

è«‹åŠ ä¸Šè©³ç´°è¨»è§£ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-code-block collapsed">
            <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
              <span class="filename">train_word2vec.py</span>
              <div class="header-buttons">
                <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
              </div>
            </div>
            <div class="lab-code-block-content">
              <pre><code class="language-python">import re
from gensim.models import Word2Vec

def prepare_corpus():
    """
    æº–å‚™ Word2Vec çš„è¨“ç·´è³‡æ–™
    """
    # ç°¡å–®çš„æ–‡æœ¬èªæ–™ï¼ˆé—œæ–¼å‹•ç‰©ï¼‰
    texts = [
        "the quick brown fox jumps over the lazy dog",
        "a cat sits on the mat",
        "dogs are loyal animals",
        "cats are independent pets",
        "the fox is clever and fast",
        "dogs love to play fetch",
        "cats enjoy sleeping in the sun",
        "the brown dog runs in the park",
        "a lazy cat naps all day",
        "foxes are wild animals"
    ]
    
    # æ¸…ç†å’Œåˆ†å‰²
    corpus = []
    for text in texts:
        # è½‰å°å¯«ã€ç§»é™¤æ¨™é»ã€åˆ†å‰²
        words = re.findall(r'\b\w+\b', text.lower())
        corpus.append(words)
    
    return corpus

# æº–å‚™è³‡æ–™
corpus = prepare_corpus()
print(f"èªæ–™åŒ…å« {len(corpus)} å€‹å¥å­")
print(f"ç¯„ä¾‹å¥å­: {corpus[0]}")</code></pre>
            </div>
          </div>

          <div class="ai-prompt-block lab-spacing-top">
            <p>
              <strong>ç¬¬äºŒå€‹ Promptï¼šè¨“ç·´ Word2Vec æ¨¡å‹</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘ä½¿ç”¨ gensim è¨“ç·´ä¸€å€‹ Word2Vec æ¨¡å‹ï¼š

è¦æ±‚ï¼š
1. ä½¿ç”¨å‰›æ‰æº–å‚™çš„èªæ–™
2. è¨­å®šåƒæ•¸ï¼š
   - vector_size: 100 (å‘é‡ç¶­åº¦)
   - window: 5 (ä¸Šä¸‹æ–‡çª—å£å¤§å°)
   - min_count: 1 (æœ€å°è©é »)
   - sg: 1 (ä½¿ç”¨ Skip-gram)
   - epochs: 100 (è¨“ç·´è¼ªæ•¸)
3. è¨“ç·´æ¨¡å‹
4. æ‰“å°è©å½™è¡¨å¤§å°
5. æ¸¬è©¦å¹¾å€‹è©çš„ç›¸ä¼¼æ€§ï¼ˆä¾‹å¦‚ "dog" å’Œ "cat"ï¼‰

è«‹åŠ ä¸Šè©³ç´°è¨»è§£ï¼Œä¸¦åœ¨è¨“ç·´æ™‚é¡¯ç¤ºé€²åº¦ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-code-block collapsed">
            <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
              <span class="filename">train_word2vec.py</span>
              <div class="header-buttons">
                <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
              </div>
            </div>
            <div class="lab-code-block-content">
              <pre><code class="language-python"># è¨“ç·´ Word2Vec æ¨¡å‹
print("\né–‹å§‹è¨“ç·´ Word2Vec æ¨¡å‹...")
model = Word2Vec(
    sentences=corpus,
    vector_size=100,      # å‘é‡ç¶­åº¦
    window=5,             # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    min_count=1,          # æœ€å°è©é »
    sg=1,                 # 1 = Skip-gram, 0 = CBOW
    epochs=100            # è¨“ç·´è¼ªæ•¸
)

print(f"\nè¨“ç·´å®Œæˆï¼")
print(f"è©å½™è¡¨å¤§å°: {len(model.wv.key_to_index)}")
print(f"å‘é‡ç¶­åº¦: {model.wv.vector_size}")

# æ¸¬è©¦ç›¸ä¼¼æ€§
print("\næ¸¬è©¦è©å½™ç›¸ä¼¼æ€§:")
if "dog" in model.wv and "cat" in model.wv:
    similarity = model.wv.similarity("dog", "cat")
    print(f"  'dog' å’Œ 'cat' çš„ç›¸ä¼¼åº¦: {similarity:.4f}")

# æ‰¾å‡ºæœ€ç›¸ä¼¼çš„è©
if "dog" in model.wv:
    similar_words = model.wv.most_similar("dog", topn=5)
    print(f"\nèˆ‡ 'dog' æœ€ç›¸ä¼¼çš„è©:")
    for word, score in similar_words:
        print(f"  {word}: {score:.4f}")

# ä¿å­˜æ¨¡å‹
model.save("word2vec.model")
print("\næ¨¡å‹å·²ä¿å­˜ç‚º 'word2vec.model'")</code></pre>
            </div>
          </div>

          <div class="ai-prompt-block lab-spacing-top">
            <p>
              <strong>åŸ·è¡Œè¨“ç·´</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘åŸ·è¡Œè¨“ç·´è…³æœ¬ä¸¦æª¢æŸ¥çµæœï¼š

1. åœ¨è™›æ“¬ç’°å¢ƒä¸­åŸ·è¡Œï¼špython train_word2vec.py
2. æª¢æŸ¥è¨“ç·´éç¨‹æ˜¯å¦æ­£å¸¸
3. ç¢ºèªæ¨¡å‹æ˜¯å¦æˆåŠŸä¿å­˜
4. å¦‚æœæœ‰éŒ¯èª¤ï¼Œè«‹å¹«æˆ‘ä¿®æ­£

è«‹åŸ·è¡Œè¨“ç·´ä¸¦å‘Šè¨´æˆ‘çµæœã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-expected-output">
            <pre>
èªæ–™åŒ…å« 10 å€‹å¥å­
ç¯„ä¾‹å¥å­: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']

é–‹å§‹è¨“ç·´ Word2Vec æ¨¡å‹...

è¨“ç·´å®Œæˆï¼
è©å½™è¡¨å¤§å°: 25
å‘é‡ç¶­åº¦: 100

æ¸¬è©¦è©å½™ç›¸ä¼¼æ€§:
  'dog' å’Œ 'cat' çš„ç›¸ä¼¼åº¦: 0.7234

èˆ‡ 'dog' æœ€ç›¸ä¼¼çš„è©:
  cat: 0.7234
  fox: 0.6891
  animals: 0.6543
  lazy: 0.6123
  brown: 0.5890

æ¨¡å‹å·²ä¿å­˜ç‚º 'word2vec.model'
            </pre>
          </div>
        </section>

        <!-- Step 4: è¦–è¦ºåŒ– -->
        <section id="step4" class="lab-section">
          <h2>ğŸ¨ Step 4: è¦–è¦ºåŒ– Embedding</h2>
          
          <p>
            æœ€æœ‰è¶£çš„éƒ¨åˆ†ä¾†äº†ï¼æˆ‘å€‘è¦æŠŠé«˜ç¶­å‘é‡ã€Œå£“ç¸®ã€åˆ° 2D å¹³é¢ï¼Œç”¨åœ–è¡¨ç•«å‡ºä¾†ã€‚
            é€™æ¨£ä½ å°±èƒ½ã€Œçœ‹åˆ°ã€è©å½™ä¹‹é–“çš„é—œä¿‚ï¼
          </p>

          <div class="visual-diagram">
            <h4>ğŸ“Š è¦–è¦ºåŒ–åŸç†</h4>
            <pre>
é«˜ç¶­å‘é‡ (100 ç¶­): [0.2, -0.5, 0.8, ...]
  â†“ t-SNE é™ç¶­
2D åº§æ¨™: (x, y)
  â†“ ç•«åœ¨åœ–è¡¨ä¸Š
è¦–è¦ºåŒ–åœ–è¡¨ï¼šå¯ä»¥çœ‹åˆ°å“ªäº›è©å½™èšé›†åœ¨ä¸€èµ·ï¼
            </pre>
          </div>

          <div class="ai-prompt-block">
            <p>
              è®“ AI å¹«ä½ å¯«è¦–è¦ºåŒ–è…³æœ¬ï¼š
            </p>
            <pre>
è«‹å¹«æˆ‘å¯«ä¸€å€‹ Python è…³æœ¬ï¼Œè¦–è¦ºåŒ– Word2Vec çš„ Embeddingï¼š

è¦æ±‚ï¼š
1. è¼‰å…¥è¨“ç·´å¥½çš„ Word2Vec æ¨¡å‹
2. é¸å–ä¸€äº›è©å½™ï¼ˆä¾‹å¦‚ï¼šdog, cat, fox, lazy, quick, brownï¼‰
3. å–å¾—é€™äº›è©å½™çš„å‘é‡
4. ä½¿ç”¨ t-SNE å°‡é«˜ç¶­å‘é‡é™ç¶­åˆ° 2D
5. ç”¨ matplotlib ç•«å‡ºæ•£é»åœ–
6. åœ¨æ¯å€‹é»æ—é‚Šæ¨™è¨»è©å½™åç¨±
7. ç”¨ä¸åŒé¡è‰²æ¨™ç¤ºä¸åŒé¡åˆ¥çš„è©ï¼ˆä¾‹å¦‚ï¼šå‹•ç‰© vs å½¢å®¹è©ï¼‰

è«‹åŠ ä¸Šè©³ç´°è¨»è§£ï¼Œä¸¦è®“åœ–è¡¨ç¾è§€æ˜“è®€ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="lab-code-block collapsed">
            <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
              <span class="filename">visualize_embeddings.py</span>
              <div class="header-buttons">
                <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
              </div>
            </div>
            <div class="lab-code-block-content">
              <pre><code class="language-python">from gensim.models import Word2Vec
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import numpy as np

# è¼‰å…¥æ¨¡å‹
model = Word2Vec.load("word2vec.model")

# é¸å–è¦è¦–è¦ºåŒ–çš„è©å½™
words = ["dog", "cat", "fox", "lazy", "quick", "brown", 
         "animals", "pets", "jumps", "runs", "sits"]

# éæ¿¾å­˜åœ¨çš„è©å½™
words = [w for w in words if w in model.wv]

# å–å¾—å‘é‡
vectors = [model.wv[word] for word in words]

# ä½¿ç”¨ t-SNE é™ç¶­åˆ° 2D
print("æ­£åœ¨é™ç¶­...")
tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(words)-1))
vectors_2d = tsne.fit_transform(vectors)

# ç¹ªåœ–
plt.figure(figsize=(12, 8))
plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], s=200, alpha=0.6)

# æ¨™è¨»è©å½™
for i, word in enumerate(words):
    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), 
                fontsize=12, fontweight='bold')

plt.title("Word2Vec Embedding è¦–è¦ºåŒ–", fontsize=16, fontweight='bold')
plt.xlabel("t-SNE ç¶­åº¦ 1")
plt.ylabel("t-SNE ç¶­åº¦ 2")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig("embedding_visualization.png", dpi=150, bbox_inches='tight')
print("åœ–è¡¨å·²ä¿å­˜ç‚º 'embedding_visualization.png'")
plt.show()</code></pre>
          </div>

          <div class="ai-prompt-block lab-spacing-top">
            <p>
              <strong>åŸ·è¡Œè¦–è¦ºåŒ–</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘åŸ·è¡Œè¦–è¦ºåŒ–è…³æœ¬ä¸¦æª¢æŸ¥çµæœï¼š

1. åœ¨è™›æ“¬ç’°å¢ƒä¸­åŸ·è¡Œï¼špython visualize_embeddings.py
2. æª¢æŸ¥æ˜¯å¦æˆåŠŸç”Ÿæˆåœ–è¡¨æª”æ¡ˆ
3. å¦‚æœæœ‰éŒ¯èª¤ï¼Œè«‹å¹«æˆ‘ä¿®æ­£

è«‹åŸ·è¡Œè¦–è¦ºåŒ–ä¸¦å‘Šè¨´æˆ‘çµæœã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>

          <div class="visual-diagram">
            <h4>ğŸ¨ é æœŸè¦–è¦ºåŒ–çµæœ</h4>
            <p>
              ä½ æ‡‰è©²æœƒçœ‹åˆ°é¡ä¼¼é€™æ¨£çš„åœ–è¡¨ï¼š
            </p>
            <pre>
        y
        â†‘
        |
    dog â—
        |
  cat â—     â— fox
        |
        |        â— animals
        |
        |  â— lazy  â— quick
        |    â— brown
        |
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x

âœ¨ è§€å¯Ÿï¼š
- å‹•ç‰©è©å½™ï¼ˆdog, cat, foxï¼‰æœƒèšé›†åœ¨ä¸€èµ·
- å½¢å®¹è©ï¼ˆlazy, quick, brownï¼‰æœƒèšé›†åœ¨ä¸€èµ·
- é€™è­‰æ˜äº† Embedding ç¢ºå¯¦æ•æ‰åˆ°äº†èªç¾©ï¼
            </pre>
          </div>

          <div class="ai-prompt-block lab-spacing-top">
            <p>
              <strong>é€²éšï¼šæ¸¬è©¦èªç¾©é‹ç®—</strong>
            </p>
            <pre>
è«‹å¹«æˆ‘æ¸¬è©¦ Word2Vec çš„èªç¾©é‹ç®—åŠŸèƒ½ï¼š

ä»»å‹™ï¼š
1. è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
2. æ¸¬è©¦ "dog" - "loyal" + "independent" æ˜¯å¦æ¥è¿‘ "cat"
3. æ¸¬è©¦ "fox" - "wild" + "pet" æ˜¯å¦æ¥è¿‘ "dog" æˆ– "cat"
4. æ‰“å°é‹ç®—çµæœå’Œç›¸ä¼¼åº¦åˆ†æ•¸

æç¤ºï¼šä½¿ç”¨ model.wv.most_similar() å’Œæ­£è² æ¨£æœ¬åˆ—è¡¨ã€‚
            </pre>
            <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
              ğŸ“‹ è¤‡è£½ Prompt
            </button>
          </div>
        </section>

        <!-- Advanced Learning: ä¸­æ–‡ Embedding çš„ç‰¹æ®Šæ€§ -->
        <section id="chinese-embedding" class="lab-section lab-spacing-top-lg">
          <h2>ğŸŒ é€²éšå­¸ç¿’ï¼šä¸­æ–‡ Embedding çš„ç‰¹æ®Šæ€§</h2>
          
          <div class="lab-intro-text">
            <p>
              æˆ‘å€‘å‰›æ‰å­¸çš„ Word2Vec æ˜¯åŸºæ–¼<strong>è‹±æ–‡</strong>è¨­è¨ˆçš„ã€‚ä½†ä¸­æ–‡æ˜¯å®Œå…¨ä¸åŒçš„èªè¨€ç³»çµ±ï¼
              è®“æˆ‘å€‘çœ‹çœ‹å¯¦éš›çš„èªè¨€æ¨¡å‹ï¼ˆå¦‚ Qwenã€ChatGLMï¼‰æ˜¯å¦‚ä½•è™•ç†ä¸­æ–‡ Embedding çš„ã€‚
            </p>
          </div>

          <!-- ä¸­æ–‡ Embedding çš„åŸºæœ¬åŸç† -->
          <div class="lab-section lab-spacing-top">
            <h3>ğŸ“ 1. ä¸­æ–‡ Embedding çš„åŸºæœ¬åŸç†</h3>
            
            <div class="visual-diagram">
              <h4>ğŸ” æ ¸å¿ƒå•é¡Œï¼šä¸­æ–‡ Embedding æ˜¯å¦èˆ‡è‹±æ–‡ç›¸åŒï¼Ÿ</h4>
              <p>
                <strong>ç­”æ¡ˆï¼šåŸºæœ¬åŸç†ç›¸åŒï¼Œä½†è¨“ç·´æ–¹å¼æœ‰å·®ç•°ï¼</strong>
              </p>
              <pre>
<strong>ç›¸åŒé»ï¼š</strong>
- éƒ½æ˜¯å°‡ tokens è½‰æ›æˆå‘é‡
- éƒ½ä½¿ç”¨ç¥ç¶“ç¶²è·¯å­¸ç¿’èªç¾©è¡¨ç¤º
- éƒ½éµå¾ªã€Œç›¸ä¼¼è©å½™æœ‰ç›¸ä¼¼å‘é‡ã€çš„åŸå‰‡

<strong>ä¸åŒé»ï¼š</strong>
- è¨“ç·´èªæ–™ï¼šä¸­æ–‡æ¨¡å‹ç”¨ä¸­æ–‡èªæ–™ï¼Œè‹±æ–‡æ¨¡å‹ç”¨è‹±æ–‡èªæ–™
- Tokenizationï¼šä¸­æ–‡éœ€è¦ç‰¹æ®Šçš„åˆ†è©è™•ç†
- èªç¾©ç†è§£ï¼šä¸­æ–‡çš„èªç¾©å–®ä½ï¼ˆå­— vs è©ï¼‰èˆ‡è‹±æ–‡ä¸åŒ
              </pre>
            </div>

            <div class="explanation">
              <h4>ğŸ“Š è¦–è¦ºåŒ–ï¼šä¸­è‹±æ–‡ Embedding çš„å·®ç•°</h4>
              <pre>
<strong>è‹±æ–‡ Embeddingï¼š</strong>
æ–‡å­—: "cat" â†’ Token: "cat" â†’ å‘é‡: [0.2, -0.5, 0.8, ...]
æ–‡å­—: "dog" â†’ Token: "dog" â†’ å‘é‡: [0.3, -0.4, 0.7, ...]

<strong>ä¸­æ–‡ Embeddingï¼ˆå­—ç´šï¼‰ï¼š</strong>
æ–‡å­—: "è²“" â†’ Token: "è²“" â†’ å‘é‡: [0.1, -0.6, 0.9, ...]
æ–‡å­—: "ç‹—" â†’ Token: "ç‹—" â†’ å‘é‡: [0.2, -0.5, 0.8, ...]

<strong>ä¸­æ–‡ Embeddingï¼ˆè©ç´šï¼‰ï¼š</strong>
æ–‡å­—: "è²“å’ª" â†’ Token: "è²“å’ª" â†’ å‘é‡: [0.15, -0.55, 0.85, ...]
æ–‡å­—: "å°ç‹—" â†’ Token: "å°ç‹—" â†’ å‘é‡: [0.25, -0.45, 0.75, ...]

âœ¨ é—œéµå·®ç•°ï¼š
- ä¸­æ–‡å¯ä»¥é¸æ“‡ã€Œå­—ç´šã€æˆ–ã€Œè©ç´šã€embedding
- è©ç´š embedding æ›´èƒ½æ•æ‰èªç¾©ï¼ˆ"è²“å’ª" æ¯” "è²“" æ›´å®Œæ•´ï¼‰
              </pre>
            </div>
          </div>

          <!-- Qwen ç­‰ä¸­æ–‡æ¨¡å‹çš„ç‰¹æ®Šè™•ç† -->
          <div class="lab-section lab-spacing-top">
            <h3>ğŸ¤– 2. Qwenã€ChatGLM ç­‰ä¸­æ–‡æ¨¡å‹çš„ç‰¹æ®Šè™•ç†</h3>
            
            <div class="visual-diagram">
              <h4>ğŸ¯ ç­–ç•¥ 1ï¼šä½¿ç”¨ä¸­æ–‡èªæ–™è¨“ç·´</h4>
              <pre>
<strong>ä»£è¡¨æ¨¡å‹ï¼š</strong>Qwenã€ChatGLMã€Baichuanã€InternLM

<strong>æ ¸å¿ƒç­–ç•¥ï¼š</strong>
1. æ”¶é›†å¤§é‡ä¸­æ–‡èªæ–™ï¼ˆæ•¸ç™¾ GB åˆ°æ•¸ TBï¼‰
   - ä¸­æ–‡ç¶­åŸºç™¾ç§‘
   - ä¸­æ–‡æ–°èæ–‡ç« 
   - ä¸­æ–‡æ›¸ç±ã€è«–æ–‡
   - ä¸­æ–‡å°è©±è³‡æ–™

2. ç”¨ä¸­æ–‡èªæ–™è¨“ç·´ Embedding
   - è©å½™è¡¨ä¸­åŒ…å«å¤§é‡ä¸­æ–‡è©å½™
   - å‘é‡ç©ºé–“åæ˜ ä¸­æ–‡èªç¾©é—œä¿‚

<strong>çµæœï¼š</strong>
- "è²“" å’Œ "ç‹—" çš„å‘é‡ç›¸ä¼¼åº¦é«˜ï¼ˆéƒ½æ˜¯å‹•ç‰©ï¼‰
- "åŒ—äº¬" å’Œ "ä¸Šæµ·" çš„å‘é‡ç›¸ä¼¼åº¦é«˜ï¼ˆéƒ½æ˜¯åŸå¸‚ï¼‰
- æ¯”ç”¨è‹±æ–‡èªæ–™è¨“ç·´çš„æ¨¡å‹æ›´ç†è§£ä¸­æ–‡èªç¾©
              </pre>
            </div>

            <div class="visual-diagram lab-spacing-top">
              <h4>ğŸ¯ ç­–ç•¥ 2ï¼šæ··åˆå­—ç´šå’Œè©ç´š Embedding</h4>
              <pre>
<strong>ä»£è¡¨æ¨¡å‹ï¼š</strong>ChatGLMã€GLM

<strong>æ ¸å¿ƒç­–ç•¥ï¼š</strong>
- å¸¸è¦‹è©å½™ï¼šä½¿ç”¨è©ç´š embeddingï¼ˆå¦‚ "äººå·¥æ™ºæ…§"ï¼‰
- ç½•è¦‹è©å½™ï¼šæ‹†æˆå­—ç´š embeddingï¼ˆå¦‚ "äººå·¥" + "æ™ºæ…§"ï¼‰
- å–®å­—ï¼šä½¿ç”¨å­—ç´š embeddingï¼ˆå¦‚ "æˆ‘"ã€"çš„"ï¼‰

<strong>ç¯„ä¾‹ï¼š</strong>
"æˆ‘æ„›äººå·¥æ™ºæ…§"
â†’ ["æˆ‘", "æ„›", "äººå·¥æ™ºæ…§"]  (è©ç´šï¼Œ3 tokens)
â†’ æˆ– ["æˆ‘", "æ„›", "äººå·¥", "æ™ºæ…§"]  (æ··åˆï¼Œ4 tokens)

<strong>å„ªé»ï¼š</strong>
- å¸¸è¦‹è©ä¿æŒèªç¾©å®Œæ•´æ€§
- ç½•è¦‹è©ä¹Ÿèƒ½è™•ç†ï¼ˆæ‹†æˆå­—ï¼‰
- å¹³è¡¡æ•ˆç‡å’Œè¦†è“‹ç‡
              </pre>
            </div>

            <div class="visual-diagram lab-spacing-top">
              <h4>ğŸ¯ ç­–ç•¥ 3ï¼šé‡å°ä¸­æ–‡èªç¾©çš„ç‰¹æ®Šå„ªåŒ–</h4>
              <pre>
<strong>ä»£è¡¨æ¨¡å‹ï¼š</strong>Qwenã€Baichuan

<strong>æ ¸å¿ƒç­–ç•¥ï¼š</strong>
1. ä½¿ç”¨ä¸­æ–‡åˆ†è©å·¥å…·é è™•ç†ï¼ˆå¦‚ jiebaï¼‰
2. é‡å°ä¸­æ–‡èªç¾©é—œä¿‚å„ªåŒ–ï¼ˆå¦‚åŒç¾©è©ã€åç¾©è©ï¼‰
3. è€ƒæ…®ä¸­æ–‡çš„èªæ³•çµæ§‹ï¼ˆå¦‚ä¸»è¬‚è³“ï¼‰

<strong>å¯¦éš›æ•ˆæœï¼š</strong>
- "è²“" å’Œ "è²“å’ª" çš„å‘é‡éå¸¸æ¥è¿‘ï¼ˆåŒç¾©è©ï¼‰
- "å¤§" å’Œ "å°" çš„å‘é‡åœ¨å‘é‡ç©ºé–“ä¸­ç›¸å°ï¼ˆåç¾©è©ï¼‰
- "åŒ—äº¬" å’Œ "ä¸­åœ‹" çš„å‘é‡æœ‰èªç¾©é—œè¯ï¼ˆåŒ…å«é—œä¿‚ï¼‰

<strong>æŠ€è¡“ç´°ç¯€ï¼š</strong>
- ä½¿ç”¨æ›´å¤§çš„è©å½™è¡¨ï¼ˆåŒ…å«æ›´å¤šä¸­æ–‡è©å½™ï¼‰
- ä½¿ç”¨æ›´é•·çš„ä¸Šä¸‹æ–‡çª—å£ï¼ˆæ•æ‰ä¸­æ–‡é•·å¥ï¼‰
- ä½¿ç”¨å¤šä»»å‹™å­¸ç¿’ï¼ˆåŒæ™‚å­¸ç¿’èªç¾©å’Œèªæ³•ï¼‰
              </pre>
            </div>

            <!-- è¥¿æ–¹æ¨¡å‹è™•ç†ä¸­æ–‡çš„æ–¹å¼ -->
            <div class="lab-section lab-spacing-top">
              <h3>ğŸŒ 3. è¥¿æ–¹æ¨¡å‹ï¼ˆGPTã€Mistralï¼‰å¦‚ä½•è™•ç†ä¸­æ–‡ï¼Ÿ</h3>
              
              <div class="visual-diagram">
                <h4>ğŸ“Š GPT ç³»åˆ—ï¼ˆOpenAIï¼‰çš„è™•ç†æ–¹å¼</h4>
                <pre>
<strong>ä»£è¡¨æ¨¡å‹ï¼š</strong>GPT-3ã€GPT-3.5ã€GPT-4

<strong>æ ¸å¿ƒç­–ç•¥ï¼š</strong>
1. <strong>å¤šèªè¨€èªæ–™è¨“ç·´</strong>
   - è¨“ç·´èªæ–™åŒ…å«å°‘é‡ä¸­æ–‡ï¼ˆç´„ 1-5%ï¼‰
   - ä¸»è¦é‚„æ˜¯è‹±æ–‡èªæ–™ï¼ˆ80%+ï¼‰
   - ä¸­æ–‡èªæ–™å“è³ªå’Œæ•¸é‡æœ‰é™

2. <strong>åŸºæ–¼è‹±æ–‡çš„ BPE Tokenization</strong>
   - ä½¿ç”¨è‹±æ–‡èªæ–™è¨“ç·´çš„ BPE tokenizer
   - ä¸­æ–‡è¢«æ‹†æˆ UTF-8 bytes æˆ–ç½•è¦‹å­è©
   - ä¾‹å¦‚ï¼š"äººå·¥æ™ºæ…§" â†’ 6-8 tokensï¼ˆGPT-3ï¼‰

3. <strong>é€šç”¨ Embedding ç©ºé–“</strong>
   - åœ¨åŒä¸€å€‹å‘é‡ç©ºé–“ä¸­å­¸ç¿’æ‰€æœ‰èªè¨€
   - ä¸­æ–‡è©å½™çš„ embedding å“è³ªå–æ±ºæ–¼è¨“ç·´è³‡æ–™ä¸­çš„å‡ºç¾é »ç‡
   - å¸¸è¦‹ä¸­æ–‡å­—ï¼ˆå¦‚ "çš„"ã€"æ˜¯"ï¼‰æœ‰è¼ƒå¥½çš„ embedding
   - ç½•è¦‹ä¸­æ–‡è©å½™çš„ embedding å“è³ªè¼ƒå·®

<strong>å¯¦éš›æ•ˆæœï¼š</strong>
âœ… å„ªé»ï¼š
- å¯ä»¥è™•ç†ä¸­æ–‡ï¼ˆåŸºæœ¬åŠŸèƒ½ï¼‰
- å¤šèªè¨€èƒ½åŠ›ï¼ˆå¯ä»¥åŒæ™‚è™•ç†ä¸­è‹±æ–‡ï¼‰
- å°å¸¸è¦‹ä¸­æ–‡è©å½™æœ‰ä¸€å®šç†è§£

âŒ ç¼ºé»ï¼š
- Token æ•ˆç‡ä½ï¼ˆä¸­æ–‡éœ€è¦æ›´å¤š tokensï¼‰
- èªç¾©ç†è§£ä¸å¦‚ä¸­æ–‡å„ªåŒ–æ¨¡å‹
- å°ä¸­æ–‡èªæ³•ã€æ–‡åŒ–èƒŒæ™¯ç†è§£æœ‰é™
- æˆæœ¬è¼ƒé«˜ï¼ˆæ›´å¤š tokens = æ›´é«˜æˆæœ¬ï¼‰

<strong>å¯¦éš›ä¾‹å­ï¼š</strong>
"æˆ‘æ„›äººå·¥æ™ºæ…§"
- GPT-3 Tokenization: ["æˆ‘", "æ„›", "äººå·¥", "æ™º", "æ…§"] â†’ 5-6 tokens
- GPT-3 Embedding: å‘é‡å“è³ªä¸­ç­‰ï¼Œèªç¾©ç†è§£ä¸€èˆ¬
                </pre>
              </div>

              <div class="visual-diagram lab-spacing-top">
                <h4>ğŸ“Š Mistral ç³»åˆ—ï¼ˆMistral AIï¼‰çš„è™•ç†æ–¹å¼</h4>
                <pre>
<strong>ä»£è¡¨æ¨¡å‹ï¼š</strong>Mistral-7Bã€Mixtral-8x7B

<strong>æ ¸å¿ƒç­–ç•¥ï¼š</strong>
1. <strong>å¤šèªè¨€èªæ–™è¨“ç·´ï¼ˆé¡ä¼¼ GPTï¼‰</strong>
   - è¨“ç·´èªæ–™åŒ…å«å¤šç¨®èªè¨€ï¼ˆè‹±æ–‡ã€æ³•æ–‡ã€è¥¿ç­ç‰™æ–‡ã€ä¸­æ–‡ç­‰ï¼‰
   - ä¸­æ–‡èªæ–™æ¯”ä¾‹è¼ƒä½ï¼ˆç´„ 2-3%ï¼‰
   - ä¸»è¦é‡å°æ­æ´²èªè¨€å„ªåŒ–

2. <strong>æ”¹é€²çš„ Tokenization</strong>
   - ä½¿ç”¨ SentencePieceï¼ˆæ¯” BPE æ›´å…ˆé€²ï¼‰
   - å°ä¸­æ–‡çš„è™•ç†ç•¥å¥½æ–¼ GPT-3
   - ä½†ä»ç„¶ä¸æ˜¯é‡å°ä¸­æ–‡å„ªåŒ–

3. <strong>å¤šèªè¨€ Embedding ç©ºé–“</strong>
   - åœ¨çµ±ä¸€å‘é‡ç©ºé–“ä¸­å­¸ç¿’æ‰€æœ‰èªè¨€
   - ä¸­æ–‡ embedding å“è³ªå–æ±ºæ–¼è¨“ç·´è³‡æ–™
   - å°ä¸­æ–‡çš„ç†è§£èƒ½åŠ›ä»‹æ–¼ GPT-3 å’Œä¸­æ–‡å„ªåŒ–æ¨¡å‹ä¹‹é–“

<strong>å¯¦éš›æ•ˆæœï¼š</strong>
âœ… å„ªé»ï¼š
- æ¯” GPT-3 å°ä¸­æ–‡çš„è™•ç†ç¨å¥½
- å¤šèªè¨€èƒ½åŠ›å¼·
- é–‹æºæ¨¡å‹ï¼Œå¯ä»¥æœ¬åœ°éƒ¨ç½²

âŒ ç¼ºé»ï¼š
- ä»ç„¶ä¸æ˜¯é‡å°ä¸­æ–‡å„ªåŒ–
- Token æ•ˆç‡ä¸å¦‚ä¸­æ–‡æ¨¡å‹
- å°ä¸­æ–‡èªç¾©ç†è§£æœ‰é™

<strong>å¯¦éš›ä¾‹å­ï¼š</strong>
"æˆ‘æ„›äººå·¥æ™ºæ…§"
- Mistral Tokenization: ["æˆ‘", "æ„›", "äººå·¥", "æ™ºæ…§"] â†’ 4-5 tokens
- Mistral Embedding: å‘é‡å“è³ªä¸­ç­‰ï¼Œç•¥å¥½æ–¼ GPT-3
                </pre>
              </div>

              <div class="visual-diagram lab-spacing-top">
                <h4>ğŸ“Š Claudeï¼ˆAnthropicï¼‰çš„è™•ç†æ–¹å¼</h4>
                <pre>
<strong>ä»£è¡¨æ¨¡å‹ï¼š</strong>Claude-3ã€Claude-3.5

<strong>æ ¸å¿ƒç­–ç•¥ï¼š</strong>
1. <strong>å¤šèªè¨€èªæ–™è¨“ç·´</strong>
   - è¨“ç·´èªæ–™åŒ…å«å¤šç¨®èªè¨€
   - ä¸­æ–‡èªæ–™æ¯”ä¾‹è¼ƒ GPT ç¨é«˜ï¼ˆç´„ 3-5%ï¼‰
   - æ³¨é‡èªæ–™å“è³ªè€Œéæ•¸é‡

2. <strong>æ”¹é€²çš„ Tokenization</strong>
   - ä½¿ç”¨è‡ªå®šç¾©çš„ tokenization æ–¹æ³•
   - å°ä¸­æ–‡çš„è™•ç†æ¯” GPT-3 æ›´å¥½
   - ä½†ä»ç„¶ä¸æ˜¯é‡å°ä¸­æ–‡å„ªåŒ–

3. <strong>èªç¾©ç†è§£å„ªåŒ–</strong>
   - æ³¨é‡èªç¾©ç†è§£è€Œé token æ•ˆç‡
   - å°ä¸­æ–‡çš„ç†è§£èƒ½åŠ›è¼ƒå¥½
   - ä½†ä»ç„¶ä¸å¦‚å°ˆé–€çš„ä¸­æ–‡æ¨¡å‹

<strong>å¯¦éš›æ•ˆæœï¼š</strong>
âœ… å„ªé»ï¼š
- å°ä¸­æ–‡çš„ç†è§£èƒ½åŠ›è¼ƒå¥½ï¼ˆåœ¨è¥¿æ–¹æ¨¡å‹ä¸­ï¼‰
- èªç¾©ç†è§£å¼·
- å¯ä»¥è™•ç†è¤‡é›œçš„ä¸­æ–‡ä»»å‹™

âŒ ç¼ºé»ï¼š
- ä»ç„¶ä¸æ˜¯é‡å°ä¸­æ–‡å„ªåŒ–
- Token æ•ˆç‡ä¸å¦‚ä¸­æ–‡æ¨¡å‹
- æˆæœ¬è¼ƒé«˜

<strong>å¯¦éš›ä¾‹å­ï¼š</strong>
"æˆ‘æ„›äººå·¥æ™ºæ…§"
- Claude Tokenization: ["æˆ‘", "æ„›", "äººå·¥", "æ™ºæ…§"] â†’ 4-5 tokens
- Claude Embedding: å‘é‡å“è³ªè¼ƒå¥½ï¼Œèªç¾©ç†è§£è¼ƒå¼·
                </pre>
              </div>

              <div class="explanation lab-spacing-top">
                <h4>ğŸ” è¥¿æ–¹æ¨¡å‹è™•ç†ä¸­æ–‡çš„å…±é€šå•é¡Œ</h4>
                <p>
                  <strong>1. Tokenization æ•ˆç‡ä½</strong>
                </p>
                <ul>
                  <li>ä¸­æ–‡è¢«æ‹†æˆå¤ªå¤š tokensï¼ˆ6-8 å€‹ vs ä¸­æ–‡æ¨¡å‹çš„ 3-4 å€‹ï¼‰</li>
                  <li>å°è‡´æˆæœ¬é«˜ã€é€Ÿåº¦æ…¢</li>
                  <li>ä¾‹å¦‚ï¼šGPT-3 è™•ç† "äººå·¥æ™ºæ…§" éœ€è¦ 6-8 tokensï¼Œè€Œ Qwen åªéœ€è¦ 1-2 tokens</li>
                </ul>
                <p>
                  <strong>2. èªç¾©ç†è§£æœ‰é™</strong>
                </p>
                <ul>
                  <li>è¨“ç·´èªæ–™ä¸­ä¸­æ–‡æ¯”ä¾‹ä½ï¼ˆ1-5%ï¼‰</li>
                  <li>å°ä¸­æ–‡èªæ³•ã€æ–‡åŒ–èƒŒæ™¯ç†è§£ä¸è¶³</li>
                  <li>ä¾‹å¦‚ï¼šå¯èƒ½ä¸ç†è§£ "è²“å’ª" å’Œ "è²“" æ˜¯åŒç¾©è©</li>
                </ul>
                <p>
                  <strong>3. æ–‡åŒ–èƒŒæ™¯ç†è§£ä¸è¶³</strong>
                </p>
                <ul>
                  <li>å°ä¸­æ–‡æ–‡åŒ–ã€ç¿’ä¿—ã€æˆèªç†è§£æœ‰é™</li>
                  <li>å¯èƒ½ç”¢ç”Ÿä¸ç¬¦åˆä¸­æ–‡ç¿’æ…£çš„å›ç­”</li>
                  <li>ä¾‹å¦‚ï¼šå¯èƒ½ä¸ç†è§£ "ä¸€çŸ³äºŒé³¥" çš„å«ç¾©</li>
                </ul>
              </div>

              <div class="visual-diagram lab-spacing-top">
                <h4>ğŸ“Š å°æ¯”è¡¨æ ¼ï¼šè¥¿æ–¹æ¨¡å‹ vs ä¸­æ–‡å„ªåŒ–æ¨¡å‹</h4>
                <pre>
<strong>æ¯”è¼ƒé …ç›®          | GPT-3/4    | Mistral    | Claude    | Qwen/ChatGLM</strong>
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
è¨“ç·´èªæ–™ä¸­æ–‡æ¯”ä¾‹    | 1-5%       | 2-3%       | 3-5%      | 80%+
Tokenization æ•ˆç‡  | ä½ (6-8)   | ä¸­ (4-5)   | ä¸­ (4-5)  | é«˜ (1-2)
èªç¾©ç†è§£èƒ½åŠ›        | ä¸­         | ä¸­         | ä¸­é«˜      | é«˜
ä¸­æ–‡èªæ³•ç†è§£        | ä½         | ä½         | ä¸­        | é«˜
æ–‡åŒ–èƒŒæ™¯ç†è§£        | ä½         | ä½         | ä¸­        | é«˜
æˆæœ¬ï¼ˆç›¸åŒä»»å‹™ï¼‰    | é«˜         | ä¸­é«˜       | é«˜        | ä½
å¤šèªè¨€èƒ½åŠ›          | é«˜         | é«˜         | é«˜        | ä¸­

<strong>é©ç”¨å ´æ™¯ï¼š</strong>
- GPT/Mistral/Claude: å¤šèªè¨€æ··åˆä»»å‹™ã€åœ‹éš›åŒ–æ‡‰ç”¨
- Qwen/ChatGLM: ç´”ä¸­æ–‡ä»»å‹™ã€ä¸­æ–‡å…§å®¹ç”Ÿæˆã€ä¸­æ–‡å•ç­”
                </pre>
              </div>

              <div class="ai-prompt-block lab-spacing-top">
                <p>
                  <strong>å‹•æ‰‹è©¦è©¦ï¼šæ¯”è¼ƒè¥¿æ–¹æ¨¡å‹å’Œä¸­æ–‡æ¨¡å‹çš„ä¸­æ–‡è™•ç†èƒ½åŠ›</strong>
                </p>
                <pre>
è«‹å¹«æˆ‘æ¯”è¼ƒä¸åŒæ¨¡å‹å°ä¸­æ–‡çš„è™•ç†èƒ½åŠ›ï¼š

1. å®‰è£ transformers å’Œ tiktokenï¼špip install transformers tiktoken torch
2. æ¸¬è©¦ä»¥ä¸‹æ¨¡å‹ï¼š
   - GPT-2 (è¥¿æ–¹æ¨¡å‹ä»£è¡¨)
   - Mistral-7B (å¦‚æœæœ‰)
   - Qwen/Qwen2-0.5B (ä¸­æ–‡å„ªåŒ–æ¨¡å‹)
3. æ¸¬è©¦é …ç›®ï¼š
   a. Tokenization æ•ˆç‡ï¼šè¨ˆç®— "æˆ‘æ„›äººå·¥æ™ºæ…§" çš„ token æ•¸é‡
   b. èªç¾©ç›¸ä¼¼åº¦ï¼šè¨ˆç®— "è²“" å’Œ "è²“å’ª" çš„ embedding ç›¸ä¼¼åº¦
   c. èªç¾©ç†è§£ï¼šæ¸¬è©¦ "ä¸€çŸ³äºŒé³¥" çš„èªç¾©ç†è§£
4. æ¯”è¼ƒçµæœä¸¦åˆ†æå·®ç•°

è«‹å¹«æˆ‘å¯«ä¸€å€‹å®Œæ•´çš„æ¯”è¼ƒè…³æœ¬ä¸¦åŸ·è¡Œã€‚
                </pre>
                <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
                  ğŸ“‹ è¤‡è£½ Prompt
                </button>
              </div>

              <div class="lab-code-block collapsed">
                <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
                  <span class="filename">compare_western_vs_chinese_models.py</span>
                  <div class="header-buttons">
                    <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                    <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
                  </div>
                </div>
                <div class="lab-code-block-content">
                  <pre><code class="language-python">import tiktoken
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# æ¸¬è©¦æ–‡å­—
test_text = "æˆ‘æ„›äººå·¥æ™ºæ…§"
test_words = ["è²“", "è²“å’ª", "ç‹—", "å°ç‹—"]

print("=" * 70)
print("æ¯”è¼ƒè¥¿æ–¹æ¨¡å‹å’Œä¸­æ–‡æ¨¡å‹çš„ä¸­æ–‡è™•ç†èƒ½åŠ›")
print("=" * 70)

# 1. Tokenization æ•ˆç‡æ¯”è¼ƒ
print("\nğŸ“Š 1. Tokenization æ•ˆç‡æ¯”è¼ƒ")
print("-" * 70)

# GPT-3.5/GPT-4 (ä½¿ç”¨ tiktoken)
try:
    enc_gpt = tiktoken.get_encoding("cl100k_base")
    gpt_tokens = enc_gpt.encode(test_text)
    print(f"GPT-3.5/4: '{test_text}' â†’ {len(gpt_tokens)} tokens")
    print(f"  Token IDs: {gpt_tokens}")
except Exception as e:
    print(f"ç„¡æ³•æ¸¬è©¦ GPT: {e}")

# Qwen (ä¸­æ–‡å„ªåŒ–)
try:
    qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B")
    qwen_tokens = qwen_tokenizer.encode(test_text)
    print(f"Qwen: '{test_text}' â†’ {len(qwen_tokens)} tokens")
    print(f"  Token IDs: {qwen_tokens}")
except Exception as e:
    print(f"ç„¡æ³•æ¸¬è©¦ Qwen: {e}")

# 2. èªç¾©ç›¸ä¼¼åº¦æ¯”è¼ƒ
print("\nğŸ“Š 2. èªç¾©ç›¸ä¼¼åº¦æ¯”è¼ƒï¼ˆ'è²“' vs 'è²“å’ª'ï¼‰")
print("-" * 70)

def get_embedding(model, tokenizer, text):
    """å–å¾—æ–‡å­—çš„ embedding"""
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        embedding = outputs.last_hidden_state.mean(dim=1).squeeze()
    return embedding.numpy()

# GPT-2 (è¥¿æ–¹æ¨¡å‹ä»£è¡¨)
try:
    print("\nGPT-2 (è¥¿æ–¹æ¨¡å‹):")
    gpt2_tokenizer = AutoTokenizer.from_pretrained("gpt2")
    gpt2_model = AutoModel.from_pretrained("gpt2")
    
    gpt2_emb_cat = get_embedding(gpt2_model, gpt2_tokenizer, "è²“")
    gpt2_emb_cat2 = get_embedding(gpt2_model, gpt2_tokenizer, "è²“å’ª")
    gpt2_sim = cosine_similarity([gpt2_emb_cat], [gpt2_emb_cat2])[0][0]
    print(f"  'è²“' â†” 'è²“å’ª' ç›¸ä¼¼åº¦: {gpt2_sim:.4f}")
except Exception as e:
    print(f"ç„¡æ³•æ¸¬è©¦ GPT-2: {e}")

# Qwen (ä¸­æ–‡å„ªåŒ–)
try:
    print("\nQwen (ä¸­æ–‡å„ªåŒ–):")
    qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B")
    qwen_model = AutoModel.from_pretrained("Qwen/Qwen2-0.5B")
    
    qwen_emb_cat = get_embedding(qwen_model, qwen_tokenizer, "è²“")
    qwen_emb_cat2 = get_embedding(qwen_model, qwen_tokenizer, "è²“å’ª")
    qwen_sim = cosine_similarity([qwen_emb_cat], [qwen_emb_cat2])[0][0]
    print(f"  'è²“' â†” 'è²“å’ª' ç›¸ä¼¼åº¦: {qwen_sim:.4f}")
except Exception as e:
    print(f"ç„¡æ³•æ¸¬è©¦ Qwen: {e}")

# 3. ç¸½çµ
print("\n" + "=" * 70)
print("ğŸ“Š ç¸½çµ")
print("=" * 70)
print("1. Tokenization æ•ˆç‡ï¼šä¸­æ–‡å„ªåŒ–æ¨¡å‹é€šå¸¸éœ€è¦æ›´å°‘çš„ tokens")
print("2. èªç¾©ç›¸ä¼¼åº¦ï¼šä¸­æ–‡å„ªåŒ–æ¨¡å‹å°åŒç¾©è©çš„ç†è§£æ›´å¥½")
print("3. å»ºè­°ï¼š")
print("   - ç´”ä¸­æ–‡ä»»å‹™ â†’ ä½¿ç”¨ä¸­æ–‡å„ªåŒ–æ¨¡å‹ï¼ˆQwenã€ChatGLMï¼‰")
print("   - å¤šèªè¨€ä»»å‹™ â†’ ä½¿ç”¨è¥¿æ–¹æ¨¡å‹ï¼ˆGPTã€Mistralã€Claudeï¼‰")</code></pre>
                </div>
              </div>
            </div>

            <div class="ai-prompt-block lab-spacing-top">
              <p>
                <strong>å‹•æ‰‹è©¦è©¦ï¼šæ¯”è¼ƒä¸åŒæ¨¡å‹çš„ä¸­æ–‡ Embedding</strong>
              </p>
              <pre>
è«‹å¹«æˆ‘æ¯”è¼ƒä¸åŒæ¨¡å‹çš„ä¸­æ–‡ Embedding æ•ˆæœï¼š

1. å®‰è£ transformers åº«ï¼špip install transformers torch
2. è¼‰å…¥ä»¥ä¸‹æ¨¡å‹çš„ tokenizer å’Œ modelï¼š
   - GPT-2 (è‹±æ–‡ç‚ºä¸»)
   - Qwen/Qwen2-0.5B (ä¸­æ–‡å„ªåŒ–)
   - æˆ– ChatGLM (å¦‚æœæœ‰)
3. æ¸¬è©¦ä»¥ä¸‹ä¸­æ–‡è©å½™çš„ embeddingï¼š
   - "è²“"ã€"ç‹—"ã€"è²“å’ª"ã€"å°ç‹—"
   - "åŒ—äº¬"ã€"ä¸Šæµ·"ã€"åŸå¸‚"
   - "å¤§"ã€"å°"ã€"å¥½"ã€"å£"
4. è¨ˆç®—è©å½™ä¹‹é–“çš„ç›¸ä¼¼åº¦
5. æ¯”è¼ƒä¸åŒæ¨¡å‹çš„çµæœï¼Œçœ‹çœ‹å“ªå€‹æ›´ç¬¦åˆä¸­æ–‡èªç¾©

è«‹å¹«æˆ‘å¯«ä¸€å€‹æ¯”è¼ƒè…³æœ¬ä¸¦åŸ·è¡Œã€‚
              </pre>
              <button class="ai-prompt-copy-btn" onclick="copyPrompt(this)">
                ğŸ“‹ è¤‡è£½ Prompt
              </button>
            </div>

            <div class="lab-code-block collapsed">
              <div class="lab-code-block-header" onclick="toggleCodeBlock(this)">
                <span class="filename">compare_chinese_embeddings.py</span>
                <div class="header-buttons">
                  <button class="toggle-btn" onclick="event.stopPropagation(); toggleCodeBlock(this.closest('.lab-code-block-header'))">å±•é–‹</button>
                  <button class="copy-btn" onclick="event.stopPropagation(); copyCode(this)">è¤‡è£½</button>
                </div>
              </div>
              <div class="lab-code-block-content">
                <pre><code class="language-python">from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def get_embedding(model, tokenizer, text):
    """å–å¾—æ–‡å­—çš„ embedding"""
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        # ä½¿ç”¨ [CLS] token æˆ–å¹³å‡æ± åŒ–
        embedding = outputs.last_hidden_state.mean(dim=1).squeeze()
    return embedding.numpy()

# æ¸¬è©¦è©å½™
test_words = ["è²“", "ç‹—", "è²“å’ª", "å°ç‹—", "åŒ—äº¬", "ä¸Šæµ·", "å¤§", "å°"]

print("=" * 60)
print("æ¯”è¼ƒä¸åŒæ¨¡å‹çš„ä¸­æ–‡ Embedding")
print("=" * 60)

# 1. Qwen (ä¸­æ–‡å„ªåŒ–)
print("\n1. Qwen æ¨¡å‹ (ä¸­æ–‡å„ªåŒ–)")
print("-" * 60)
try:
    qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B")
    qwen_model = AutoModel.from_pretrained("Qwen/Qwen2-0.5B")
    
    qwen_embeddings = {}
    for word in test_words:
        emb = get_embedding(qwen_model, qwen_tokenizer, word)
        qwen_embeddings[word] = emb
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    print("è©å½™ç›¸ä¼¼åº¦ï¼ˆQwenï¼‰:")
    for i, word1 in enumerate(test_words):
        for word2 in test_words[i+1:]:
            sim = cosine_similarity(
                [qwen_embeddings[word1]], 
                [qwen_embeddings[word2]]
            )[0][0]
            if sim > 0.7:  # åªé¡¯ç¤ºé«˜ç›¸ä¼¼åº¦
                print(f"  '{word1}' â†” '{word2}': {sim:.4f}")
except Exception as e:
    print(f"ç„¡æ³•è¼‰å…¥ Qwen: {e}")

# 2. GPT-2 (è‹±æ–‡ç‚ºä¸»ï¼Œå°æ¯”ç”¨)
print("\n2. GPT-2 æ¨¡å‹ (è‹±æ–‡ç‚ºä¸»ï¼Œå°æ¯”ç”¨)")
print("-" * 60)
try:
    gpt2_tokenizer = AutoTokenizer.from_pretrained("gpt2")
    gpt2_model = AutoModel.from_pretrained("gpt2")
    
    gpt2_embeddings = {}
    for word in test_words:
        emb = get_embedding(gpt2_model, gpt2_tokenizer, word)
        gpt2_embeddings[word] = emb
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    print("è©å½™ç›¸ä¼¼åº¦ï¼ˆGPT-2ï¼‰:")
    for i, word1 in enumerate(test_words):
        for word2 in test_words[i+1:]:
            sim = cosine_similarity(
                [gpt2_embeddings[word1]], 
                [gpt2_embeddings[word2]]
            )[0][0]
            if sim > 0.7:
                print(f"  '{word1}' â†” '{word2}': {sim:.4f}")
except Exception as e:
    print(f"ç„¡æ³•è¼‰å…¥ GPT-2: {e}")

print("\n" + "=" * 60)
print("ğŸ“Š ç¸½çµ")
print("=" * 60)
print("ä¸­æ–‡å„ªåŒ–çš„æ¨¡å‹ï¼ˆå¦‚ Qwenï¼‰é€šå¸¸èƒ½æ›´å¥½åœ°æ•æ‰ä¸­æ–‡èªç¾©é—œä¿‚")
print("ä¾‹å¦‚ï¼š'è²“' å’Œ 'è²“å’ª' åœ¨ Qwen ä¸­ç›¸ä¼¼åº¦æ›´é«˜")</code></pre>
              </div>
            </div>
          </div>

          <!-- å¯¦éš›å½±éŸ¿ -->
          <div class="lab-section lab-spacing-top">
            <h3>ğŸ’¡ 3. å¯¦éš›å½±éŸ¿ï¼šç‚ºä»€éº¼é€™å¾ˆé‡è¦ï¼Ÿ</h3>
            
            <div class="visual-diagram">
              <h4>ğŸ¯ èªç¾©ç†è§£èƒ½åŠ›</h4>
              <pre>
<strong>ä½¿ç”¨è‹±æ–‡èªæ–™è¨“ç·´çš„æ¨¡å‹ï¼ˆå¦‚ GPT-3ï¼‰ï¼š</strong>
- "è²“" å’Œ "ç‹—" çš„å‘é‡å¯èƒ½ä¸å¤ªç›¸ä¼¼
- å› ç‚ºåœ¨è‹±æ–‡èªæ–™ä¸­ï¼Œé€™äº›ä¸­æ–‡å­—å‡ºç¾é »ç‡ä½
- æ¨¡å‹ç„¡æ³•ç†è§£ä¸­æ–‡çš„èªç¾©é—œä¿‚

<strong>ä½¿ç”¨ä¸­æ–‡èªæ–™è¨“ç·´çš„æ¨¡å‹ï¼ˆå¦‚ Qwenï¼‰ï¼š</strong>
- "è²“" å’Œ "ç‹—" çš„å‘é‡éå¸¸ç›¸ä¼¼ï¼ˆéƒ½æ˜¯å‹•ç‰©ï¼‰
- "è²“" å’Œ "è²“å’ª" çš„å‘é‡å¹¾ä¹ç›¸åŒï¼ˆåŒç¾©è©ï¼‰
- æ¨¡å‹èƒ½ç†è§£ä¸­æ–‡çš„èªç¾©é—œä¿‚

<strong>å¯¦éš›æ‡‰ç”¨ï¼š</strong>
- ä¸­æ–‡å•ç­”ç³»çµ±ï¼šéœ€è¦ç†è§£ä¸­æ–‡èªç¾©
- ä¸­æ–‡ç¿»è­¯ï¼šéœ€è¦æ•æ‰èªç¾©ç›¸ä¼¼æ€§
- ä¸­æ–‡æƒ…æ„Ÿåˆ†æï¼šéœ€è¦ç†è§£è©å½™çš„æƒ…æ„Ÿè‰²å½©
              </pre>
            </div>

            <div class="visual-diagram lab-spacing-top">
              <h4>ğŸ’° æ•ˆç‡å’Œæˆæœ¬</h4>
              <pre>
<strong>Token æ•ˆç‡ï¼š</strong>
- ä¸­æ–‡å„ªåŒ–æ¨¡å‹ï¼šæ›´å°‘çš„ tokens è¡¨é”ç›¸åŒæ„æ€
- ä¾‹å¦‚ï¼š"äººå·¥æ™ºæ…§" â†’ 1-2 tokens (Qwen)
- è‹±æ–‡æ¨¡å‹ï¼šå¯èƒ½éœ€è¦ 4-6 tokens (GPT-3)

<strong>æˆæœ¬å½±éŸ¿ï¼š</strong>
- æ›´å°‘çš„ tokens = æ›´ä½çš„ API æˆæœ¬
- æ›´å¿«çš„æ¨ç†é€Ÿåº¦
- æ›´å¥½çš„èªç¾©ç†è§£

<strong>å¯¦éš›ä¾‹å­ï¼š</strong>
åŒæ¨£æ„æ€çš„å°è©±ï¼š
- GPT-3: "æˆ‘æ„›äººå·¥æ™ºæ…§" â†’ 12 tokens â†’ $0.000018
- Qwen: "æˆ‘æ„›äººå·¥æ™ºæ…§" â†’ 5 tokens â†’ $0.0000075
â†’ æˆæœ¬é™ä½ 58%ï¼
              </pre>
            </div>

            <div class="lab-setup lab-spacing-top">
              <h4>ğŸ“š å»¶ä¼¸é–±è®€</h4>
              <ul>
                <li>
                  <strong>è«–æ–‡ï¼š</strong>
                  <a href="https://arxiv.org/abs/2309.16609" target="_blank">Qwen Technical Report</a> - äº†è§£ Qwen çš„æŠ€è¡“ç´°ç¯€
                </li>
                <li>
                  <strong>è«–æ–‡ï¼š</strong>
                  <a href="https://arxiv.org/abs/2103.10360" target="_blank">GLM: General Language Model Pretraining</a> - ChatGLM çš„åŸºç¤
                </li>
                <li>
                  <strong>å¯¦éš›æ‡‰ç”¨ï¼š</strong>
                  æ¯”è¼ƒä¸åŒæ¨¡å‹çš„ä¸­æ–‡ç†è§£èƒ½åŠ›ï¼Œé¸æ“‡æœ€é©åˆä½ ä»»å‹™çš„æ¨¡å‹
                </li>
              </ul>
            </div>
          </div>
        </section>

        <!-- Challenge -->
        <div class="lab-challenge lab-spacing-top-lg">
          <h4>ğŸ’ª æŒ‘æˆ°ä»»å‹™</h4>
          <p>
            <strong>é€²éšæŒ‘æˆ° 1ï¼š</strong>
            ç”¨æ›´å¤§çš„èªæ–™è¨“ç·´ Word2Vecï¼ˆä¾‹å¦‚ï¼šä¸‹è¼‰ Wikipedia çš„å°å‹è³‡æ–™é›†ï¼‰ã€‚
            æ¯”è¼ƒä¸åŒåƒæ•¸ï¼ˆvector_size, windowï¼‰å°çµæœçš„å½±éŸ¿ã€‚
          </p>
          <p>
            <strong>é€²éšæŒ‘æˆ° 2ï¼š</strong>
            å¯¦ä½œä¸€å€‹ç°¡å–®çš„ã€Œè©å½™é¡æ¯”ã€éŠæˆ²ï¼š
            çµ¦å®š "dog : loyal = cat : ?"ï¼Œè®“æ¨¡å‹é æ¸¬ç­”æ¡ˆã€‚
          </p>
          <p>
            <strong>é€²éšæŒ‘æˆ° 3ï¼š</strong>
            æ¯”è¼ƒ Word2Vec å’Œ GloVe çš„å·®ç•°ã€‚
            å®‰è£ glove-python ä¸¦è¨“ç·´ä¸€å€‹ GloVe æ¨¡å‹ï¼Œæ¯”è¼ƒå…©è€…çš„è¦–è¦ºåŒ–çµæœã€‚
          </p>
        </div>

        <!-- Next Steps -->
        <div class="lab-setup lab-spacing-top-lg">
          <h4>ğŸ¯ ä¸‹ä¸€æ­¥</h4>
          <ul>
            <li>
              âœ… æ­å–œï¼ä½ å·²ç¶“å­¸æœƒäº† Embeddingã€‚ç¾åœ¨æ–‡å­—å¯ä»¥è®Šæˆå‘é‡äº†ï¼
            </li>
            <li>
              â¡ï¸ æ¥ä¸‹ä¾†æˆ‘å€‘è¦å­¸ <strong>ä½ç½®ç·¨ç¢¼</strong>ï¼š
              Transformer å¦‚ä½•çŸ¥é“è©å½™çš„ã€Œé †åºã€ï¼Ÿé€™æ˜¯ç†è§£ Transformer çš„é—œéµï¼
            </li>
            <li>
              ğŸ“š å»ºè­°ï¼šå…ˆå®Œæˆè¦–è¦ºåŒ–ï¼Œçœ‹çœ‹ä½ çš„ Embedding æ˜¯å¦åˆç†ã€‚
            </li>
          </ul>
        </div>

        <div class="chapter-nav lab-spacing-top-lg">
          <a href="../index.html" class="nav-link">â† è¿”å› Phase 1</a>
          <a href="../01-tokenization/index.html" class="nav-link">â† ä¸Šä¸€å€‹å¯¦é©—</a>
          <a href="../03-positional-encoding/index.html" class="nav-link">ä¸‹ä¸€å€‹å¯¦é©—ï¼šä½ç½®ç·¨ç¢¼ â†’</a>
        </div>
      </div>
    </div>

    <script>
      function copyPrompt(btn) {
        const promptBlock = btn.closest(".ai-prompt-block");
        const preElement = promptBlock.querySelector("pre");
        const text = preElement.textContent;

        navigator.clipboard.writeText(text).then(() => {
          btn.classList.add("copied");
          btn.textContent = "âœ“ å·²è¤‡è£½ï¼";
          setTimeout(() => {
            btn.classList.remove("copied");
            btn.textContent = "ğŸ“‹ è¤‡è£½ Prompt";
          }, 2000);
        });
      }

      function toggleCodeBlock(header) {
        const codeBlock = header.closest(".lab-code-block");
        const toggleBtn = header.querySelector(".toggle-btn");
        
        codeBlock.classList.toggle("collapsed");
        
        if (codeBlock.classList.contains("collapsed")) {
          toggleBtn.textContent = "å±•é–‹";
        } else {
          toggleBtn.textContent = "æ”¶èµ·";
        }
      }

      function copyCode(btn) {
        const codeBlock = btn.closest(".lab-code-block");
        const codeElement = codeBlock.querySelector("pre code");
        const text = codeElement.textContent;

        navigator.clipboard.writeText(text).then(() => {
          btn.textContent = "âœ“ å·²è¤‡è£½";
          setTimeout(() => {
            btn.textContent = "è¤‡è£½";
          }, 2000);
        });
      }

      function copyTerminal(btn) {
        const terminalBlock = btn.closest(".terminal-command");
        const codeElement = terminalBlock.querySelector("code");
        const text = codeElement.textContent;

        navigator.clipboard.writeText(text).then(() => {
          btn.textContent = "âœ“ å·²è¤‡è£½";
          setTimeout(() => {
            btn.textContent = "ğŸ“‹ è¤‡è£½æŒ‡ä»¤";
          }, 2000);
        });
      }
    </script>
  </body>
</html>
