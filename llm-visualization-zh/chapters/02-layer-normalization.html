<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第二章：Layer Normalization | nano-gpt 視覺化教學</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700;900&family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../styles/llm-viz.css">
    
</head>
<body>
    <div class="breadcrumb">
        <a href="../../index.html">🏠 首頁</a>
        <span style="margin: 0 10px; color: #999;">/</span>
        <a href="../index.html">📚 LLM 視覺化教學</a>
        <span style="margin: 0 10px; color: #999;">/</span>
        <span style="color: #666;">第二章：Layer Normalization</span>
    </div>

    <!-- Hero Section -->
    <section class="hero-section">
        <div class="hero-overlay"></div>
        <div class="hero-content">
            <div class="nano-gpt-badge">nano-gpt · Transformer Block</div>
            <h1 class="hero-title">Layer Normalization</h1>
            <p class="hero-subtitle">穩定訓練的關鍵：標準化每一列</p>
            <p class="hero-meta">讓平均值為 0，標準差為 1</p>
        </div>
    </section>

    <!-- Main Content -->
    <div class="story-container">
        <div class="content-wrapper">
            
            <h2 style="border: none; margin-top: 0; text-align: center; color: var(--gold);">🗺️ 本章在架構中的位置</h2>

            <div class="info-box" style="background: linear-gradient(135deg, rgba(255, 215, 0, 0.15) 0%, rgba(255, 215, 0, 0.08) 100%); border-color: var(--gold); margin: 40px 0 60px 0;">
                <div style="text-align: center; margin-bottom: 30px;">
                    <div style="display: inline-block; padding: 15px 40px; background: rgba(255, 215, 0, 0.2); border: 2px solid var(--gold); border-radius: 50px; font-size: 1.2rem; font-weight: 700; color: var(--gold);">
                        📍 第二章：Layer Normalization
                    </div>
                </div>
                
                <figure style="margin: 30px 0;">
                    <img src="../../images/layer_norm_chapter_focus_20260105211353.png" alt="Layer Normalization 章節架構特寫" style="max-width: 100%; border-radius: 15px; box-shadow: 0 10px 40px rgba(0,0,0,0.2);">
                    <figcaption style="margin-top: 20px; font-size: 1rem; color: #666; line-height: 1.8;">
                        <strong style="color: #0a0e27;">Layer Normalization 位置</strong><br>
                        本章聚焦於<span style="color: #ffd700; font-weight: 700;"> 黃色區域</span>：<br>
                        Transformer Block 的第一步 - Layer Normalization
                    </figcaption>
                </figure>

                <div style="background: rgba(255, 255, 255, 0.6); padding: 25px; border-radius: 12px; margin-top: 30px;">
                    <p style="margin: 0; text-align: center; color: #2c3e50; line-height: 1.9;">
                        <strong style="color: #ffd700;">🟡 Layer Normalization 是 Transformer 的穩定劑</strong><br>
                        從上一章的 Input Embedding 矩陣進入 Transformer Block 後，<br>
                        第一步就是對每一列（每個 token）進行正規化處理。
                    </p>
                </div>
            </div>
            
            <div class="story-lead">
                「上一章的 Input Embedding 矩陣，<br>
                現在是我們第一個 Transformer Block 的輸入。<br><br>
                Transformer Block 的<strong>第一步</strong>，<br>
                就是對這個矩陣套用 <strong>Layer Normalization</strong>（層歸一化）。」
            </div>

            <div class="info-box">
                <div class="info-box-title">
                    <span>💡</span>
                    <span>核心概念</span>
                </div>
                <p><strong>Layer Normalization 是什麼？</strong></p>
                <p>這是一個操作，會<strong>分別</strong>正規化矩陣中的每一列的值。</p>
                <ul style="margin-left: 20px; line-height: 2; margin-top: 15px;">
                    <li><strong>為什麼重要？</strong>正規化是訓練深度神經網絡的重要步驟</li>
                    <li><strong>效果？</strong>有助於提高模型在訓練期間的穩定性</li>
                    <li><strong>範圍？</strong>對每一列（每個 token）獨立處理</li>
                </ul>
            </div>

            <h2>🎯 Layer Normalization 的目標</h2>

            <p>我們可以將每一列分開來看，所以先聚焦在<strong>第 4 列</strong>（t = 3）。</p>

            <div class="formula-box">
                <div class="formula-title">Layer Normalization 的兩個目標</div>
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin: 30px 0;">
                    <div style="padding: 20px; background: rgba(255, 215, 0, 0.1); border-radius: 10px;">
                        <div style="font-size: 3rem; margin-bottom: 10px;">0</div>
                        <div style="font-size: 1.1rem; color: #666;">使列的平均值 = 0</div>
                    </div>
                    <div style="padding: 20px; background: rgba(255, 215, 0, 0.1); border-radius: 10px;">
                        <div style="font-size: 3rem; margin-bottom: 10px;">1</div>
                        <div style="font-size: 1.1rem; color: #666;">使列的標準差 = 1</div>
                    </div>
                </div>
                <p class="formula-desc">
                    為了達成這個目標，我們需要找到這兩個量（平均值 μ 和標準差 σ），<br>
                    然後減去平均值並除以標準差。
                </p>
            </div>

            <h2>📐 數學公式</h2>

            <div class="matrix-box">
                <div class="matrix-title">Layer Normalization 公式</div>
                <div class="code-visualization">
                    <pre><span class="code-comment"># 步驟 1：計算平均值和變異數</span>
μ = E[x]             <span class="code-comment"># 平均值（mean）</span>
σ² = Var[x]          <span class="code-comment"># 變異數（variance）</span>
σ = √Var[x]          <span class="code-comment"># 標準差（standard deviation）</span>

<span class="code-comment"># 步驟 2：正規化</span>
x_normalized = (x - μ) / √(σ² + ε)

<span class="code-comment"># 其中 ε = 1×10⁻⁵ (防止除以零)</span>

<span class="code-comment"># 步驟 3：縮放和位移（可學習參數）</span>
y = γ · x_normalized + β

<span class="code-comment"># γ (gamma): 縮放參數（可學習）</span>
<span class="code-comment"># β (beta): 位移參數（可學習）</span></pre>
                </div>
            </div>

            <div class="info-box">
                <div class="info-box-title">
                    <span>📊</span>
                    <span>數學符號說明</span>
                </div>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li><strong>E[x]</strong>：期望值（Expected value），即平均值</li>
                    <li><strong>Var[x]</strong>：變異數（Variance），標準差的平方</li>
                    <li><strong>ε（epsilon）</strong>：極小值 1×10⁻⁵，防止除以零</li>
                    <li><strong>γ（gamma）</strong>：可學習的縮放權重</li>
                    <li><strong>β（beta）</strong>：可學習的位移偏差</li>
                    <li><strong>C</strong>：列的長度（在 nano-gpt 中 C = 48）</li>
                </ul>
            </div>

            <h2>📍 步驟詳解：第 4 列的正規化</h2>

            <figure>
                <img src="../../images/layer_norm_process_20260105211427.png" alt="Layer Normalization 計算流程">
                <figcaption>
                    視覺化：Layer Normalization 的三個步驟
                </figcaption>
            </figure>

            <div class="step-box">
                <div style="display: flex; align-items: center; margin-bottom: 20px;">
                    <span class="step-number">1</span>
                    <span class="step-title">計算統計量：μ 和 σ</span>
                </div>
                <p>對第 4 列（長度 C = 48）計算：</p>
                <div class="code-visualization">
                    <pre><span class="code-comment"># 假設第 4 列的值為：</span>
column_4 = [<span class="code-number">0.234</span>, <span class="code-number">-0.512</span>, <span class="code-number">0.823</span>, ... , <span class="code-number">0.156</span>]  <span class="code-comment"># 48 個值</span>

<span class="code-comment"># 計算平均值</span>
μ = <span class="code-keyword">sum</span>(column_4) / <span class="code-number">48</span>
<span class="code-comment"># 例如：μ = 0.125</span>

<span class="code-comment"># 計算變異數</span>
σ² = <span class="code-keyword">sum</span>((column_4 - μ)²) / <span class="code-number">48</span>
<span class="code-comment"># 例如：σ² = 0.342</span>

<span class="code-comment"># 計算標準差</span>
σ = <span class="code-keyword">sqrt</span>(σ²)
<span class="code-comment"># 例如：σ = 0.585</span></pre>
                </div>
            </div>

            <div class="step-box">
                <div style="display: flex; align-items: center; margin-bottom: 20px;">
                    <span class="step-number">2</span>
                    <span class="step-title">標準化：減去平均值，除以標準差</span>
                </div>
                <p>對列中的每個元素套用標準化公式：</p>
                <div class="formula-box">
                    <div class="formula">x_norm = (x - μ) / √(σ² + ε)</div>
                    <div class="formula-desc">
                        <strong>為什麼加 ε？</strong><br>
                        當 σ² 非常接近零時，加上 ε = 1×10⁻⁵ 可以防止除以零的錯誤
                    </div>
                </div>
                <div class="code-visualization">
                    <pre><span class="code-comment"># 對每個值進行標準化</span>
<span class="code-keyword">for</span> i <span class="code-keyword">in</span> <span class="code-keyword">range</span>(<span class="code-number">48</span>):
    x_norm[i] = (column_4[i] - μ) / <span class="code-keyword">sqrt</span>(σ² + <span class="code-number">1e-5</span>)

<span class="code-comment"># 例如：</span>
<span class="code-comment"># x[0] = 0.234 → x_norm[0] = (0.234 - 0.125) / 0.585 = 0.186</span>
<span class="code-comment"># x[1] = -0.512 → x_norm[1] = (-0.512 - 0.125) / 0.585 = -1.089</span></pre>
                </div>
                <p style="margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 8px; color: #666;">
                    <strong>✨ 結果：</strong>標準化後的列，平均值 ≈ 0，標準差 ≈ 1
                </p>
            </div>

            <div class="step-box">
                <div style="display: flex; align-items: center; margin-bottom: 20px;">
                    <span class="step-number">3</span>
                    <span class="step-title">縮放和位移：γ 和 β</span>
                </div>
                <p>最後，我們用兩個<strong>可學習的參數</strong>對標準化後的值進行調整：</p>
                <div class="formula-box">
                    <div class="formula">y = γ · x_norm + β</div>
                    <div class="formula-desc">
                        <strong>γ (gamma)</strong>：縮放權重，控制數值的範圍<br>
                        <strong>β (beta)</strong>：位移偏差，控制數值的中心
                    </div>
                </div>
                <div class="code-visualization">
                    <pre><span class="code-comment"># 套用可學習的縮放和位移</span>
y = γ * x_norm + β

<span class="code-comment"># γ 和 β 是在訓練過程中學習的參數</span>
<span class="code-comment"># 每個 column position 都有自己的 γ 和 β</span>

<span class="code-comment"># 例如，假設：</span>
<span class="code-comment"># γ = 1.2, β = 0.1</span>
<span class="code-comment"># y[0] = 1.2 * 0.186 + 0.1 = 0.323</span>
<span class="code-comment"># y[1] = 1.2 * (-1.089) + 0.1 = -1.207</span></pre>
                </div>
            </div>

            <div class="info-box" style="background: linear-gradient(135deg, rgba(0, 255, 65, 0.1) 0%, rgba(0, 255, 65, 0.05) 100%); border-color: var(--matrix-green);">
                <div class="info-box-title" style="color: var(--matrix-green);">
                    <span>🔵</span>
                    <span>為什麼需要 γ 和 β？</span>
                </div>
                <p>標準化後，所有值的分布都變成平均值 0、標準差 1。但這樣的限制可能太嚴格了！</p>
                <p style="margin-top: 15px;">透過可學習的 γ 和 β，模型可以：</p>
                <ul style="margin-left: 20px; line-height: 2; margin-top: 10px;">
                    <li><strong>γ</strong>：學習最適合的數值範圍（縮放）</li>
                    <li><strong>β</strong>：學習最適合的數值中心（位移）</li>
                    <li>在必要時，甚至可以「還原」標準化（如果 γ=σ, β=μ）</li>
                </ul>
            </div>

            <h2>🔄 對整個矩陣的操作</h2>

            <p>我們對 Input Embedding 矩陣的<strong>每一列</strong>執行相同的操作。</p>

            <div class="matrix-box">
                <div class="matrix-title">完整的 Layer Normalization 過程</div>
                <div class="code-visualization">
                    <pre><span class="code-comment"># Input Embedding: [T × C] = [6 × 48]</span>

Position   Before LayerNorm        After LayerNorm
────────────────────────────────────────────────────────
   0       [48 個原始值]     →     [48 個正規化值]
   1       [48 個原始值]     →     [48 個正規化值]
   2       [48 個原始值]     →     [48 個正規化值]
   <span class="code-keyword">3</span>       <span class="code-keyword">[48 個原始值]</span>     →     <span class="code-keyword">[48 個正規化值]</span>  ← 剛剛的例子
   4       [48 個原始值]     →     [48 個正規化值]
   5       [48 個原始值]     →     [48 個正規化值]

<span class="code-comment"># 每一列獨立計算 μ, σ, 並套用自己的 γ, β</span>
<span class="code-comment"># 最終結果：Normalized Input Embedding [6 × 48]</span></pre>
                </div>
            </div>

            <div class="pull-quote">
                我們在 aggregation layer 中計算並儲存這些值（μ 和 σ），<br>
                因為我們要將它們套用到列中的所有值。<br><br>
                最終，正規化後的 Input Embedding 已經準備好，<br>
                可以傳遞到 <strong>Self-Attention 層</strong>了。
            </div>

            <h2>💡 關鍵理解</h2>

            <div class="info-box" style="background: linear-gradient(135deg, rgba(255, 215, 0, 0.1) 0%, rgba(255, 215, 0, 0.05) 100%); border-color: var(--gold);">
                <div class="info-box-title" style="color: var(--gold);">
                    <span>🔑</span>
                    <span>重要概念總結</span>
                </div>
                <ol style="margin-left: 20px; line-height: 2;">
                    <li><strong>獨立處理</strong>：每一列（每個 token）獨立正規化</li>
                    <li><strong>統一分布</strong>：所有列都變成平均值 0、標準差 1</li>
                    <li><strong>防止爆炸/消失</strong>：穩定梯度，避免數值過大或過小</li>
                    <li><strong>可學習參數</strong>：γ 和 β 讓模型保有彈性</li>
                    <li><strong>訓練穩定性</strong>：這是深度網絡能有效訓練的關鍵</li>
                </ol>
            </div>

            <h2>🎓 nano-gpt 的具體數值</h2>

            <div class="matrix-box">
                <div class="code-visualization">
                    <pre><span class="code-comment"># nano-gpt Layer Normalization 配置</span>

輸入矩陣形狀：[<span class="code-number">6</span> × <span class="code-number">48</span>]
每列長度（C）：<span class="code-number">48</span>

<span class="code-comment"># 可學習參數</span>
γ (gamma)：[<span class="code-number">48</span>] <span class="code-comment"># 每個位置一個縮放參數</span>
β (beta)： [<span class="code-number">48</span>] <span class="code-comment"># 每個位置一個位移參數</span>

<span class="code-comment"># 總共可學習參數</span>
Layer Norm 參數 = <span class="code-number">48</span> + <span class="code-number">48</span> = <span class="code-number">96</span> 個

<span class="code-comment"># 計算成本</span>
每列的計算：
  - 計算 μ: <span class="code-number">48</span> 次加法 + 1 次除法
  - 計算 σ: <span class="code-number">48</span> 次減法、48 次平方、48 次加法、1 次開根號
  - 標準化: <span class="code-number">48</span> 次減法、48 次除法
  - 縮放位移: <span class="code-number">48</span> 次乘法、48 次加法</pre>
                </div>
            </div>

            <h2>🎬 下一步</h2>

            <p>正規化後的 Input Embedding 矩陣 [6 × 48] 現在準備好了，接下來：</p>
            
            <div class="comparison-grid">
                <div class="comparison-card" style="border-top-color: #daa520;">
                    <div class="card-title">→ 第三章</div>
                    <p><strong>Self-Attention</strong></p>
                    <p>正規化後的矩陣進入 Transformer 的核心機制</p>
                </div>

                <div class="comparison-card" style="border-top-color: var(--purple);">
                    <div class="card-title">→ 第四章</div>
                    <p><strong>Multi-Head Attention</strong></p>
                    <p>平行處理多個注意力視角</p>
                </div>

                <div class="comparison-card" style="border-top-color: #87ceeb;">
                    <div class="card-title">→ 第五章</div>
                    <p><strong>Feed Forward Network</strong></p>
                    <p>每個 token 的獨立處理</p>
                </div>
            </div>

            <h2 style="margin-top: 80px;">🗺️ 回顧架構全貌</h2>

            <div class="info-box" style="background: linear-gradient(135deg, rgba(255, 215, 0, 0.1) 0%, rgba(255, 215, 0, 0.05) 100%); border-color: var(--gold);">
                <div class="info-box-title" style="color: var(--gold);">
                    <span>📍</span>
                    <span>Layer Normalization 在完整架構中的位置</span>
                </div>
                <figure style="margin: 20px 0 0 0;">
                    <img src="../../images/llm_architecture_full_20260105210724.png" alt="LLM 完整架構" style="max-width: 100%; border-radius: 15px; box-shadow: 0 10px 40px rgba(0,0,0,0.2);">
                    <figcaption style="margin-top: 20px; font-size: 1rem; color: #666; line-height: 1.8;">
                        完整架構圖：我們剛剛完成了<span style="color: #ffd700; font-weight: 700;"> Layer Normalization（黃色）</span>，<br>
                        這是進入 Transformer Block 的第一步，<br>
                        接下來將進入 <span style="color: #daa520; font-weight: 700;">Self-Attention（米色）</span>
                    </figcaption>
                </figure>
                <div style="text-align: center; margin-top: 30px;">
                    <a href="../index.html" style="display: inline-block; padding: 15px 40px; background: var(--gold); color: #0a0e27; text-decoration: none; border-radius: 50px; font-weight: 700; transition: transform 0.3s ease;">
                        ← 返回教學首頁查看所有章節
                    </a>
                </div>
            </div>

        </div>
    </div>

    <!-- Chapter Navigation -->
    <div style="background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); padding: 60px 40px; margin-top: 80px;">
        <div style="max-width: 900px; margin: 0 auto;">
            
            <h2 style="font-family: 'Noto Serif TC', serif; font-size: 2rem; color: #0a0e27; text-align: center; margin-bottom: 40px; border: none; padding: 0;">
                🎯 本章重點回顧
            </h2>

            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-bottom: 50px;">
                <div style="background: white; padding: 25px; border-radius: 15px; box-shadow: 0 2px 10px rgba(0,0,0,0.05);">
                    <div style="font-size: 2rem; margin-bottom: 10px;">📊</div>
                    <div style="font-weight: 700; color: #0a0e27; margin-bottom: 8px;">計算均值與標準差</div>
                    <div style="font-size: 0.95rem; color: #666; line-height: 1.6;">對每個 token 的 48 維向量分別正規化</div>
                </div>

                <div style="background: white; padding: 25px; border-radius: 15px; box-shadow: 0 2px 10px rgba(0,0,0,0.05);">
                    <div style="font-size: 2rem; margin-bottom: 10px;">⚖️</div>
                    <div style="font-weight: 700; color: #0a0e27; margin-bottom: 8px;">正規化公式</div>
                    <div style="font-size: 0.95rem; color: #666; line-height: 1.6;">減去均值，除以標準差，確保穩定分佈</div>
                </div>

                <div style="background: white; padding: 25px; border-radius: 15px; box-shadow: 0 2px 10px rgba(0,0,0,0.05);">
                    <div style="font-size: 2rem; margin-bottom: 10px;">🎚️</div>
                    <div style="font-weight: 700; color: #0a0e27; margin-bottom: 8px;">學習參數 γ 和 β</div>
                    <div style="font-size: 0.95rem; color: #666; line-height: 1.6;">可訓練的縮放和偏移參數</div>
                </div>

                <div style="background: white; padding: 25px; border-radius: 15px; box-shadow: 0 2px 10px rgba(0,0,0,0.05);">
                    <div style="font-size: 2rem; margin-bottom: 10px;">🔑</div>
                    <div style="font-weight: 700; color: #0a0e27; margin-bottom: 8px;">訓練穩定性</div>
                    <div style="font-size: 0.95rem; color: #666; line-height: 1.6;">防止梯度消失/爆炸，加速收斂</div>
                </div>
            </div>

            <div style="border-top: 2px solid #dee2e6; padding-top: 40px; margin-top: 40px;">
                <h3 style="font-family: 'Noto Serif TC', serif; font-size: 1.5rem; color: #0a0e27; text-align: center; margin-bottom: 30px;">
                    📚 章節導航
                </h3>

                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 30px;">
                    <a href="01-embedding.html" style="display: flex; align-items: center; gap: 15px; padding: 25px; background: white; border-radius: 15px; text-decoration: none; color: #0a0e27; transition: transform 0.3s ease, box-shadow 0.3s ease; box-shadow: 0 2px 10px rgba(0,0,0,0.05);">
                        <div style="font-size: 2rem;">⬅️</div>
                        <div>
                            <div style="font-size: 0.85rem; color: #666; margin-bottom: 5px;">上一章</div>
                            <div style="font-weight: 700; font-size: 1.1rem;">Embedding</div>
                            <div style="font-size: 0.9rem; color: #999; margin-top: 3px;">Token + Position</div>
                        </div>
                    </a>

                    <a href="03a-self-attention-qkv.html" style="display: flex; align-items: center; justify-content: flex-end; gap: 15px; padding: 25px; background: linear-gradient(135deg, #ffd700 0%, #f0c000 100%); border-radius: 15px; text-decoration: none; color: #0a0e27; transition: transform 0.3s ease, box-shadow 0.3s ease; box-shadow: 0 4px 15px rgba(255, 215, 0, 0.3);">
                        <div style="text-align: right;">
                            <div style="font-size: 0.85rem; opacity: 0.8; margin-bottom: 5px;">下一章</div>
                            <div style="font-weight: 700; font-size: 1.1rem;">Self-Attention: Q/K/V</div>
                            <div style="font-size: 0.9rem; opacity: 0.9; margin-top: 5px;">生成 Query、Key、Value 向量</div>
                        </div>
                        <div style="font-size: 2rem;">➡️</div>
                    </a>
                </div>

                <div style="text-align: center; padding: 20px; background: rgba(255, 215, 0, 0.1); border-radius: 12px; border: 2px dashed var(--gold);">
                    <div style="font-size: 0.95rem; color: #666; line-height: 1.8;">
                        <strong style="color: #0a0e27;">💡 下一步學什麼？</strong><br>
                        Layer Normalization 完成後，正規化的向量準備進入<strong style="color: var(--gold);"> Self-Attention 層</strong>！<br>
                        這是 Transformer 的核心：讓 tokens 之間互相「溝通」。<br>
                        首先要生成 Q (Query)、K (Key)、V (Value) 三種向量。
                    </div>
                </div>
            </div>

        </div>
    </div>

    <!-- Footer Quote -->
    <div class="footer-quote">
        <p class="footer-quote-text">
            「Layer Normalization：<br>
            讓每個 token 的數值分布保持穩定，<br>
            是訓練深度網絡的關鍵。」
        </p>
        <p class="footer-quote-author">— LLM 視覺化教學 · 2026</p>
    </div>

</body>
</html>

