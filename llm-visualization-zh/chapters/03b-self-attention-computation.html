<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第四章：Self-Attention（下）- Attention 計算 | nano-gpt 視覺化教學</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700;900&family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../styles/llm-viz.css">
    
</head>
<body>
    <div class="breadcrumb">
        <a href="../../index.html">🏠 首頁</a>
        <span style="margin: 0 10px; color: #999;">/</span>
        <a href="../index.html">📚 LLM 視覺化教學</a>
        <span style="margin: 0 10px; color: #999;">/</span>
        <span style="color: #666;">第四章：Self-Attention（下）</span>
    </div>

    <!-- Hero Section -->
    <section class="hero-section">
        <div class="hero-overlay"></div>
        <div class="hero-content">
            <div class="nano-gpt-badge">nano-gpt · Attention 計算核心</div>
            <h1 class="hero-title">Self-Attention (下)</h1>
            <p class="hero-subtitle">從相似度到輸出：Attention 的完整計算</p>
            <p style="font-size: 1rem; color: var(--silver); margin-top: 20px;">
                Dot Product · Scaling · Softmax · Weighted Sum
            </p>
        </div>
    </section>

    <!-- Main Content -->
    <div class="story-container">
        <div class="content-wrapper">
            
            <div class="story-lead">
                接續上一章，我們已經為每個 token 生成了 Q、K、V 向量。<br><br>
                現在，讓我們學習如何使用這些向量：<br>
                用 <strong>Query</strong> 尋找相關的 <strong>Keys</strong>，<br>
                然後提取對應的 <strong>Values</strong>。
            </div>

            <h2>🔍 Self-Attention 的「字典查詢」</h2>

            <p>回顧軟體中的字典查詢：</p>

            <div class="matrix-box" style="background: linear-gradient(135deg, #2d3748 0%, #1a202c 100%);">
                <div class="matrix-title">軟體 vs Self-Attention</div>
                <div class="comparison-grid" style="margin: 0;">
                    <div class="comparison-card" style="background: rgba(255,255,255,0.1); color: white; border-top-color: var(--matrix-green);">
                        <div class="card-title" style="color: var(--matrix-green);">傳統字典</div>
                        <div class="code-visualization" style="margin: 15px 0 0 0; padding: 20px;">
                            <pre><span class="code-comment"># 精確匹配</span>
table = {
  <span class="code-string">"key1"</span>: <span class="code-string">"value1"</span>
}
result = table[<span class="code-string">"key1"</span>]
<span class="code-comment"># 返回單一值</span></pre>
                        </div>
                    </div>

                    <div class="comparison-card" style="background: rgba(255,255,255,0.1); color: white; border-top-color: var(--attention-brown);">
                        <div class="card-title" style="color: var(--attention-brown);">Self-Attention</div>
                        <div class="code-visualization" style="margin: 15px 0 0 0; padding: 20px;">
                            <pre><span class="code-comment"># 相似度匹配</span>
scores = Q @ K.T
weights = softmax(scores)
result = weights @ V
<span class="code-comment"># 返回加權組合</span></pre>
                        </div>
                    </div>
                </div>
            </div>

            <h2>📍 完整的 Attention 計算流程</h2>

            <figure>
                <img src="../../images/attention_calculation_20260105212752.png" alt="Attention 計算完整流程">
                <figcaption>
                    視覺化：從 Q/K/V 向量到最終輸出的完整計算過程
                </figcaption>
            </figure>

            <h3>步驟 1：計算相似度（Dot Product）</h3>

            <div class="step-box">
                <div style="display: flex; align-items: center; margin-bottom: 20px;">
                    <span class="step-number">1</span>
                    <span class="step-title">Query 與所有 Keys 的點積</span>
                </div>
                <p>我們用當前位置的 <strong>Q 向量</strong>與過去所有位置的 <strong>K 向量</strong>做點積。</p>
                
                <p style="margin-top: 20px;"><strong>為什麼用點積？</strong></p>
                <p>點積能夠測量兩個向量的相似度：</p>
                <ul style="margin-left: 20px; line-height: 2; margin-top: 10px;">
                    <li>如果方向相似 → 點積<strong>較大</strong></li>
                    <li>如果方向相反 → 點積<strong>較小或負值</strong></li>
                </ul>
            </div>

            <div class="matrix-box">
                <div class="matrix-title">具體範例：t = 5 的位置</div>
                <div class="code-visualization">
                    <pre><span class="code-comment"># 假設我們在第 6 個 token (t = 5)</span>

Q[5] = [<span class="code-number">16</span> 個浮點數]  <span class="code-comment"># 當前位置的 Query</span>

<span class="code-comment"># 只能看到過去（因果 self-attention）</span>
K[0] = [<span class="code-number">16</span> 個浮點數]  <span class="code-comment"># 第 1 個位置的 Key</span>
K[1] = [<span class="code-number">16</span> 個浮點數]  <span class="code-comment"># 第 2 個位置的 Key</span>
K[2] = [<span class="code-number">16</span> 個浮點數]
K[3] = [<span class="code-number">16</span> 個浮點數]
K[4] = [<span class="code-number">16</span> 個浮點數]
K[5] = [<span class="code-number">16</span> 個浮點數]  <span class="code-comment"># 自己的 Key</span>

<span class="code-comment"># 計算與每個 Key 的相似度</span>
score[5][0] = Q[5] · K[0]  <span class="code-comment"># = 0.32</span>
score[5][1] = Q[5] · K[1]  <span class="code-comment"># = -0.15</span>
score[5][2] = Q[5] · K[2]  <span class="code-comment"># = 0.89</span>
score[5][3] = Q[5] · K[3]  <span class="code-comment"># = 0.56</span>
score[5][4] = Q[5] · K[4]  <span class="code-comment"># = 1.24</span>
score[5][5] = Q[5] · K[5]  <span class="code-comment"># = 0.78 (與自己)</span>

<span class="code-comment"># 這些分數存儲在 Attention Matrix 的第 5 列</span></pre>
                </div>
            </div>

            <div class="info-box">
                <div class="info-box-title">
                    <span>🔒</span>
                    <span>因果 Self-Attention (Causal Self-Attention)</span>
                </div>
                <p>注意我們只使用<strong>過去和當前</strong>的 Keys，不使用未來的 Keys。</p>
                <p style="margin-top: 15px;">這被稱為<strong>因果</strong>或<strong>遮罩</strong> Self-Attention，確保模型不能「看到未來」。</p>
                <p style="margin-top: 15px; font-style: italic; color: #666;">這對於生成任務（如文字生成）非常重要：模型只能根據已生成的內容預測下一個字。</p>
            </div>

            <h3>步驟 2：Scaling（縮放）</h3>

            <div class="step-box">
                <div style="display: flex; align-items: center; margin-bottom: 20px;">
                    <span class="step-number">2</span>
                    <span class="step-title">除以 √A</span>
                </div>
                <p>在點積後，我們將結果除以 <code>√A</code>，其中 <code>A</code> 是 Q/K/V 向量的長度。</p>
            </div>

            <div class="formula-box">
                <div class="formula-title">Scaled Dot Product</div>
                <div class="formula">
                    scaled_score = score / √A
                </div>
                <div class="formula-desc">
                    在 nano-gpt 中：A = 16（head 維度）<br>
                    所以我們除以 √16 = 4
                </div>
            </div>

            <div class="info-box" style="background: linear-gradient(135deg, rgba(218, 165, 32, 0.1) 0%, rgba(218, 165, 32, 0.05) 100%); border-color: var(--attention-brown);">
                <div class="info-box-title" style="color: var(--attention-brown);">
                    <span>🎯</span>
                    <span>為什麼要 Scaling？</span>
                </div>
                <p>防止點積結果太大，導致 Softmax 梯度消失。</p>
                <p style="margin-top: 15px;"><strong>數學直覺：</strong></p>
                <ul style="margin-left: 20px; line-height: 2; margin-top: 10px;">
                    <li>向量越長，點積結果越大</li>
                    <li>大的數值進入 Softmax 會導致極端的機率（接近 0 或 1）</li>
                    <li>Scaling 讓數值保持在合理範圍，Softmax 更穩定</li>
                </ul>
            </div>

            <div class="matrix-box">
                <div class="matrix-title">Scaling 範例</div>
                <div class="code-visualization">
                    <pre><span class="code-comment"># 原始分數</span>
scores = [<span class="code-number">0.32</span>, <span class="code-number">-0.15</span>, <span class="code-number">0.89</span>, <span class="code-number">0.56</span>, <span class="code-number">1.24</span>, <span class="code-number">0.78</span>]

<span class="code-comment"># Scaling</span>
A = <span class="code-number">16</span>  <span class="code-comment"># Q/K/V 向量長度</span>
scale_factor = sqrt(A) = <span class="code-number">4.0</span>

scaled_scores = [
    <span class="code-number">0.32</span>/<span class="code-number">4</span>, <span class="code-number">-0.15</span>/<span class="code-number">4</span>, <span class="code-number">0.89</span>/<span class="code-number">4</span>, 
    <span class="code-number">0.56</span>/<span class="code-number">4</span>, <span class="code-number">1.24</span>/<span class="code-number">4</span>, <span class="code-number">0.78</span>/<span class="code-number">4</span>
]
= [<span class="code-number">0.08</span>, <span class="code-number">-0.04</span>, <span class="code-number">0.22</span>, <span class="code-number">0.14</span>, <span class="code-number">0.31</span>, <span class="code-number">0.20</span>]</pre>
                </div>
            </div>

            <h3>步驟 3：Softmax（正規化為機率）</h3>

            <div class="step-box">
                <div style="display: flex; align-items: center; margin-bottom: 20px;">
                    <span class="step-number">3</span>
                    <span class="step-title">將分數轉換為機率分佈</span>
                </div>
                <p>Softmax 將任意數值轉換為總和為 1 的機率分佈。</p>
            </div>

            <div class="formula-box">
                <div class="formula-title">Softmax 公式</div>
                <div class="formula" style="font-size: 1.5rem;">
                    softmax(xᵢ) = exp(xᵢ) / Σ exp(xⱼ)
                </div>
                <div class="formula-desc">
                    對每個元素取指數，然後除以所有元素指數的總和
                </div>
            </div>

            <div class="matrix-box">
                <div class="matrix-title">Softmax 範例</div>
                <div class="code-visualization">
                    <pre><span class="code-comment"># 縮放後的分數</span>
scaled = [<span class="code-number">0.08</span>, <span class="code-number">-0.04</span>, <span class="code-number">0.22</span>, <span class="code-number">0.14</span>, <span class="code-number">0.31</span>, <span class="code-number">0.20</span>]

<span class="code-comment"># 步驟 1: 對每個元素取指數</span>
exp_values = [
    exp(<span class="code-number">0.08</span>)=<span class="code-number">1.08</span>, exp(<span class="code-number">-0.04</span>)=<span class="code-number">0.96</span>, exp(<span class="code-number">0.22</span>)=<span class="code-number">1.25</span>,
    exp(<span class="code-number">0.14</span>)=<span class="code-number">1.15</span>, exp(<span class="code-number">0.31</span>)=<span class="code-number">1.36</span>, exp(<span class="code-number">0.20</span>)=<span class="code-number">1.22</span>
]

<span class="code-comment"># 步驟 2: 計算總和</span>
sum_exp = <span class="code-number">1.08</span> + <span class="code-number">0.96</span> + <span class="code-number">1.25</span> + <span class="code-number">1.15</span> + <span class="code-number">1.36</span> + <span class="code-number">1.22</span> = <span class="code-number">7.02</span>

<span class="code-comment"># 步驟 3: 正規化</span>
attention_weights = [
    <span class="code-number">1.08</span>/<span class="code-number">7.02</span>=<span class="code-number">0.15</span>, <span class="code-number">0.96</span>/<span class="code-number">7.02</span>=<span class="code-number">0.14</span>, <span class="code-number">1.25</span>/<span class="code-number">7.02</span>=<span class="code-number">0.18</span>,
    <span class="code-number">1.15</span>/<span class="code-number">7.02</span>=<span class="code-number">0.16</span>, <span class="code-number">1.36</span>/<span class="code-number">7.02</span>=<span class="code-number">0.19</span>, <span class="code-number">1.22</span>/<span class="code-number">7.02</span>=<span class="code-number">0.17</span>
]

<span class="code-comment"># 總和 = 1.0 ✅</span>
<span class="code-comment"># 這些就是 Attention Matrix 中的權重！</span></pre>
                </div>
            </div>

            <div class="attention-visual">
                <h3 style="text-align: center; color: #0a0e27; margin-bottom: 30px;">📊 Attention Matrix 視覺化</h3>
                <p style="text-align: center; color: #666; margin-bottom: 20px;">顏色深淺代表注意力權重（越深 = 越重要）</p>
                <div class="attention-grid">
                    <div class="attention-cell" style="background: rgba(218, 165, 32, 0.3);">0.15</div>
                    <div class="attention-cell" style="background: rgba(218, 165, 32, 0.3);">0.14</div>
                    <div class="attention-cell" style="background: rgba(218, 165, 32, 0.5);">0.18</div>
                    <div class="attention-cell" style="background: rgba(218, 165, 32, 0.4);">0.16</div>
                    <div class="attention-cell" style="background: rgba(218, 165, 32, 0.6);">0.19</div>
                    <div class="attention-cell" style="background: rgba(218, 165, 32, 0.4);">0.17</div>
                </div>
                <p style="text-align: center; color: #666; margin-top: 20px; font-size: 0.95rem;">
                    這列代表位置 5 對過去所有位置（0-5）的注意力分配<br>
                    位置 4 獲得最高注意力 (0.19)
                </p>
            </div>

            <h3>步驟 4：加權求和（Weighted Sum）</h3>

            <div class="step-box">
                <div style="display: flex; align-items: center; margin-bottom: 20px;">
                    <span class="step-number">4</span>
                    <span class="step-title">使用權重提取 Values</span>
                </div>
                <p>現在我們有了注意力權重，最後一步是用這些權重對 <strong>V 向量</strong>進行加權平均。</p>
            </div>

            <div class="matrix-box">
                <div class="matrix-title">加權求和過程</div>
                <div class="code-visualization">
                    <pre><span class="code-comment"># 注意力權重（來自 Softmax）</span>
weights = [<span class="code-number">0.15</span>, <span class="code-number">0.14</span>, <span class="code-number">0.18</span>, <span class="code-number">0.16</span>, <span class="code-number">0.19</span>, <span class="code-number">0.17</span>]

<span class="code-comment"># 所有位置的 V 向量（每個都是 [16]）</span>
V[0] = [v₀₀, v₀₁, ..., v₀₁₅]
V[1] = [v₁₀, v₁₁, ..., v₁₁₅]
...
V[5] = [v₅₀, v₅₁, ..., v₅₁₅]

<span class="code-comment"># 輸出 = 加權組合</span>
output = <span class="code-number">0.15</span>*V[0] + <span class="code-number">0.14</span>*V[1] + <span class="code-number">0.18</span>*V[2] 
       + <span class="code-number">0.16</span>*V[3] + <span class="code-number">0.19</span>*V[4] + <span class="code-number">0.17</span>*V[5]

<span class="code-comment"># 結果：一個新的向量 [16]</span>
<span class="code-comment"># 這個向量融合了所有相關位置的資訊！</span></pre>
                </div>
            </div>

            <div class="pull-quote">
                輸出向量會被權重<strong>最高</strong>的 V 向量主導。<br><br>
                這就是 Self-Attention 的核心：<br>
                每個位置能從其他相關位置「提取」資訊。
            </div>

            <h2>🔄 對所有列重複此過程</h2>

            <p>我們剛剛詳細說明了<strong>一個位置</strong>（t = 5）的計算過程。實際上，我們需要對<strong>所有 6 個位置</strong>重複這個過程。</p>

            <div class="matrix-box">
                <div class="matrix-title">完整的 Self-Attention 輸出</div>
                <div class="code-visualization">
                    <pre><span class="code-comment"># 輸入：[6 × 48] 正規化矩陣</span>
<span class="code-comment"># Q/K/V 矩陣：[6 × 16] (每個)</span>

<span class="code-comment"># 對每個位置計算輸出</span>
<span class="code-keyword">for</span> t <span class="code-keyword">in</span> <span class="code-keyword">range</span>(<span class="code-number">6</span>):
    <span class="code-comment"># 1. 計算與所有過去 Keys 的相似度</span>
    scores = [Q[t] · K[i] <span class="code-keyword">for</span> i <span class="code-keyword">in</span> <span class="code-keyword">range</span>(t+<span class="code-number">1</span>)]
    
    <span class="code-comment"># 2. Scaling</span>
    scaled = scores / sqrt(<span class="code-number">16</span>)
    
    <span class="code-comment"># 3. Softmax</span>
    attention_weights[t] = softmax(scaled)
    
    <span class="code-comment"># 4. 加權求和</span>
    output[t] = <span class="code-keyword">sum</span>(attention_weights[t][i] * V[i] 
                    <span class="code-keyword">for</span> i <span class="code-keyword">in</span> <span class="code-keyword">range</span>(t+<span class="code-number">1</span>))

<span class="code-comment"># 最終輸出：[6 × 16] 矩陣</span>
<span class="code-comment"># 每列都融合了過去相關位置的資訊</span></pre>
                </div>
            </div>

            <h2>💡 Self-Attention 的目標</h2>

            <div class="info-box" style="background: linear-gradient(135deg, rgba(218, 165, 32, 0.15) 0%, rgba(218, 165, 32, 0.08) 100%); border-color: var(--attention-brown);">
                <div class="info-box-title" style="color: var(--attention-brown);">
                    <span>🎯</span>
                    <span>Self-Attention 的核心目標</span>
                </div>
                <p style="font-size: 1.2rem; line-height: 2;">
                    <strong>每一列（token）想要從其他列找到相關資訊並提取其值。</strong>
                </p>
                <p style="margin-top: 20px;">它通過以下方式實現：</p>
                <ul style="margin-left: 20px; line-height: 2; margin-top: 15px;">
                    <li>用自己的 <strong>Query</strong> 與其他的 <strong>Keys</strong> 比較（相似度）</li>
                    <li>根據相似度分配注意力權重（Softmax）</li>
                    <li>用權重提取對應的 <strong>Values</strong>（加權求和）</li>
                    <li><strong>限制</strong>：只能看到過去（因果遮罩）</li>
                </ul>
            </div>

            <h2>🎓 nano-gpt 的參數統計</h2>

            <div class="matrix-box">
                <div class="code-visualization">
                    <pre><span class="code-comment"># Single Head Self-Attention 總結</span>

<span class="code-comment">## 輸入/輸出形狀</span>
輸入：[<span class="code-number">6</span> × <span class="code-number">48</span>] 正規化矩陣
Q/K/V：每個都是 [<span class="code-number">6</span> × <span class="code-number">16</span>]
Attention Matrix：[<span class="code-number">6</span> × <span class="code-number">6</span>] (實際是下三角)
輸出：[<span class="code-number">6</span> × <span class="code-number">16</span>]

<span class="code-comment">## 參數數量（單個 head）</span>
Q_weights: [<span class="code-number">16</span> × <span class="code-number">48</span>] + [<span class="code-number">16</span>] = <span class="code-number">784</span>
K_weights: [<span class="code-number">16</span> × <span class="code-number">48</span>] + [<span class="code-number">16</span>] = <span class="code-number">784</span>
V_weights: [<span class="code-number">16</span> × <span class="code-number">48</span>] + [<span class="code-number">16</span>] = <span class="code-number">784</span>
────────────────────────────────────
總計：<span class="code-number">2,352</span> 個參數（單個 head）

<span class="code-comment">## 關鍵超參數</span>
C (模型維度)：<span class="code-number">48</span>
d_k (head 維度)：<span class="code-number">16</span>
T (序列長度)：<span class="code-number">6</span>
縮放因子：√<span class="code-number">16</span> = <span class="code-number">4</span></pre>
                </div>
            </div>

            <h2>🚀 下一步：Multi-Head Attention</h2>

            <p>我們剛剛探索了<strong>單個 head</strong> 的 Self-Attention。</p>

            <p>實際上，Transformer 會平行運行<strong>多個 heads</strong>，每個 head 學習不同的關注模式。</p>

            <div class="comparison-grid">
                <div class="comparison-card" style="border-top-color: var(--attention-brown);">
                    <div class="card-title">→ 第五章</div>
                    <p><strong>Multi-Head Attention</strong></p>
                    <p>多個注意力視角的平行處理</p>
                </div>

                <div class="comparison-card" style="border-top-color: #87ceeb;">
                    <div class="card-title">→ 第六章</div>
                    <p><strong>Feed Forward Network</strong></p>
                    <p>Transformer Block 的第二組件</p>
                </div>

                <div class="comparison-card" style="border-top-color: var(--purple);">
                    <div class="card-title">→ 第七章</div>
                    <p><strong>Residual Connection</strong></p>
                    <p>穩定深層網絡訓練的技巧</p>
                </div>
            </div>

            <h2 style="margin-top: 80px;">🗺️ 回顧架構全貌</h2>

            <div class="info-box" style="background: linear-gradient(135deg, rgba(218, 165, 32, 0.1) 0%, rgba(218, 165, 32, 0.05) 100%); border-color: var(--attention-brown);">
                <div class="info-box-title" style="color: var(--attention-brown);">
                    <span>📍</span>
                    <span>Self-Attention 完整計算在架構中的位置</span>
                </div>
                <figure style="margin: 20px 0 0 0;">
                    <img src="../../images/llm_architecture_full_20260105210724.png" alt="LLM 完整架構" style="max-width: 100%; border-radius: 15px; box-shadow: 0 10px 40px rgba(0,0,0,0.2);">
                    <figcaption style="margin-top: 20px; font-size: 1rem; color: #666; line-height: 1.8;">
                        完整架構圖：我們剛剛完成了 <span style="color: #daa520; font-weight: 700;">Self-Attention 的完整計算過程</span><br>
                        （從 Q/K/V 生成到最終輸出）
                    </figcaption>
                </figure>
                <div style="text-align: center; margin-top: 30px;">
                    <a href="../index.html" style="display: inline-block; padding: 15px 40px; background: var(--attention-brown); color: #0a0e27; text-decoration: none; border-radius: 50px; font-weight: 700; transition: transform 0.3s ease;">
                        ← 返回教學首頁查看所有章節
                    </a>
                </div>
            </div>

        </div>
    </div>

    <!-- Chapter Navigation -->
    <div style="background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); padding: 60px 40px; margin-top: 80px;">
        <div style="max-width: 900px; margin: 0 auto;">
            
            <h2 style="font-family: 'Noto Serif TC', serif; font-size: 2rem; color: #0a0e27; text-align: center; margin-bottom: 40px; border: none; padding: 0;">
                🎯 本章重點回顧
            </h2>

            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(230px, 1fr)); gap: 20px; margin-bottom: 50px;">
                <div style="background: white; padding: 25px; border-radius: 15px; box-shadow: 0 2px 10px rgba(0,0,0,0.05);">
                    <div style="font-size: 2rem; margin-bottom: 10px;">⚡</div>
                    <div style="font-weight: 700; color: #0a0e27; margin-bottom: 8px;">Dot Product 相似度</div>
                    <div style="font-size: 0.95rem; color: #666; line-height: 1.6;">Q·K 計算 tokens 之間的相關性</div>
                </div>

                <div style="background: white; padding: 25px; border-radius: 15px; box-shadow: 0 2px 10px rgba(0,0,0,0.05);">
                    <div style="font-size: 2rem; margin-bottom: 10px;">📏</div>
                    <div style="font-weight: 700; color: #0a0e27; margin-bottom: 8px;">Scaling 縮放</div>
                    <div style="font-size: 0.95rem; color: #666; line-height: 1.6;">除以 √head_dim 穩定數值</div>
                </div>

                <div style="background: white; padding: 25px; border-radius: 15px; box-shadow: 0 2px 10px rgba(0,0,0,0.05);">
                    <div style="font-size: 2rem; margin-bottom: 10px;">🎯</div>
                    <div style="font-weight: 700; color: #0a0e27; margin-bottom: 8px;">Softmax 正規化</div>
                    <div style="font-size: 0.95rem; color: #666; line-height: 1.6;">轉換為機率分佈（總和為 1）</div>
                </div>

                <div style="background: white; padding: 25px; border-radius: 15px; box-shadow: 0 2px 10px rgba(0,0,0,0.05);">
                    <div style="font-size: 2rem; margin-bottom: 10px;">➕</div>
                    <div style="font-weight: 700; color: #0a0e27; margin-bottom: 8px;">加權和輸出</div>
                    <div style="font-size: 0.95rem; color: #666; line-height: 1.6;">用注意力權重加權 V 向量</div>
                </div>
            </div>

            <div style="border-top: 2px solid #dee2e6; padding-top: 40px; margin-top: 40px;">
                <h3 style="font-family: 'Noto Serif TC', serif; font-size: 1.5rem; color: #0a0e27; text-align: center; margin-bottom: 30px;">
                    📚 章節導航
                </h3>

                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 30px;">
                    <a href="03a-self-attention-qkv.html" style="display: flex; align-items: center; gap: 15px; padding: 25px; background: white; border-radius: 15px; text-decoration: none; color: #0a0e27; transition: transform 0.3s ease, box-shadow 0.3s ease; box-shadow: 0 2px 10px rgba(0,0,0,0.05);">
                        <div style="font-size: 2rem;">⬅️</div>
                        <div>
                            <div style="font-size: 0.85rem; color: #666; margin-bottom: 5px;">上一章</div>
                            <div style="font-weight: 700; font-size: 1.1rem;">Self-Attention: Q/K/V</div>
                            <div style="font-size: 0.9rem; color: #999; margin-top: 3px;">Q/K/V 向量生成</div>
                        </div>
                    </a>

                    <a href="04-multihead-projection-residual.html" style="display: flex; align-items: center; justify-content: flex-end; gap: 15px; padding: 25px; background: linear-gradient(135deg, #ffd700 0%, #f0c000 100%); border-radius: 15px; text-decoration: none; color: #0a0e27; transition: transform 0.3s ease, box-shadow 0.3s ease; box-shadow: 0 4px 15px rgba(255, 215, 0, 0.3);">
                        <div style="text-align: right;">
                            <div style="font-size: 0.85rem; opacity: 0.8; margin-bottom: 5px;">下一章</div>
                            <div style="font-weight: 700; font-size: 1.1rem;">Multi-Head & Projection</div>
                            <div style="font-size: 0.9rem; opacity: 0.9; margin-top: 5px;">合併多個 heads</div>
                        </div>
                        <div style="font-size: 2rem;">➡️</div>
                    </a>
                </div>

                <div style="text-align: center; padding: 20px; background: rgba(255, 215, 0, 0.1); border-radius: 12px; border: 2px dashed var(--gold);">
                    <div style="font-size: 0.95rem; color: #666; line-height: 1.8;">
                        <strong style="color: #0a0e27;">💡 下一步學什麼？</strong><br>
                        單個 head 的計算完成！但 Multi-Head Attention 有<strong style="color: var(--gold);">多個平行的 heads</strong>，<br>
                        需要合併它們的輸出並進行 Projection，形成最終的 Self-Attention 輸出。
                    </div>
                </div>
            </div>

        </div>
    </div>

    <!-- Footer Quote -->
    <div class="footer-quote">
        <p class="footer-quote-text">
            「Self-Attention：讓每個 token<br>
            都能聽到其他 tokens 的聲音。」
        </p>
        <p class="footer-quote-author">— LLM 視覺化教學 · 2026</p>
    </div>

</body>
</html>

