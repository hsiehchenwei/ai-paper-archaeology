<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬ 3 é :é è¨“ç·´ä»»å‹™ - BERT è«–æ–‡æ·±åº¦è§£æ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <h1>ğŸ“ ç¬¬ 3 é :é è¨“ç·´ä»»å‹™è©³è§£</h1>

        <h2>ä»»å‹™ 1:Masked Language Model (MLM)</h2>

        <div class="text-pair">
            <div class="original-text">
                In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a "masked LM" (MLM). In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random.
            </div>
            <div class="translation">
                ç‚ºäº†è¨“ç·´æ·±åº¦é›™å‘è¡¨ç¤º,æˆ‘å€‘ç°¡å–®åœ°éš¨æ©Ÿé®è”½ä¸€å®šæ¯”ä¾‹çš„è¼¸å…¥æ¨™è¨˜,ç„¶å¾Œé æ¸¬é€™äº›è¢«é®è”½çš„æ¨™è¨˜ã€‚æˆ‘å€‘å°‡æ­¤ç¨‹åºç¨±ç‚ºã€Œé®è”½ LMã€(MLM)ã€‚åœ¨æˆ‘å€‘æ‰€æœ‰çš„å¯¦é©—ä¸­,æˆ‘å€‘éš¨æ©Ÿé®è”½æ¯å€‹åºåˆ—ä¸­ 15% çš„ WordPiece æ¨™è¨˜ã€‚
            </div>
        </div>

        <div class="key-concept">
            <h4>ğŸ¯ MLM æ ¸å¿ƒæ©Ÿåˆ¶</h4>
            <p><strong>æ­¥é©Ÿ:</strong></p>
            <ol>
                <li>éš¨æ©Ÿé¸æ“‡ 15% çš„ tokens</li>
                <li>å°‡é¸ä¸­çš„ tokens æ›¿æ›æˆ [MASK]</li>
                <li>è®“æ¨¡å‹é æ¸¬åŸå§‹çš„ token</li>
            </ol>

            <pre><code>åŸå¥: "my dog is hairy"
é®è”½: "my dog is [MASK]"
é æ¸¬: "hairy"</code></pre>
        </div>

        <div class="problem">
            <h4>âš ï¸ å•é¡Œ:[MASK] åªåœ¨é è¨“ç·´å‡ºç¾!</h4>
            <p>é è¨“ç·´æ™‚:[MASK] åˆ°è™•éƒ½æ˜¯</p>
            <p>å¾®èª¿æ™‚:[MASK] å®Œå…¨ä¸å‡ºç¾</p>
            <p><strong>â†’ Pre-training å’Œ Fine-tuning çš„ Mismatch!</strong></p>
        </div>

        <div class="solution">
            <h4>âœ… BERT çš„è°æ˜è§£æ³•</h4>
            <p>é¸ä¸­ 15% çš„ tokens å¾Œ,ä¸æ˜¯å…¨éƒ¨æ›æˆ [MASK]:</p>
            <ul>
                <li><strong>80%</strong>: æ›æˆ [MASK]</li>
                <li><strong>10%</strong>: æ›æˆéš¨æ©Ÿ token</li>
                <li><strong>10%</strong>: ä¿æŒä¸è®Š</li>
            </ul>

            <h5>ç¯„ä¾‹</h5>
            <pre><code>åŸå¥: "my dog is hairy"

å¦‚æœé¸ä¸­ "hairy":
- 80% æ©Ÿç‡: "my dog is [MASK]"
- 10% æ©Ÿç‡: "my dog is apple"  (éš¨æ©Ÿå­—)
- 10% æ©Ÿç‡: "my dog is hairy"  (ä¸è®Š)</code></pre>

            <p><strong>å¥½è™•:</strong></p>
            <ul>
                <li>æ¸›å°‘ pre-training å’Œ fine-tuning çš„å·®ç•°</li>
                <li>æ¨¡å‹å­¸æœƒå°ä»»ä½• token éƒ½ä¿æŒè­¦è¦º</li>
            </ul>
        </div>

        <div class="analogy">
            <h4>ğŸ¤– AI é«”é©—é€£çµ:ç‚ºä»€éº¼å¡«ç©ºé€™éº¼æœ‰æ•ˆ?</h4>
            <p><strong>ä½ æœ‰æ²’æœ‰æƒ³é...</strong></p>
            <p>ç‚ºä»€éº¼åšã€Œå¡«ç©ºé¡Œã€æ¯”ã€Œä½œæ–‡é¡Œã€æ›´èƒ½æ¸¬è©¦ç†è§£?</p>

            <p><strong>ä½œæ–‡é¡Œ (GPT æ–¹å¼)</strong>:</p>
            <pre><code>"è«‹çºŒå¯«: The cat sat on the"
â†’ ä½ å¯ä»¥äº‚å¯«,ä¸ä¸€å®šç†è§£å‰æ–‡</code></pre>

            <p><strong>å¡«ç©ºé¡Œ (BERT æ–¹å¼)</strong>:</p>
            <pre><code>"The cat ___ on the mat"
â†’ ä½ å¿…é ˆçœŸçš„ç†è§£å¥æ„æ‰èƒ½å¡«å°!</code></pre>

            <p><strong>é€™å°±æ˜¯ç‚ºä»€éº¼ BERT çš„ã€Œç†è§£èƒ½åŠ›ã€æ›´å¼·!</strong></p>
        </div>

        <h2>ä»»å‹™ 2:Next Sentence Prediction (NSP)</h2>

        <div class="text-pair">
            <div class="original-text">
                Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task.
            </div>
            <div class="translation">
                è¨±å¤šé‡è¦çš„ä¸‹æ¸¸ä»»å‹™,å¦‚å•ç­” (QA) å’Œè‡ªç„¶èªè¨€æ¨ç† (NLI),éƒ½æ˜¯åŸºæ–¼ç†è§£å…©å€‹å¥å­ä¹‹é–“çš„é—œä¿‚ã€‚ç‚ºäº†è¨“ç·´ä¸€å€‹ç†è§£å¥å­é—œä¿‚çš„æ¨¡å‹,æˆ‘å€‘å°äºŒå…ƒåŒ–çš„ä¸‹ä¸€å¥é æ¸¬ä»»å‹™é€²è¡Œé è¨“ç·´ã€‚
            </div>
        </div>

        <div class="key-concept">
            <h4>ğŸ¯ NSP æ©Ÿåˆ¶</h4>
            <p><strong>ä»»å‹™:</strong>åˆ¤æ–·å¥å­ B æ˜¯å¦æ˜¯å¥å­ A çš„ã€Œä¸‹ä¸€å¥ã€</p>

            <h5>è¨“ç·´è³‡æ–™ç”Ÿæˆ:</h5>
            <ul>
                <li><strong>50%</strong>: B æ˜¯ A çš„çœŸå¯¦ä¸‹ä¸€å¥ (IsNext)</li>
                <li><strong>50%</strong>: B æ˜¯éš¨æ©ŸæŠ½çš„å¥å­ (NotNext)</li>
            </ul>

            <h5>ç¯„ä¾‹</h5>
            <pre><code>âœ“ IsNext:
A: "The man went to the store."
B: "He bought a gallon of milk."

âœ— NotNext:
A: "The man went to the store."
B: "Penguins are flightless birds."</code></pre>
        </div>

        <div class="explanation">
            <h4>ğŸ” ç‚ºä»€éº¼éœ€è¦ NSP?</h4>
            <p><strong>å•ç­”ä»»å‹™çš„éœ€æ±‚:</strong></p>
            <pre><code>å•é¡Œ: "What did Mary buy?"
æ–‡ç« : "Mary went to the store. She bought milk."

æ¨¡å‹éœ€è¦ç†è§£:
1. "She" æŒ‡çš„æ˜¯ "Mary" (ä»£åè©æ¶ˆè§£)
2. ç¬¬äºŒå¥æ˜¯ç¬¬ä¸€å¥çš„å»¶çºŒ (å¥å­é—œä¿‚)</code></pre>

            <p>NSP ä»»å‹™å¼·è¿«æ¨¡å‹å­¸ç¿’å¥å­ä¹‹é–“çš„é€£è²«æ€§!</p>
        </div>

        <div class="analogy">
            <h4>ğŸ’¡ ç”Ÿæ´»é¡æ¯”:å°è©±ç†è§£</h4>
            <p><strong>æƒ…å¢ƒ:åˆ¤æ–·å°è©±æ˜¯å¦åˆç†</strong></p>

            <p>âœ“ åˆç†å°è©±:</p>
            <blockquote>
                A: ã€Œä»Šå¤©å¤©æ°£å¥½å¥½!ã€<br>
                B: ã€Œå°å•Š,æˆ‘å€‘å»å…¬åœ’å§!ã€
            </blockquote>

            <p>âœ— ä¸åˆç†å°è©±:</p>
            <blockquote>
                A: ã€Œä»Šå¤©å¤©æ°£å¥½å¥½!ã€<br>
                B: ã€Œä¼éµæ˜¯ä¸æœƒé£›çš„é³¥ã€‚ã€
            </blockquote>

            <p>NSP è¨“ç·´è®“ BERT å­¸æœƒåˆ¤æ–·ã€Œé€™å…©å¥è©±æœ‰æ²’æœ‰é—œä¿‚ã€!</p>
        </div>

        <h2>ğŸ“š é è¨“ç·´è³‡æ–™</h2>

        <div class="key-concept">
            <h4>è¨“ç·´èªæ–™</h4>
            <ul>
                <li><strong>BooksCorpus</strong>: 800M è©</li>
                <li><strong>English Wikipedia</strong>: 2,500M è©</li>
                <li><strong>ç¸½è¨ˆ</strong>: ç´„ 33 å„„è©</li>
            </ul>

            <p><strong>ç‚ºä»€éº¼ç”¨æ›¸ç±å’Œ Wikipedia?</strong></p>
            <ul>
                <li>å“è³ªé«˜ã€å…§å®¹é€£è²«</li>
                <li>éœ€è¦ã€Œæ–‡æª”ç´šã€èªæ–™,ä¸æ˜¯å–®ç¨çš„å¥å­</li>
                <li>æ‰èƒ½è¨“ç·´ NSP ä»»å‹™</li>
            </ul>
        </div>

        <div class="nav-bar">
            <a href="02-bert-architecture.html" class="nav-btn">â† ä¸Šä¸€é </a>
            <a href="index.html" class="nav-btn">ğŸ“‘ ç›®éŒ„</a>
            <a href="04-fine-tuning-and-results.html" class="nav-btn primary">ä¸‹ä¸€é  â†’ å¾®èª¿èˆ‡çµæœ</a>
        </div>
    </div>
</body>
</html>

