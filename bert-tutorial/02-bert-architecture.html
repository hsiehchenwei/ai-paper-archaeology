<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬ 2 é :BERT æ¶æ§‹è©³è§£ - BERT è«–æ–‡æ·±åº¦è§£æ</title>
    <link rel="stylesheet" href="../transformer-tutorial/style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <h1>âš™ï¸ ç¬¬ 2 é :BERT æ¶æ§‹è©³è§£</h1>

        <h2>ğŸ—ï¸ æ¨¡å‹æ¶æ§‹:Transformer Encoder</h2>

        <div class="text-pair">
            <div class="original-text">
                BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017). Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture.
            </div>
            <div class="translation">
                BERT çš„æ¨¡å‹æ¶æ§‹æ˜¯ä¸€å€‹å¤šå±¤é›™å‘ Transformer ç·¨ç¢¼å™¨,åŸºæ–¼ Vaswani ç­‰äºº (2017) æè¿°çš„åŸå§‹å¯¦ä½œã€‚ç”±æ–¼ Transformers çš„ä½¿ç”¨å·²ç¶“è®Šå¾—æ™®é,è€Œæˆ‘å€‘çš„å¯¦ä½œå¹¾ä¹èˆ‡åŸå§‹ç‰ˆæœ¬ç›¸åŒ,å› æ­¤æˆ‘å€‘å°‡çœç•¥æ¨¡å‹æ¶æ§‹çš„è©³ç›¡èƒŒæ™¯æè¿°ã€‚
            </div>
        </div>

        <div class="key-concept">
            <h4>ğŸ“ BERT çš„å…©å€‹å°ºå¯¸</h4>
            <table>
                <thead>
                    <tr>
                        <th>æ¨¡å‹</th>
                        <th>å±¤æ•¸ (L)</th>
                        <th>éš±è—ç¶­åº¦ (H)</th>
                        <th>æ³¨æ„åŠ›é ­ (A)</th>
                        <th>åƒæ•¸é‡</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>BERT-BASE</strong></td>
                        <td>12</td>
                        <td>768</td>
                        <td>12</td>
                        <td>110M</td>
                    </tr>
                    <tr>
                        <td><strong>BERT-LARGE</strong></td>
                        <td>24</td>
                        <td>1024</td>
                        <td>16</td>
                        <td>340M</td>
                    </tr>
                </tbody>
            </table>

            <p><strong>è¨­è¨ˆè€ƒé‡:</strong></p>
            <p>BERT-BASE çš„å¤§å°èˆ‡ GPT ç›¸åŒ (110M),æ–¹ä¾¿æ¯”è¼ƒ!</p>
        </div>

        <div class="figure">
            <img src="images/BERT_Overall.png" alt="BERT æ•´é«”æ¶æ§‹" style="max-width: 100%; height: auto;">
            <p class="caption">
                <strong>åœ– 2.1:BERT çš„æ•´é«”æ¶æ§‹</strong><br>
                å±•ç¤ºäº† BERT çš„ Transformer Encoder çµæ§‹ã€‚<br>
                â€¢ <strong>è¼¸å…¥å±¤</strong>:Token + Segment + Position Embeddings<br>
                â€¢ <strong>ä¸­é–“å±¤</strong>:å¤šå±¤ Transformer Encoder (12 å±¤æˆ– 24 å±¤)<br>
                â€¢ <strong>è¼¸å‡ºå±¤</strong>:æ¯å€‹ token éƒ½æœ‰å°æ‡‰çš„è¼¸å‡ºå‘é‡<br>
                æ³¨æ„æ¯å€‹ token éƒ½èƒ½ã€Œçœ‹åˆ°ã€æ•´å€‹åºåˆ—çš„è³‡è¨Š(é›™å‘æ³¨æ„åŠ›)ï¼
            </p>
        </div>

        <div class="explanation">
            <h4>ğŸ” é—œéµå·®ç•°:BERT vs GPT</h4>
            <table>
                <thead>
                    <tr>
                        <th>ç‰¹æ€§</th>
                        <th>BERT</th>
                        <th>GPT</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>æ¶æ§‹</strong></td>
                        <td>Transformer Encoder</td>
                        <td>Transformer Decoder</td>
                    </tr>
                    <tr>
                        <td><strong>æ³¨æ„åŠ›</strong></td>
                        <td>é›™å‘ (çœ‹å·¦å³)</td>
                        <td>å–®å‘ (åªçœ‹å·¦)</td>
                    </tr>
                    <tr>
                        <td><strong>è¨“ç·´ä»»å‹™</strong></td>
                        <td>Masked LM + NSP</td>
                        <td>é æ¸¬ä¸‹ä¸€å€‹å­—</td>
                    </tr>
                    <tr>
                        <td><strong>æ“…é•·</strong></td>
                        <td>ç†è§£ä»»å‹™</td>
                        <td>ç”Ÿæˆä»»å‹™</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h2>ğŸ”¤ è¼¸å…¥è¡¨ç¤º:ä¸‰ç¨® Embeddings</h2>

        <div class="text-pair">
            <div class="original-text">
                For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.
            </div>
            <div class="translation">
                å°æ–¼çµ¦å®šçš„æ¨™è¨˜,å…¶è¼¸å…¥è¡¨ç¤ºæ˜¯é€éå°‡ç›¸æ‡‰çš„æ¨™è¨˜åµŒå…¥ã€ç‰‡æ®µåµŒå…¥å’Œä½ç½®åµŒå…¥ç›¸åŠ ä¾†æ§‹å»ºçš„ã€‚
            </div>
        </div>

        <div class="key-concept">
            <h4>ğŸ“Š BERT è¼¸å…¥ = ä¸‰ç¨® Embedding ç›¸åŠ </h4>
            <pre><code>è¼¸å…¥è¡¨ç¤º = Token Embedding + Segment Embedding + Position Embedding</code></pre>

            <h5>1ï¸âƒ£ Token Embedding (è©åµŒå…¥)</h5>
            <ul>
                <li>å°‡æ¯å€‹å­—è½‰æ›æˆå‘é‡</li>
                <li>ä½¿ç”¨ WordPiece,è©å½™è¡¨å¤§å° 30,000</li>
            </ul>

            <h5>2ï¸âƒ£ Segment Embedding (ç‰‡æ®µåµŒå…¥)</h5>
            <ul>
                <li>å€åˆ†å¥å­ A å’Œå¥å­ B</li>
                <li>åªæœ‰å…©ç¨®å€¼:E<sub>A</sub> æˆ– E<sub>B</sub></li>
            </ul>

            <h5>3ï¸âƒ£ Position Embedding (ä½ç½®åµŒå…¥)</h5>
            <ul>
                <li>æ¨™è¨˜æ¯å€‹å­—çš„ä½ç½®</li>
                <li>æ”¯æ´æœ€å¤š 512 å€‹ tokens</li>
            </ul>
        </div>

        <div class="figure">
            <img src="images/Input_Emebeddings.png" alt="BERT è¼¸å…¥ Embeddings" style="max-width: 100%; height: auto;">
            <p class="caption">
                <strong>åœ– 2.2:BERT çš„è¼¸å…¥è¡¨ç¤º</strong><br>
                å±•ç¤ºäº†å¦‚ä½•å°‡æ–‡å­—è½‰æ›æˆ BERT çš„è¼¸å…¥å‘é‡ã€‚<br>
                â€¢ <strong>Token Embeddings</strong>:æ¯å€‹è©çš„å‘é‡è¡¨ç¤º<br>
                â€¢ <strong>Segment Embeddings</strong>:æ¨™è¨˜å±¬æ–¼å¥å­ A é‚„æ˜¯å¥å­ B (ç”¨ E<sub>A</sub> æˆ– E<sub>B</sub>)<br>
                â€¢ <strong>Position Embeddings</strong>:æ¨™è¨˜æ¯å€‹ token çš„ä½ç½® (0, 1, 2, ...)<br>
                ä¸‰è€…ç›¸åŠ å¾Œ,å°±æ˜¯é€é€² Transformer çš„è¼¸å…¥ï¼
            </p>
        </div>

        <div class="explanation">
            <h4>ğŸ” ç‰¹æ®Š Tokens</h4>
            <p><strong>[CLS]</strong> - Classification token</p>
            <ul>
                <li>æ°¸é æ˜¯ç¬¬ä¸€å€‹ token</li>
                <li>ç”¨æ–¼åˆ†é¡ä»»å‹™çš„æ•´å¥è¡¨ç¤º</li>
            </ul>

            <p><strong>[SEP]</strong> - Separator token</p>
            <ul>
                <li>åˆ†éš”å…©å€‹å¥å­</li>
                <li>ä¹Ÿç”¨æ–¼æ¨™è¨˜å¥å­çµå°¾</li>
            </ul>

            <p><strong>[MASK]</strong> - Mask token</p>
            <ul>
                <li>é è¨“ç·´æ™‚ç”¨ä¾†é®è”½å­—è©</li>
                <li>å¾®èª¿æ™‚ä¸æœƒå‡ºç¾</li>
            </ul>
        </div>

        <div class="analogy">
            <h4>ğŸ’¡ ç”Ÿæ´»é¡æ¯”:ä¸‰ç¨®æ¨™è¨˜ç³»çµ±</h4>
            <p><strong>æƒ³åƒä½ åœ¨è®€ä¸€æœ¬é›™èªå°ç…§æ›¸:</strong></p>

            <p><strong>Token Embedding</strong> = æ¯å€‹å­—çš„æ„æ€</p>
            <ul>
                <li>"cat" â†’ [0.2, -0.5, 0.8, ...] (è²“çš„å‘é‡)</li>
            </ul>

            <p><strong>Segment Embedding</strong> = æ¨™è¨˜é€™æ˜¯è‹±æ–‡é‚„æ˜¯ä¸­æ–‡é‚£ä¸€å´</p>
            <ul>
                <li>å·¦é (è‹±æ–‡) â†’ E<sub>A</sub></li>
                <li>å³é (ä¸­æ–‡) â†’ E<sub>B</sub></li>
            </ul>

            <p><strong>Position Embedding</strong> = é€™å€‹å­—åœ¨ç¬¬å¹¾å€‹ä½ç½®</p>
            <ul>
                <li>ç¬¬ 1 å€‹å­— â†’ P<sub>0</sub></li>
                <li>ç¬¬ 2 å€‹å­— â†’ P<sub>1</sub></li>
            </ul>
        </div>

        <div class="analogy">
            <h4>ğŸ¤– AI é«”é©—é€£çµ:ç‚ºä»€éº¼ ChatGPT æœ‰ Token é™åˆ¶?</h4>
            <p><strong>BERT</strong>: æœ€å¤š 512 tokens</p>
            <p><strong>GPT-3</strong>: æœ€å¤š 2048 tokens</p>
            <p><strong>GPT-4</strong>: æœ€å¤š 8192 / 32768 tokens</p>

            <p><strong>é™åˆ¶ä¾†æº:</strong></p>
            <p>Position Embedding çš„é•·åº¦æ˜¯å›ºå®šçš„!</p>
            <ul>
                <li>BERT åªè¨“ç·´äº† 512 å€‹ä½ç½®åµŒå…¥</li>
                <li>è¶…éå°±ç„¡æ³•è™•ç†</li>
            </ul>

            <p><strong>è§£æ±ºæ–¹æ¡ˆ:</strong></p>
            <ul>
                <li>è¨“ç·´æ›´é•·çš„ä½ç½®åµŒå…¥</li>
                <li>ä½¿ç”¨ç›¸å°ä½ç½®ç·¨ç¢¼</li>
                <li>ä½¿ç”¨ ALiBiã€RoPE ç­‰æ–°æŠ€è¡“</li>
            </ul>
        </div>

        <div class="nav-bar">
            <a href="01-abstract-and-introduction.html" class="nav-btn">â† ä¸Šä¸€é </a>
            <a href="index.html" class="nav-btn">ğŸ“‘ ç›®éŒ„</a>
            <a href="03-pretraining-tasks.html" class="nav-btn primary">ä¸‹ä¸€é  â†’ é è¨“ç·´ä»»å‹™</a>
        </div>
    </div>
</body>
</html>

