<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬ 1 é :æ‘˜è¦èˆ‡å¼•è¨€ - BERT è«–æ–‡æ·±åº¦è§£æ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <div class="header-section">
            <h1>ğŸ“– ç¬¬ 1 é :æ‘˜è¦èˆ‡å¼•è¨€</h1>
            <div class="paper-meta">
                <div class="meta-item">
                    <strong>è«–æ–‡æ¨™é¡Œ</strong>
                    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
                </div>
                <div class="meta-item">
                    <strong>ä½œè€…</strong>
                    Jacob Devlin et al. (Google AI Language)
                </div>
                <div class="meta-item">
                    <strong>ç™¼è¡¨</strong>
                    NAACL 2019
                </div>
                <div class="meta-item">
                    <strong>é—œéµå‰µæ–°</strong>
                    é›™å‘é è¨“ç·´ã€Masked LM
                </div>
            </div>
        </div>

        <h2>ğŸ“„ Abstract (æ‘˜è¦)</h2>

        <div class="text-pair">
            <div class="original-text">
                We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.
            </div>
            <div class="translation">
                æˆ‘å€‘ä»‹ç´¹ä¸€å€‹æ–°çš„èªè¨€è¡¨ç¤ºæ¨¡å‹ BERT,ä»£è¡¨ä¾†è‡ª Transformers çš„é›™å‘ç·¨ç¢¼å™¨è¡¨ç¤ºã€‚èˆ‡è¿‘æœŸçš„èªè¨€è¡¨ç¤ºæ¨¡å‹ä¸åŒ,BERT æ—¨åœ¨é€éåœ¨æ‰€æœ‰å±¤ä¸­åŒæ™‚æ¢ä»¶åŒ–å·¦å³ä¸Šä¸‹æ–‡,å¾ç„¡æ¨™è¨˜æ–‡æœ¬ä¸­é è¨“ç·´æ·±åº¦é›™å‘è¡¨ç¤ºã€‚
            </div>
        </div>

        <div class="key-concept">
            <h4>ğŸ¯ BERT æ˜¯ä»€éº¼?</h4>
            <p><strong>å…¨å:</strong> <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers</p>
            
            <p><strong>æ ¸å¿ƒç‰¹è‰²:</strong></p>
            <ul>
                <li><strong>Bidirectional (é›™å‘)</strong>:åŒæ™‚çœ‹å·¦é‚Šå’Œå³é‚Šçš„æ–‡å­—</li>
                <li><strong>Encoder (ç·¨ç¢¼å™¨)</strong>:ä½¿ç”¨ Transformer Encoder æ¶æ§‹</li>
                <li><strong>Pre-training (é è¨“ç·´)</strong>:å…ˆåœ¨å¤§é‡è³‡æ–™ä¸Šå­¸ç¿’,å†é‡å°ç‰¹å®šä»»å‹™å¾®èª¿</li>
            </ul>
        </div>

        <div class="explanation">
            <h4>ğŸ” ç‚ºä»€éº¼å«ã€Œé›™å‘ã€?</h4>
            <p><strong>å°æ¯”èˆŠæ–¹æ³•:</strong></p>
            
            <h5>âŒ å–®å‘æ¨¡å‹ (ä¾‹å¦‚ GPT)</h5>
            <pre><code>å¥å­: "The cat sat on the mat"
è™•ç†æ–¹å¼: The â†’ cat â†’ sat â†’ on â†’ the â†’ mat
         (åªèƒ½çœ‹ã€Œå·¦é‚Šã€çš„å­—)</code></pre>

            <h5>âœ… é›™å‘æ¨¡å‹ (BERT)</h5>
            <pre><code>å¥å­: "The cat sat on the mat"
è™•ç†æ–¹å¼: æ¯å€‹å­—éƒ½èƒ½åŒæ™‚çœ‹ã€Œå·¦é‚Šã€å’Œã€Œå³é‚Šã€!

"sat" å¯ä»¥çœ‹åˆ°:
â† å·¦é‚Š: The, cat
â†’ å³é‚Š: on, the, mat</code></pre>
        </div>

        <div class="analogy">
            <h4>ğŸ’¡ ç”Ÿæ´»é¡æ¯”:é–±è®€ç†è§£çš„å·®ç•°</h4>
            <p><strong>å–®å‘é–±è®€ (GPT çš„æ–¹å¼)</strong></p>
            <p>æƒ³åƒä½ ç”¨ä¸€å¼µç´™é®ä½å³é‚Š,ä¸€å€‹å­—ä¸€å€‹å­—å¾€å³è®€:</p>
            <pre><code>"The cat [é®ä½] [é®ä½] [é®ä½] [é®ä½]"
â†’ ä½ åªçŸ¥é“æœ‰éš»è²“,ä½†ä¸çŸ¥é“è²“åšäº†ä»€éº¼</code></pre>

            <p><strong>é›™å‘é–±è®€ (BERT çš„æ–¹å¼)</strong></p>
            <p>ä½ å¯ä»¥çœ‹æ•´å¥è©±:</p>
            <pre><code>"The cat sat on the mat"
â†’ ä½ çŸ¥é“è²“ã€Œåã€åœ¨å¢Šå­ä¸Š,ç†è§£æ›´å®Œæ•´!</code></pre>
        </div>

        <div class="figure">
            <img src="images/BERT_comparisons.png" alt="BERT èˆ‡å…¶ä»–æ¨¡å‹çš„æ¶æ§‹å°æ¯”" style="max-width: 100%; height: auto;">
            <p class="caption">
                <strong>åœ– 1.1:BERT èˆ‡å…¶ä»–é è¨“ç·´æ¨¡å‹çš„æ¶æ§‹å°æ¯”</strong><br>
                å·¦å´æ˜¯ <strong>BERT</strong>:é›™å‘ Encoder,å¯ä»¥åŒæ™‚çœ‹åˆ°ä¸Šä¸‹æ–‡çš„å·¦å³å…©é‚Š<br>
                ä¸­é–“æ˜¯ <strong>GPT</strong>:å–®å‘ Decoder,åªèƒ½å¾å·¦åˆ°å³çœ‹<br>
                å³å´æ˜¯ <strong>ELMo</strong>:æ·ºå±¤é›™å‘(å…©å€‹ç¨ç«‹çš„å–®å‘ LSTM)<br>
                <strong>BERT çš„å„ªå‹¢:</strong>æ·±å±¤é›™å‘,èƒ½æ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡!
            </p>
        </div>

        <div class="text-pair">
            <div class="original-text">
                As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
            </div>
            <div class="translation">
                å› æ­¤,é è¨“ç·´çš„ BERT æ¨¡å‹åªéœ€æ·»åŠ ä¸€å€‹é¡å¤–çš„è¼¸å‡ºå±¤å°±å¯ä»¥é€²è¡Œå¾®èª¿,ç‚ºå»£æ³›çš„ä»»å‹™å‰µå»ºæœ€å…ˆé€²çš„æ¨¡å‹,ä¾‹å¦‚å•ç­”å’Œèªè¨€æ¨ç†,è€Œä¸éœ€è¦å¤§é‡ç‰¹å®šä»»å‹™çš„æ¶æ§‹ä¿®æ”¹ã€‚
            </div>
        </div>

        <div class="key-concept">
            <h4>ğŸš€ BERT çš„é©šäººæˆæœ</h4>
            <p><strong>åœ¨ 11 å€‹ NLP ä»»å‹™ä¸Šåˆ·æ–°ç´€éŒ„!</strong></p>
            <table>
                <thead>
                    <tr>
                        <th>ä»»å‹™</th>
                        <th>æˆç¸¾</th>
                        <th>æå‡å¹…åº¦</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>GLUE</strong></td>
                        <td>80.5%</td>
                        <td>+7.7%</td>
                    </tr>
                    <tr>
                        <td><strong>MultiNLI</strong></td>
                        <td>86.7%</td>
                        <td>+4.6%</td>
                    </tr>
                    <tr>
                        <td><strong>SQuAD v1.1</strong></td>
                        <td>93.2 F1</td>
                        <td>+1.5</td>
                    </tr>
                    <tr>
                        <td><strong>SQuAD v2.0</strong></td>
                        <td>83.1 F1</td>
                        <td>+5.1</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="analogy">
            <h4>ğŸ¤– AI é«”é©—é€£çµ:ç‚ºä»€éº¼ Google æœå°‹è®Šè°æ˜äº†?</h4>
            <p><strong>ä½ å¯èƒ½æ³¨æ„åˆ°...</strong></p>
            <p>2019 å¹´å¾Œ,Google æœå°‹é–‹å§‹æ›´ã€Œæ‡‚ã€ä½ çš„å•é¡Œ!</p>

            <p><strong>ç¯„ä¾‹:</strong></p>
            <blockquote>
                æœå°‹: "2019 brazil traveler to usa need a visa"<br>
                <br>
                <strong>èˆŠ Google (2019 å‰)</strong>: åªçœ‹é—œéµå­— "brazil", "usa", "visa"<br>
                <strong>æ–° Google (2019 å¾Œ)</strong>: ç†è§£ã€Œå·´è¥¿äººã€è¦å»ã€Œç¾åœ‹ã€éœ€ä¸éœ€è¦ç°½è­‰
            </blockquote>

            <p><strong>ç§˜å¯†æ­¦å™¨:</strong> Google åœ¨ 2019 å¹´å°‡ <strong>BERT æ•´åˆé€²æœå°‹å¼•æ“</strong>!</p>
            <p>BERT çš„ã€Œé›™å‘ç†è§£ã€è®“æœå°‹å¼•æ“æ›´æ‡‚èªæ„!</p>
        </div>

        <h2>ğŸŒŸ å¼•è¨€:ç‚ºä»€éº¼éœ€è¦ BERT?</h2>

        <div class="text-pair">
            <div class="original-text">
                Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, as well as token-level tasks such as named entity recognition and question answering.
            </div>
            <div class="translation">
                èªè¨€æ¨¡å‹é è¨“ç·´å·²è¢«è­‰æ˜å°æ”¹å–„è¨±å¤šè‡ªç„¶èªè¨€è™•ç†ä»»å‹™æœ‰æ•ˆã€‚é€™äº›åŒ…æ‹¬å¥å­å±¤ç´šçš„ä»»å‹™,å¦‚è‡ªç„¶èªè¨€æ¨ç†å’Œæ”¹å¯«,ä»¥åŠæ¨™è¨˜å±¤ç´šçš„ä»»å‹™,å¦‚å‘½åå¯¦é«”è­˜åˆ¥å’Œå•ç­”ã€‚
            </div>
        </div>

        <div class="explanation">
            <h4>ğŸ” ä»€éº¼æ˜¯ã€Œé è¨“ç·´ã€+ã€Œå¾®èª¿ã€?</h4>
            <p><strong>å…©éšæ®µå­¸ç¿’ç­–ç•¥:</strong></p>

            <h5>éšæ®µ 1:é è¨“ç·´ (Pre-training)</h5>
            <ul>
                <li><strong>è³‡æ–™</strong>:å¤§é‡ç„¡æ¨™è¨˜æ–‡æœ¬ (ä¾‹å¦‚ Wikipedia)</li>
                <li><strong>ç›®æ¨™</strong>:å­¸ç¿’ã€Œé€šç”¨èªè¨€çŸ¥è­˜ã€</li>
                <li><strong>æ™‚é–“</strong>:æ•¸å¤©åˆ°æ•¸é€±</li>
                <li><strong>æˆæœ¬</strong>:é«˜ (éœ€è¦å¤§é‡ GPU)</li>
            </ul>

            <h5>éšæ®µ 2:å¾®èª¿ (Fine-tuning)</h5>
            <ul>
                <li><strong>è³‡æ–™</strong>:å°‘é‡æ¨™è¨˜è³‡æ–™ (ä¾‹å¦‚ 1 è¬ç­†å•ç­”)</li>
                <li><strong>ç›®æ¨™</strong>:é‡å°ç‰¹å®šä»»å‹™å„ªåŒ–</li>
                <li><strong>æ™‚é–“</strong>:æ•¸åˆ†é˜åˆ°æ•¸å°æ™‚</li>
                <li><strong>æˆæœ¬</strong>:ä½</li>
            </ul>
        </div>

        <div class="analogy">
            <h4>ğŸ’¡ ç”Ÿæ´»é¡æ¯”:é€šæ‰ â†’ å°ˆå®¶</h4>
            <p><strong>é è¨“ç·´ = å¤§å­¸é€šè­˜æ•™è‚²</strong></p>
            <ul>
                <li>å­¸ç¿’å»£æ³›çš„åŸºç¤çŸ¥è­˜(æ•¸å­¸ã€ç‰©ç†ã€èªè¨€...)</li>
                <li>æ™‚é–“é•·ã€ç¯„åœå»£</li>
            </ul>

            <p><strong>å¾®èª¿ = å°ˆæ¥­è¨“ç·´</strong></p>
            <ul>
                <li>é‡å°ç‰¹å®šé ˜åŸŸæ·±åŒ–(ä¾‹å¦‚:è®Šæˆã€Œå•ç­”å°ˆå®¶ã€)</li>
                <li>æ™‚é–“çŸ­ã€ç›®æ¨™æ˜ç¢º</li>
            </ul>

            <p><strong>å¥½è™•:</strong></p>
            <p>æœ‰äº†é€šè­˜åŸºç¤,å­¸æ–°å°ˆæ¥­æœƒæ›´å¿«!</p>
        </div>

        <div class="text-pair">
            <div class="original-text">
                There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters.
            </div>
            <div class="translation">
                ç›®å‰æœ‰å…©ç¨®ç­–ç•¥å¯å°‡é è¨“ç·´çš„èªè¨€è¡¨ç¤ºæ‡‰ç”¨æ–¼ä¸‹æ¸¸ä»»å‹™:åŸºæ–¼ç‰¹å¾µå’Œå¾®èª¿ã€‚åŸºæ–¼ç‰¹å¾µçš„æ–¹æ³•,å¦‚ ELMo,ä½¿ç”¨åŒ…å«é è¨“ç·´è¡¨ç¤ºä½œç‚ºé¡å¤–ç‰¹å¾µçš„ä»»å‹™ç‰¹å®šæ¶æ§‹ã€‚å¾®èª¿æ–¹æ³•,å¦‚ç”Ÿæˆå¼é è¨“ç·´ Transformer (OpenAI GPT),å¼•å…¥æœ€å°‘çš„ä»»å‹™ç‰¹å®šåƒæ•¸,ä¸¦é€éç°¡å–®åœ°å¾®èª¿æ‰€æœ‰é è¨“ç·´åƒæ•¸ä¾†åœ¨ä¸‹æ¸¸ä»»å‹™ä¸Šè¨“ç·´ã€‚
            </div>
        </div>

        <div class="key-concept">
            <h4>ğŸ“Š å…©ç¨®æ‡‰ç”¨ç­–ç•¥</h4>
            <table>
                <thead>
                    <tr>
                        <th>ç­–ç•¥</th>
                        <th>ä»£è¡¨æ¨¡å‹</th>
                        <th>æ–¹å¼</th>
                        <th>å„ªé»</th>
                        <th>ç¼ºé»</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Feature-based</strong></td>
                        <td>ELMo</td>
                        <td>æŠŠé è¨“ç·´æ¨¡å‹ç•¶ã€Œç‰¹å¾µæå–å™¨ã€</td>
                        <td>éˆæ´»</td>
                        <td>éœ€è¦è¨­è¨ˆä»»å‹™æ¶æ§‹</td>
                    </tr>
                    <tr>
                        <td><strong>Fine-tuning</strong></td>
                        <td>GPT, BERT</td>
                        <td>ç›´æ¥å¾®èª¿æ•´å€‹æ¨¡å‹</td>
                        <td>ç°¡å–®ã€æ•ˆæœå¥½</td>
                        <td>æ¯å€‹ä»»å‹™è¦å­˜ä¸€ä»½æ¨¡å‹</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="analogy">
            <h4>ğŸ”§ å·¥ç¨‹é¡æ¯”:å…©ç¨®æ‡‰ç”¨æ–¹å¼</h4>
            <pre><code>// Feature-based (ELMo æ–¹å¼)
pretrained_model = load_elmo()
features = pretrained_model.extract_features(text)  // å‡çµ,ä¸æ›´æ–°
custom_model = build_task_model()  // è‡ªå·±è¨­è¨ˆ
output = custom_model(features)  // çµ„åˆä½¿ç”¨

// Fine-tuning (BERT æ–¹å¼)
pretrained_model = load_bert()
pretrained_model.add_output_layer(num_classes)  // åªåŠ ä¸€å±¤
pretrained_model.train(task_data)  // å…¨éƒ¨ä¸€èµ·è¨“ç·´!
output = pretrained_model(text)
</code></pre>
        </div>

        <h2>âŒ å–®å‘æ¨¡å‹çš„å•é¡Œ</h2>

        <div class="text-pair">
            <div class="original-text">
                We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.
            </div>
            <div class="translation">
                æˆ‘å€‘èªç‚ºç›®å‰çš„æŠ€è¡“é™åˆ¶äº†é è¨“ç·´è¡¨ç¤ºçš„èƒ½åŠ›,ç‰¹åˆ¥æ˜¯å°æ–¼å¾®èª¿æ–¹æ³•ã€‚ä¸»è¦é™åˆ¶æ˜¯æ¨™æº–èªè¨€æ¨¡å‹æ˜¯å–®å‘çš„,é€™é™åˆ¶äº†é è¨“ç·´æœŸé–“å¯ä»¥ä½¿ç”¨çš„æ¶æ§‹é¸æ“‡ã€‚
            </div>
        </div>

        <div class="problem">
            <h4>âŒ GPT çš„é™åˆ¶:åªèƒ½å¾€å³çœ‹</h4>
            <p><strong>GPT çš„æ¶æ§‹:</strong></p>
            <pre><code>å¥å­: "The cat sat on the mat"

è™•ç† "sat":
âœ“ å¯ä»¥çœ‹: The, cat (å·¦é‚Š)
âœ— ä¸èƒ½çœ‹: on, the, mat (å³é‚Š)

ç‚ºä»€éº¼? GPT æ˜¯ã€Œèªè¨€ç”Ÿæˆæ¨¡å‹ã€,
é æ¸¬ã€Œä¸‹ä¸€å€‹å­—ã€æ™‚ä¸èƒ½å·çœ‹ç­”æ¡ˆ!</code></pre>

            <p><strong>å•é¡Œ:</strong></p>
            <ul>
                <li>å°æ–¼ã€Œå¥å­å±¤ç´šã€ä»»å‹™é‚„å¥½</li>
                <li>ä½†å°æ–¼ã€Œè©å±¤ç´šã€ä»»å‹™ (ä¾‹å¦‚å•ç­”) å¾ˆç³Ÿ!</li>
            </ul>
        </div>

        <div class="explanation">
            <h4>ğŸ¤” ç‚ºä»€éº¼å•ç­”ä»»å‹™éœ€è¦é›™å‘?</h4>
            <p><strong>ç¯„ä¾‹:</strong></p>
            <pre><code>æ–‡ç« : "Mary went to the store. She bought milk."
å•é¡Œ: "What did Mary buy?"

åˆ†æ "She" é€™å€‹å­—:
â† å¾€å·¦çœ‹: Mary went to the store
â†’ å¾€å³çœ‹: bought milk

è¦çŸ¥é“ "She" = "Mary",éœ€è¦çœ‹ã€Œå·¦é‚Šã€!
è¦çŸ¥é“ Mary è²·äº†ä»€éº¼,éœ€è¦çœ‹ã€Œå³é‚Šã€!

å–®å‘æ¨¡å‹: è™•ç† "She" æ™‚çœ‹ä¸åˆ° "bought milk" âœ—
é›™å‘æ¨¡å‹: å¯ä»¥åŒæ™‚çœ‹å·¦å³ âœ“
</code></pre>
        </div>

        <div class="analogy">
            <h4>ğŸ’¡ ç”Ÿæ´»é¡æ¯”:å¡«ç©ºé¡Œ vs ä½œæ–‡é¡Œ</h4>
            <p><strong>GPT (å–®å‘) = ä½œæ–‡é¡Œ</strong></p>
            <p>ã€Œè«‹çºŒå¯«: The cat sat on the ___ã€</p>
            <ul>
                <li>ä½ åªèƒ½çœ‹å‰é¢çš„å­—</li>
                <li>ç„¶å¾Œã€Œé æ¸¬ã€ä¸‹ä¸€å€‹å­—</li>
            </ul>

            <p><strong>BERT (é›™å‘) = å¡«ç©ºé¡Œ</strong></p>
            <p>ã€ŒThe cat ___ on the matã€</p>
            <ul>
                <li>ä½ å¯ä»¥çœ‹ã€Œå‰å¾Œæ–‡ã€</li>
                <li>æ ¹æ“šä¸Šä¸‹æ–‡åˆ¤æ–·ç©ºæ ¼æ‡‰è©²å¡«ä»€éº¼</li>
            </ul>

            <p><strong>å“ªå€‹æ›´èƒ½ç†è§£å¥æ„?</strong> ç•¶ç„¶æ˜¯å¡«ç©ºé¡Œ!</p>
        </div>

        <h2>âœ… BERT çš„è§£æ±ºæ–¹æ¡ˆ</h2>

        <div class="text-pair">
            <div class="original-text">
                In this paper, we improve the fine-tuning based approaches by proposing BERT. BERT alleviates the previously mentioned unidirectionality constraint by using a "masked language model" (MLM) pre-training objective, inspired by the Cloze task.
            </div>
            <div class="translation">
                åœ¨æœ¬æ–‡ä¸­,æˆ‘å€‘é€éæå‡º BERT ä¾†æ”¹é€²åŸºæ–¼å¾®èª¿çš„æ–¹æ³•ã€‚BERT é€éä½¿ç”¨ã€Œé®ç½©èªè¨€æ¨¡å‹ã€(MLM) é è¨“ç·´ç›®æ¨™ä¾†ç·©è§£å…ˆå‰æåˆ°çš„å–®å‘æ€§é™åˆ¶,éˆæ„Ÿä¾†è‡ªå®Œå½¢å¡«ç©ºä»»å‹™ã€‚
            </div>
        </div>

        <div class="solution">
            <h4>âœ… æ ¸å¿ƒå‰µæ–°:Masked Language Model (MLM)</h4>
            <p><strong>æ¦‚å¿µ:</strong></p>
            <p>éš¨æ©Ÿã€Œé®ä½ã€15% çš„å­—,è®“æ¨¡å‹é æ¸¬è¢«é®ä½çš„å­—!</p>

            <pre><code>åŸå¥: "The cat sat on the mat"
é®ç½©: "The cat [MASK] on the mat"
ä»»å‹™: é æ¸¬ [MASK] = "sat"

æ¨¡å‹å¯ä»¥çœ‹:
â† å·¦é‚Š: The, cat
â†’ å³é‚Š: on, the, mat
â†’ é”æˆã€Œé›™å‘ã€ç†è§£!</code></pre>
        </div>

        <div class="analogy">
            <h4>ğŸ’¡ ç”Ÿæ´»é¡æ¯”:å®Œå½¢å¡«ç©ºæ¸¬é©—</h4>
            <p><strong>åœ‹ä¸­è‹±æ–‡è€ƒè©¦:</strong></p>
            <blockquote>
                "The weather was very ____ yesterday, so we stayed home."<br>
                (A) sunny  (B) rainy  (C) hot
            </blockquote>

            <p>ä½ æ€éº¼ä½œç­”?</p>
            <ul>
                <li>çœ‹å‰æ–‡: "The weather was very"</li>
                <li>çœ‹å¾Œæ–‡: "so we stayed home"</li>
                <li>æ¨è«–: å› ç‚ºå¾…åœ¨å®¶,å¯èƒ½å¤©æ°£ä¸å¥½ â†’ é¸ (B) rainy</li>
            </ul>

            <p><strong>BERT çš„è¨“ç·´å°±æ˜¯åšã€Œå®Œå½¢å¡«ç©ºã€!</strong></p>
        </div>

        <div class="text-pair">
            <div class="original-text">
                In addition to the masked language model, we also use a "next sentence prediction" task that jointly pre-trains text-pair representations.
            </div>
            <div class="translation">
                é™¤äº†é®ç½©èªè¨€æ¨¡å‹ä¹‹å¤–,æˆ‘å€‘é‚„ä½¿ç”¨ã€Œä¸‹ä¸€å¥é æ¸¬ã€ä»»å‹™ä¾†å…±åŒé è¨“ç·´æ–‡æœ¬å°è¡¨ç¤ºã€‚
            </div>
        </div>

        <div class="key-concept">
            <h4>ğŸ¯ BERT çš„å…©å€‹é è¨“ç·´ä»»å‹™</h4>
            
            <h5>ä»»å‹™ 1:Masked LM (é®ç½©èªè¨€æ¨¡å‹)</h5>
            <ul>
                <li><strong>ç›®æ¨™</strong>:é æ¸¬è¢«é®ä½çš„å­—</li>
                <li><strong>ç›®çš„</strong>:å­¸ç¿’è©å±¤ç´šçš„ç†è§£</li>
            </ul>

            <h5>ä»»å‹™ 2:Next Sentence Prediction (ä¸‹ä¸€å¥é æ¸¬)</h5>
            <ul>
                <li><strong>ç›®æ¨™</strong>:åˆ¤æ–·å¥å­ B æ˜¯å¦æ˜¯å¥å­ A çš„ä¸‹ä¸€å¥</li>
                <li><strong>ç›®çš„</strong>:å­¸ç¿’å¥å­ä¹‹é–“çš„é—œä¿‚</li>
            </ul>

            <p><em>ğŸ’¡ è©³ç´°å…§å®¹æˆ‘å€‘æœƒåœ¨ç¬¬ 3 é æ·±å…¥æ¢è¨!</em></p>
        </div>

        <div class="analogy">
            <h4>ğŸ¤– AI é«”é©—é€£çµ:å¾ BERT åˆ° ChatGPT</h4>
            <p><strong>æ™‚é–“ç·š:</strong></p>
            <ul>
                <li><strong>2017</strong>: Transformer è«–æ–‡ç™¼è¡¨</li>
                <li><strong>2018</strong>: GPT (å–®å‘) å’Œ BERT (é›™å‘) åŒå¹´ç™¼è¡¨</li>
                <li><strong>2019</strong>: BERT æ”¹è®Š NLP é ˜åŸŸ</li>
                <li><strong>2020</strong>: GPT-3 (175B åƒæ•¸)</li>
                <li><strong>2022</strong>: ChatGPT ç™¼å¸ƒ</li>
            </ul>

            <p><strong>é—œéµå·®ç•°:</strong></p>
            <ul>
                <li><strong>BERT</strong>: é›™å‘ã€æ“…é•·ã€Œç†è§£ã€ä»»å‹™ (åˆ†é¡ã€å•ç­”)</li>
                <li><strong>GPT</strong>: å–®å‘ã€æ“…é•·ã€Œç”Ÿæˆã€ä»»å‹™ (å¯«ä½œã€å°è©±)</li>
            </ul>

            <p><strong>ç‚ºä»€éº¼ ChatGPT ç”¨ GPT ä¸ç”¨ BERT?</strong></p>
            <p>å› ç‚ºå°è©±ç³»çµ±éœ€è¦ã€Œé€å­—ç”Ÿæˆã€å›æ‡‰,BERT çš„é›™å‘æ©Ÿåˆ¶ä¸é©åˆç”Ÿæˆä»»å‹™!</p>

            <p><strong>ä½† BERT ä»å¾ˆé‡è¦!</strong></p>
            <ul>
                <li>Google æœå°‹ã€Gmail æ™ºèƒ½å›è¦†éƒ½ç”¨ BERT</li>
                <li>BERT é–‹å‰µäº†ã€Œå¤§è¦æ¨¡é è¨“ç·´ã€çš„æ™‚ä»£</li>
                <li>å¾ŒçºŒæ¨¡å‹ (RoBERTa, ALBERT, ELECTRA) éƒ½æ˜¯æ”¹é€² BERT</li>
            </ul>
        </div>

        <div class="nav-bar">
            <button class="nav-btn" disabled>â† ä¸Šä¸€é </button>
            <a href="index.html" class="nav-btn">ğŸ“‘ ç›®éŒ„</a>
            <a href="02-bert-architecture.html" class="nav-btn primary">ä¸‹ä¸€é  â†’ BERT æ¶æ§‹</a>
        </div>
    </div>
</body>
</html>

