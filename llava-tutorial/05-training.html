<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLaVA 第 5 章：兩階段訓練策略 - 從對齊到微調的優雅範式</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="hero-section" style="background-image: url('images/generated/two_stage_training.png'); height: 80vh;">
        <div class="hero-overlay"></div>
        <div class="hero-content">
            <h1>訓練一個「會看圖的助手」</h1>
            <p class="hero-subtitle">凍結、對齊、解凍、微調——兩階段的優雅舞步</p>
            <p class="hero-meta">LLaVA 深度解析 · 第 5 章</p>
        </div>
    </div>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">🏠 首頁</a>
            <span>/</span>
            <a href="index.html">LLaVA 教學</a>
            <span>/</span>
            <span class="current">第 5 章</span>
        </div>

        <div class="story-container">
            <p class="story-lead drop-cap">
                訓練 LLaVA 就像教一個孩子學習——你不能一開始就讓他背誦百科全書。
                LLaVA 採用了<strong>兩階段訓練策略</strong>：
                第一階段，讓模型學會「看懂圖片」（特徵對齊）；
                第二階段，教模型「如何回答問題」（端到端微調）。
                這個漸進式的學習過程，既高效又優雅。
            </p>

            <div class="section-divider"><span>✦</span></div>

            <!-- 1. 輸入序列格式 -->
            <h2>📝 基礎：輸入序列的格式設計</h2>
            <div class="paper-section">
                <div class="explanation">
                    <p>在深入訓練策略之前，我們需要先理解 LLaVA 如何組織輸入資料。</p>
                    
                    <h4>多輪對話的序列化</h4>
                    <p>對於一張圖片 \( \mathbf{X}_v \) 和多輪對話 \( (\mathbf{X}_q^1, \mathbf{X}_a^1, \cdots, \mathbf{X}_q^T, \mathbf{X}_a^T) \)，
                    LLaVA 將它們組織成一個統一的序列：</p>
                </div>

                <div class="key-concept">
                    <h4>序列組織規則</h4>
                    <div style="background: white; padding: 20px; border-radius: 8px; margin: 20px 0;">
                        <p><strong>第一輪（t=1）</strong>：圖片可以放在問題前或後</p>
                        <pre style="background: #f8f9fa; padding: 10px; border-radius: 8px; font-size: 0.9rem;">
選項 A: Human: [圖片 token] [問題] ###
選項 B: Human: [問題] [圖片 token] ###
                        </pre>
                        <p style="margin-top: 15px;"><strong>後續輪（t>1）</strong>：不再包含圖片</p>
                        <pre style="background: #f8f9fa; padding: 10px; border-radius: 8px; font-size: 0.9rem;">
Human: [問題] ###
                        </pre>
                    </div>
                    <p><strong>設計理由</strong>：圖片只在第一輪出現，後續輪次依賴模型對圖片的「記憶」。
                    這模擬了真實對話場景——你不會在每句話都重複展示圖片。</p>
                </div>

                <div class="paper-section">
                    <div class="explanation">
                        <h4>完整序列範例</h4>
                        <pre style="background: #1e293b; color: #f1f5f9; padding: 20px; border-radius: 8px; font-size: 0.9rem; line-height: 1.8;">
A chat between a curious human and an assistant. ###
Human: [圖片 token × 257] What is unusual about this image? ###
<span style="color: #10b981;">Assistant: The unusual aspect of this image is a man ironing clothes on the back of a van. ###</span>
Human: Why is this dangerous? ###
<span style="color: #10b981;">Assistant: This is dangerous because the person could fall from the moving vehicle. ###</span>
                        </pre>
                        <p style="margin-top: 15px;"><strong>重點</strong>：只有<span style="color: #10b981;">綠色部分</span>（Assistant 的回應）會被用來計算 loss。
                        模型不需要預測人類的問題，只需要學會生成正確的回答。</p>
                    </div>
                </div>
            </div>

            <!-- 2. 訓練目標 -->
            <h2>🎯 訓練目標：自回歸語言建模</h2>
            <div class="paper-section">
                <div class="explanation">
                    <p>LLaVA 使用標準的<strong>自回歸語言建模目標</strong>，與 GPT 系列相同。</p>
                </div>

                <div class="key-concept">
                    <h4>數學公式</h4>
                    <p>給定長度為 \( L \) 的序列，模型最大化目標答案的條件概率：</p>
                    <div style="background: white; padding: 20px; border-radius: 8px; text-align: center; margin: 20px 0;">
                        \[
                        p(\mathbf{X}_a | \mathbf{X}_v, \mathbf{X}_{\text{instruct}}) = \prod_{i=1}^{L} p_\theta({\color{green} x_i} | \mathbf{X}_v, \mathbf{X}_{\text{instruct}, <i}, \mathbf{X}_{\text{a}, <i})
                        \]
                    </div>
                    <p>符號說明：</p>
                    <ul>
                        <li>\( \theta \)：可訓練參數（在不同階段有不同的選擇）</li>
                        <li>\( {\color{green} x_i} \)：第 \( i \) 個預測 token（只計算 Assistant 回應）</li>
                        <li>\( \mathbf{X}_{\text{instruct}, <i} \)：所有之前的指令 token</li>
                        <li>\( \mathbf{X}_{\text{a}, <i} \)：所有之前的回答 token</li>
                        <li>\( \mathbf{X}_v \)：視覺特徵（在所有時間步都存在）</li>
                    </ul>
                    <p style="margin-top: 15px;"><strong>本質</strong>：給定圖片和前文，預測下一個詞。這是語言模型的標準任務。</p>
                </div>
            </div>

            <div class="figure figure-ai">
                <img src="images/generated/two_stage_training.png" alt="兩階段訓練流程">
                <div class="caption">
                    <strong>流程圖：</strong>Stage 1 凍結大部分參數只訓練投影層，Stage 2 解凍 LLM 進行端到端微調。
                </div>
            </div>

            <!-- 3. Stage 1: 預訓練特徵對齊 -->
            <h2>❄️ Stage 1: 預訓練特徵對齊</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>目標：訓練一個「兼容的視覺分詞器」</h4>
                    <p>
                        第一階段的核心任務是<strong>特徵對齊</strong>：讓 CLIP 的視覺特徵能夠被 Vicuna「理解」。
                        想像一下，CLIP 和 Vicuna 說著不同的語言，投影層 \( \mathbf{W} \) 就是翻譯字典。
                        這個階段我們要訓練這本字典。
                    </p>
                </div>

                <div class="paradigm-grid">
                    <div class="paradigm-card" style="background: linear-gradient(135deg, #eff6ff 0%, #dbeafe 100%);">
                        <h4>訓練資料</h4>
                        <ul style="font-size: 0.9rem;">
                            <li><strong>來源</strong>：Conceptual Captions (CC3M)</li>
                            <li><strong>篩選後</strong>：595K 圖文對</li>
                            <li><strong>格式</strong>：簡單的「描述這張圖」指令</li>
                        </ul>
                        <p style="margin-top: 10px;"><strong>範例</strong>：</p>
                        <pre style="background: white; padding: 10px; border-radius: 8px; font-size: 0.85rem; margin-top: 5px;">
Q: "Describe the image briefly."
A: "A group of people walking on a beach."
                        </pre>
                    </div>
                    <div class="paradigm-card" style="background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%);">
                        <h4>可訓練參數</h4>
                        <ul style="font-size: 0.9rem;">
                            <li>✅ <strong>投影層 \( \mathbf{W} \)</strong>：約 2M 參數</li>
                            <li>❄️ <strong>CLIP 視覺編碼器</strong>：凍結</li>
                            <li>❄️ <strong>Vicuna 語言模型</strong>：凍結</li>
                        </ul>
                        <p style="margin-top: 10px;"><strong>記憶體需求</strong>：僅需載入投影層的梯度，非常高效</p>
                    </div>
                    <div class="paradigm-card" style="background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%);">
                        <h4>訓練超參數</h4>
                        <ul style="font-size: 0.9rem;">
                            <li><strong>Epoch</strong>：1 epoch</li>
                            <li><strong>Learning Rate</strong>：2e-3</li>
                            <li><strong>Batch Size</strong>：128</li>
                            <li><strong>Hardware</strong>：8× A100 GPU</li>
                        </ul>
                        <p style="margin-top: 10px;"><strong>訓練時間</strong>：約 4 小時</p>
                    </div>
                </div>

                <div class="key-concept">
                    <h4>💡 為什麼要凍結 CLIP 和 Vicuna？</h4>
                    <p><strong>保護預訓練知識</strong>：</p>
                    <ul>
                        <li><strong>CLIP</strong>：已經在 400M 圖文對上訓練，擁有強大的視覺理解能力。如果微調，可能會「忘記」這些知識。</li>
                        <li><strong>Vicuna</strong>：已經過指令微調，會用自然語言回答問題。如果在簡單的圖文對上訓練，可能會退化。</li>
                    </ul>
                    <p><strong>計算效率</strong>：只訓練 2M 參數遠比訓練 13B 參數快得多。</p>
                    <p><strong>避免過擬合</strong>：在資料有限的情況下，訓練所有參數容易過擬合。</p>
                </div>

                <div class="paper-section">
                    <div class="explanation">
                        <h4>這個階段學到了什麼？</h4>
                        <p>經過 Stage 1，投影層學會了：</p>
                        <ul>
                            <li>將「貓」的視覺特徵映射到 Vicuna 的「cat」word embedding 附近</li>
                            <li>將「跑步」的動作特徵映射到「running」附近</li>
                            <li>保持空間關係（「左邊的人」和「右邊的車」的相對位置）</li>
                        </ul>
                        <p style="margin-top: 15px;"><strong>類比</strong>：這就像讓兩個不同母語的人學會「手語」——一種共同的溝通方式。</p>
                    </div>
                </div>
            </div>

            <!-- 4. Stage 2: 端到端微調 -->
            <h2>🔥 Stage 2: 端到端微調</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>目標：學會「作為助手」與人類互動</h4>
                    <p>
                        第一階段讓模型學會了「看圖」，但還不會「對話」。
                        第二階段的任務是教模型如何：
                    </p>
                    <ul>
                        <li>理解複雜的人類指令（「解釋為什麼這個場景不尋常」）</li>
                        <li>進行多輪對話（記住之前的問答）</li>
                        <li>進行深度推理（「這個場景有什麼潛在風險？」）</li>
                        <li>以助手的口吻回應（自然、友善、詳細）</li>
                    </ul>
                </div>

                <div class="paradigm-grid">
                    <div class="paradigm-card" style="background: linear-gradient(135deg, #eff6ff 0%, #dbeafe 100%);">
                        <h4>訓練資料</h4>
                        <p><strong>場景 1：Multimodal Chatbot</strong></p>
                        <ul style="font-size: 0.9rem;">
                            <li>LLaVA-Instruct-158K（第 3 章生成的資料）</li>
                            <li>涵蓋對話、描述、推理三種類型</li>
                            <li>在訓練時均勻採樣這三種類型</li>
                        </ul>
                        <p style="margin-top: 10px;"><strong>場景 2：Science QA</strong></p>
                        <ul style="font-size: 0.9rem;">
                            <li>21K 科學問答題</li>
                            <li>格式：問題 + 圖片 → 推理過程 + 答案</li>
                        </ul>
                    </div>
                    <div class="paradigm-card" style="background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%);">
                        <h4>可訓練參數</h4>
                        <ul style="font-size: 0.9rem;">
                            <li>✅ <strong>投影層 \( \mathbf{W} \)</strong>：繼續訓練</li>
                            <li>✅ <strong>Vicuna 語言模型</strong>：<strong>解凍</strong>！</li>
                            <li>❄️ <strong>CLIP 視覺編碼器</strong>：保持凍結</li>
                        </ul>
                        <p style="margin-top: 10px;"><strong>為什麼不解凍 CLIP？</strong>視覺編碼器已經足夠強大，微調可能破壞其泛化能力。</p>
                    </div>
                    <div class="paradigm-card" style="background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%);">
                        <h4>訓練超參數</h4>
                        <ul style="font-size: 0.9rem;">
                            <li><strong>Epoch</strong>：3 epochs (Chatbot) / 12 epochs (Science QA)</li>
                            <li><strong>Learning Rate</strong>：2e-5（比 Stage 1 小 100 倍）</li>
                            <li><strong>Batch Size</strong>：32</li>
                            <li><strong>Hardware</strong>：8× A100 GPU</li>
                        </ul>
                        <p style="margin-top: 10px;"><strong>訓練時間</strong>：約 10 小時（Chatbot）</p>
                    </div>
                </div>

                <div class="key-concept">
                    <h4>💡 為什麼 Learning Rate 變小了？</h4>
                    <p>
                        <strong>Stage 1</strong>：Learning rate = 2e-3（較大）<br>
                        <strong>Stage 2</strong>：Learning rate = 2e-5（小 100 倍）
                    </p>
                    <p style="margin-top: 10px;"><strong>原因</strong>：</p>
                    <ul>
                        <li><strong>Stage 1</strong>：投影層是隨機初始化的，需要大步長才能快速收斂</li>
                        <li><strong>Stage 2</strong>：Vicuna 已經過精心微調，大步長會破壞其預訓練知識。小步長確保「微調」而不是「重訓」。</li>
                    </ul>
                </div>

                <div class="paper-section">
                    <div class="explanation">
                        <h4>這個階段學到了什麼？</h4>
                        <p>經過 Stage 2，LLaVA 學會了：</p>
                        <ul>
                            <li><strong>指令理解</strong>：區分「描述圖片」和「解釋為什麼」的差異</li>
                            <li><strong>推理能力</strong>：從視覺線索推導出隱含資訊（「這個場景不安全因為...」）</li>
                            <li><strong>對話記憶</strong>：在多輪對話中記住之前的問答</li>
                            <li><strong>助手風格</strong>：使用「在這張圖中，我看到...」的自然語言</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- 5. 兩階段的哲學 -->
            <h2>🧘 兩階段訓練的哲學</h2>
            <div class="paper-section">
                <div class="key-concept">
                    <h4>為什麼不一步到位？</h4>
                    <p>你可能會問：為什麼不直接在 LLaVA-Instruct-158K 上訓練所有參數？</p>
                    
                    <p style="margin-top: 15px;"><strong>原因 1：避免災難性遺忘</strong></p>
                    <p>
                        如果一開始就解凍 Vicuna，它可能會「忘記」如何好好說話。
                        158K 資料相比 Vicuna 的原始訓練資料（70K ShareGPT + 數十億預訓練 tokens）太少了。
                    </p>

                    <p style="margin-top: 15px;"><strong>原因 2：對齊是基礎</strong></p>
                    <p>
                        如果視覺特徵沒有對齊到語言空間，LLM 看到的就是「亂碼」。
                        先用大量圖文對（595K）做對齊，確保基礎穩固。
                    </p>

                    <p style="margin-top: 15px;"><strong>原因 3：計算效率</strong></p>
                    <p>
                        Stage 1 只訓練 2M 參數，可以快速迭代測試不同的資料配置。
                        確定方向正確後，再投入資源訓練 13B 參數。
                    </p>
                </div>

                <div class="key-concept">
                    <h4>💡 類比：學習外語的過程</h4>
                    <p><strong>Stage 1（特徵對齊）</strong>：</p>
                    <p>
                        就像學外語時先學「單詞表」——把中文的「貓」對應到英文的「cat」。
                        這個階段只需要大量的單詞和簡單句子，不需要複雜對話。
                    </p>
                    <p style="margin-top: 15px;"><strong>Stage 2（端到端微調）</strong>：</p>
                    <p>
                        學會單詞後，開始練習「對話」——如何問問題、如何回答、如何連貫表達。
                        這個階段需要高品質的對話範例，而不只是單詞表。
                    </p>
                </div>
            </div>

            <div class="quote-block">
                「先學會『看』，再學會『說』——這是 LLaVA 訓練的核心哲學。」
            </div>

            <!-- 6. 訓練細節與技巧 -->
            <h2>⚙️ 訓練細節與技巧</h2>
            <div class="paper-section">
                <div class="explanation">
                    <h4>超參數遵循 Vicuna</h4>
                    <p>
                        LLaVA 的訓練超參數（optimizer, warmup, weight decay 等）
                        完全遵循 Vicuna 的設定。這是因為：
                    </p>
                    <ul>
                        <li>Vicuna 已經過精心調優，沒必要重新搜索超參數</li>
                        <li>保持一致性有助於公平比較</li>
                        <li>節省大量實驗時間</li>
                    </ul>
                </div>

                <div class="paradigm-grid">
                    <div class="paradigm-card">
                        <h4>資料採樣策略</h4>
                        <p>在 Multimodal Chatbot 訓練中，三種資料類型均勻採樣：</p>
                        <ul style="font-size: 0.9rem;">
                            <li>Conversation（多輪對話）</li>
                            <li>Detailed Description（詳細描述）</li>
                            <li>Complex Reasoning（複雜推理）</li>
                        </ul>
                        <p style="margin-top: 10px;"><strong>重要性</strong>：確保模型在所有能力上均衡發展</p>
                    </div>
                    <div class="paradigm-card">
                        <h4>圖片位置隨機化</h4>
                        <p>在第一輪對話中，圖片 token 隨機放在問題前或後：</p>
                        <ul style="font-size: 0.9rem;">
                            <li>50% 機率：[圖片] [問題]</li>
                            <li>50% 機率：[問題] [圖片]</li>
                        </ul>
                        <p style="margin-top: 10px;"><strong>目的</strong>：避免模型過度依賴位置資訊</p>
                    </div>
                    <div class="paradigm-card">
                        <h4>Gradient Checkpointing</h4>
                        <p>為了在 8× A100 (40GB) 上訓練 13B 模型，使用了：</p>
                        <ul style="font-size: 0.9rem;">
                            <li>Gradient Checkpointing（犧牲計算換記憶體）</li>
                            <li>Mixed Precision Training (FP16)</li>
                            <li>Distributed Data Parallel (DDP)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- 7. 消融實驗洞察 -->
            <h2>🔬 消融實驗：設計選擇的驗證</h2>
            <div class="paper-section">
                <div class="explanation">
                    <p>LLaVA 論文進行了多個消融實驗（詳見第 6 章），以下是與訓練策略相關的關鍵發現：</p>
                </div>

                <div class="paradigm-grid" style="grid-template-columns: 1fr;">
                    <div class="paradigm-card">
                        <h4>🧪 實驗 1：跳過預訓練</h4>
                        <p><strong>設定</strong>：直接在 LLaVA-Instruct-158K 上訓練，不經過 Stage 1</p>
                        <p><strong>結果</strong>：Science QA 準確率從 90.92% 降到 85.81%（<span style="color: red;">-5.11%</span>）</p>
                        <p><strong>結論</strong>：預訓練階段至關重要，它為後續微調奠定了基礎</p>
                    </div>
                    
                    <div class="paradigm-card">
                        <h4>🧪 實驗 2：資料類型的影響</h4>
                        <p><strong>設定</strong>：只用 Conversation 資料訓練，移除 Detailed Description 和 Complex Reasoning</p>
                        <p><strong>結果</strong>：LLaVA-Bench (COCO) 整體分數從 85.1% 降到 73.8%（<span style="color: red;">-11.3%</span>）</p>
                        <p><strong>結論</strong>：三種資料類型的多樣性對模型能力至關重要</p>
                    </div>

                    <div class="paradigm-card">
                        <h4>🧪 實驗 3：Chain-of-Thought 的順序</h4>
                        <p><strong>設定 A</strong>：先生成推理過程，再給出答案（reasoning-first）</p>
                        <p><strong>設定 B</strong>：先給出答案，再解釋推理（answer-first）</p>
                        <p><strong>結果</strong>：reasoning-first 收斂更快（6 epochs vs 12 epochs），最終準確率相當</p>
                        <p><strong>結論</strong>：CoT 風格的訓練加速收斂，但不改變最終效能上限</p>
                    </div>
                </div>
            </div>

            <!-- 8. 總結 -->
            <h2>📌 本章重點回顧</h2>
            <div class="chapter-summary">
                <h3>核心要點</h3>
                <ul>
                    <li><strong>兩階段策略</strong>：Stage 1 特徵對齊 + Stage 2 端到端微調</li>
                    <li><strong>Stage 1</strong>：凍結 CLIP 和 Vicuna，只訓練投影層（595K 圖文對，4 小時）</li>
                    <li><strong>Stage 2</strong>：解凍 Vicuna，訓練投影層 + LLM（158K 指令資料，10 小時）</li>
                    <li><strong>設計哲學</strong>：先學會「看」，再學會「說」；保護預訓練知識，漸進式學習</li>
                    <li><strong>關鍵驗證</strong>：跳過 Stage 1 會導致 5% 的效能下降</li>
                </ul>
                
                <h3>下一章預告</h3>
                <p>
                    第 6 章將展示 LLaVA 的實驗結果——它在 LLaVA-Bench 和 Science QA 上的驚人表現，
                    以及與 GPT-4、BLIP-2、OpenFlamingo 的詳細對比。
                </p>
            </div>

            <div class="nav-buttons" style="display: flex; justify-content: space-between; margin-top: 40px;">
                <a href="04-architecture.html" class="nav-button">
                    ← 上一章：模型架構解析
                </a>
                <a href="06-experiments.html" class="nav-button nav-button-next">
                    下一章：實驗結果與分析 →
                </a>
            </div>
        </div>
    </div>
</body>
</html>
