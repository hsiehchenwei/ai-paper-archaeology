<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Transformer 論文深度解析 - 第6頁：Results</title>
    <link rel="stylesheet" href="../styles/global.css" />
    <link rel="stylesheet" href="../styles/paper-reading.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=JetBrains+Mono:wght@400;700&family=Merriweather:ital,wght@0,400;0,700;1,400&display=swap"
      rel="stylesheet"
    />
    <!-- MathJax for mathematical formulas -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$']],
          displayMath: [['$$', '$$']]
        }
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <div class="container">
      <h1>
        📖 Attention Is All You Need <br /><span
          style="
            font-size: 0.6em;
            color: var(--text-secondary);
            font-weight: normal;
          "
          >深度解析系列</span
        >
      </h1>
      <h2>第 6 頁：Results（實驗結果）</h2>

      <div class="header-section">
        <div class="paper-meta">
          <div class="meta-item">
            <strong>目前章節</strong>
            6. Results
          </div>
          <div class="meta-item">
            <strong>核心主題</strong>
            機器翻譯、模型變體、泛化能力
          </div>
          <div class="meta-item">
            <strong>閱讀難度</strong>
            ⭐⭐ (實驗數據分析)
          </div>
        </div>
      </div>

      <!-- ============= 6.1 Machine Translation ============= -->
      <h3>1. 機器翻譯結果 (Machine Translation)</h3>

      <div class="text-pair">
        <div class="original-text">
          On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.
        </div>
        <div class="translation">
          <p>
            在 WMT 2014 英德翻譯任務中，大型 Transformer 模型（表 2 中的 Transformer (big)）比之前報告的最佳模型（包括集成模型）高出超過 <strong>2.0 BLEU</strong>，建立了新的 SOTA（State-of-the-art）分數 <strong>28.4 BLEU</strong>。此模型的配置列在表 3 的最底行。訓練時間為 8 個 P100 GPU 上 <strong>3.5 天</strong>。即使是我們的基礎模型也超越了所有先前發表的模型和集成，且訓練成本只是競爭模型的<strong>一小部分</strong>。
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>🎯 什麼是 BLEU Score？</h4>
        <p><strong>BLEU (Bilingual Evaluation Understudy)</strong> 是機器翻譯品質的標準評估指標。</p>
        <ul>
          <li><strong>計算原理：</strong>比對「機器翻譯」與「人類參考翻譯」的 n-gram 重疊率。</li>
          <li><strong>分數範圍：</strong>0-100 分（越高越好）。</li>
          <li><strong>產業標準：</strong>
            <ul>
              <li>BLEU < 20：翻譯品質差</li>
              <li>20-30：可接受</li>
              <li>30-40：良好（專業水準）</li>
              <li>> 40：接近人類翻譯</li>
            </ul>
          </li>
        </ul>
      </div>

      <div class="figure-container">
        <img src="images/table-2-results.png" alt="Table 2: Translation Results" />
        <div class="figure-caption">
          <strong>表 2：翻譯結果對比</strong><br>
          Transformer 在英德、英法翻譯任務上都達到了 SOTA，且訓練成本遠低於競爭對手。
        </div>
      </div>

      <div class="explanation">
        <h4>📊 表 2 解讀：為什麼 Transformer 是「革命性」的？</h4>
        <p><strong>1. 性能突破 (BLEU 分數)</strong></p>
        <ul>
          <li><strong>英德翻譯 (EN-DE)：</strong>
            <ul>
              <li>之前最佳：ConvS2S Ensemble = <strong>26.36</strong></li>
              <li>Transformer (big)：<strong>28.4</strong> → 提升 <strong>+2.04 BLEU</strong></li>
              <li>這在機器翻譯界是<strong>巨大的進步</strong>（通常提升 0.5 就算重大突破）</li>
            </ul>
          </li>
          <li><strong>英法翻譯 (EN-FR)：</strong>
            <ul>
              <li>之前最佳：ConvS2S Ensemble = <strong>41.29</strong></li>
              <li>Transformer (big)：<strong>41.8</strong></li>
              <li>持平甚至略勝，但看下一個指標更驚人...</li>
            </ul>
          </li>
        </ul>

        <p><strong>2. 訓練成本 (FLOPs：浮點運算次數)</strong></p>
        <table>
          <tr>
            <th>模型</th>
            <th>英德成本</th>
            <th>英法成本</th>
            <th>與 Transformer 對比</th>
          </tr>
          <tr>
            <td>ConvS2S Ensemble</td>
            <td>$7.7 \times 10^{19}$</td>
            <td>$1.2 \times 10^{21}$</td>
            <td>英德成本是 Transformer 的 <strong>33 倍</strong></td>
          </tr>
          <tr>
            <td>GNMT + RL Ensemble</td>
            <td>$1.8 \times 10^{20}$</td>
            <td>$1.1 \times 10^{21}$</td>
            <td>英德成本是 Transformer 的 <strong>78 倍</strong></td>
          </tr>
          <tr>
            <td><strong>Transformer (big)</strong></td>
            <td>$2.3 \times 10^{19}$</td>
            <td>-</td>
            <td><strong>基準</strong></td>
          </tr>
        </table>
        <p><strong>結論：</strong>Transformer 不僅表現更好，而且<strong>便宜得多</strong>。這就是為什麼它能迅速取代 RNN/CNN 成為主流。</p>
      </div>

      <div class="analogy">
        <h4>🤖 AI 體驗連結：為什麼 ChatGPT 能如此快速迭代？</h4>
        <p><strong>訓練成本 = 創新速度</strong></p>
        <ul>
          <li><strong>舊時代 (RNN)：</strong>訓練一個 SOTA 模型需要數月、成本高昂 → 研究進展緩慢。</li>
          <li><strong>Transformer 時代：</strong>訓練成本降低 10-100 倍 → 可以快速實驗各種變體。</li>
        </ul>
        <p><strong>這就是為什麼 2017-2024 短短 7 年，AI 從「會翻譯」進化到「會寫詩、寫程式、畫圖」：</strong></p>
        <ul>
          <li>2017：Transformer 論文（翻譯 SOTA）</li>
          <li>2018：BERT（理解）、GPT-1（生成）</li>
          <li>2019：GPT-2（1.5B 參數）</li>
          <li>2020：GPT-3（175B 參數，出現「湧現能力」）</li>
          <li>2022：ChatGPT（RLHF，對話能力）</li>
          <li>2023：GPT-4（多模態）</li>
          <li>2024：持續進化中...</li>
        </ul>
        <p>如果訓練成本還是 2017 年的 RNN 水平，我們可能還停留在「能翻譯句子」的階段。</p>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <!-- ============= 6.2 Model Variations ============= -->
      <h3>2. 模型變體實驗 (Model Variations)</h3>

      <div class="text-pair">
        <div class="original-text">
          To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.
        </div>
        <div class="translation">
          <p>
            為了評估 Transformer 不同組件的重要性，我們以不同方式變化基礎模型，測量在英德翻譯開發集（newstest2013）上的性能變化。我們使用了前面描述的束搜索（beam search），但沒有進行檢查點平均。我們在表 3 中展示這些結果。
          </p>
        </div>
      </div>

      <div class="figure-container">
        <img src="images/table-3-variations.png" alt="Table 3: Model Variations" />
        <div class="figure-caption">
          <strong>表 3：模型變體消融實驗</strong><br>
          系統性地測試每個超參數的影響，揭示哪些設計選擇是關鍵。
        </div>
      </div>

      <div class="explanation">
        <h4>🔬 表 3 解讀：消融實驗 (Ablation Study)</h4>
        <p><strong>消融實驗 (Ablation Study)：</strong>像外科手術一樣，逐個「切除」或「調整」模型的某個部分，觀察性能變化，從而理解每個組件的貢獻。</p>

        <p><strong>(A) 多頭注意力的頭數 ($h$) 實驗</strong></p>
        <table>
          <tr>
            <th>頭數 ($h$)</th>
            <th>$d_k$ (每頭維度)</th>
            <th>PPL (越低越好)</th>
            <th>BLEU (越高越好)</th>
            <th>結論</th>
          </tr>
          <tr>
            <td>1</td>
            <td>512</td>
            <td>5.29</td>
            <td>24.9</td>
            <td>❌ 單頭表現最差</td>
          </tr>
          <tr>
            <td>4</td>
            <td>128</td>
            <td>5.00</td>
            <td>25.5</td>
            <td>✅ 接近最佳</td>
          </tr>
          <tr>
            <td><strong>8 (base)</strong></td>
            <td><strong>64</strong></td>
            <td><strong>4.92</strong></td>
            <td><strong>25.8</strong></td>
            <td>🏆 最佳平衡</td>
          </tr>
          <tr>
            <td>16</td>
            <td>32</td>
            <td>4.91</td>
            <td>25.8</td>
            <td>⚠️ 持平（但計算量更大）</td>
          </tr>
          <tr>
            <td>32</td>
            <td>16</td>
            <td>5.01</td>
            <td>25.4</td>
            <td>❌ 太多頭反而變差</td>
          </tr>
        </table>
        <p><strong>啟示：</strong>多頭注意力確實有效，但並非越多越好。<strong>8 個頭</strong>是最佳平衡點（多樣性 vs 每頭容量）。</p>

        <p><strong>(B) Key 維度 ($d_k$) 實驗</strong></p>
        <p>降低 $d_k$（16 或 32）會<strong>降低性能</strong>。這表明「兼容性計算」需要足夠的表達空間。</p>

        <p><strong>(C) 層數 ($N$) 和模型大小 ($d_{model}$) 實驗</strong></p>
        <table>
          <tr>
            <th>層數 ($N$)</th>
            <th>$d_{model}$</th>
            <th>PPL</th>
            <th>BLEU</th>
            <th>參數量 (M)</th>
          </tr>
          <tr>
            <td>2</td>
            <td>512</td>
            <td>6.11</td>
            <td>23.7</td>
            <td>36</td>
          </tr>
          <tr>
            <td>4</td>
            <td>512</td>
            <td>5.19</td>
            <td>25.3</td>
            <td>50</td>
          </tr>
          <tr>
            <td><strong>6 (base)</strong></td>
            <td><strong>512</strong></td>
            <td><strong>4.92</strong></td>
            <td><strong>25.8</strong></td>
            <td><strong>65</strong></td>
          </tr>
          <tr>
            <td>8</td>
            <td>512</td>
            <td>4.88</td>
            <td>25.5</td>
            <td>80</td>
          </tr>
        </table>
        <p><strong>結論：</strong>更深的模型通常更好，但 6 層已經是很好的平衡點（8 層提升不明顯）。</p>

        <p><strong>(D) Dropout 的影響</strong></p>
        <ul>
          <li><strong>Dropout = 0.0：</strong>PPL 5.77，BLEU 24.6（<strong>過擬合</strong>）</li>
          <li><strong>Dropout = 0.1：</strong>PPL 4.92，BLEU 25.8（<strong>最佳</strong>）</li>
          <li><strong>Dropout = 0.2：</strong>性能略降（正則化過強）</li>
        </ul>
        <p><strong>Dropout 非常重要！</strong>沒有它，模型會嚴重過擬合。</p>

        <p><strong>(E) 位置編碼：Sinusoidal vs Learned</strong></p>
        <p>兩種方法結果<strong>幾乎相同</strong>（PPL 4.92 vs 4.92，BLEU 25.7 vs 25.7）。論文選擇 Sinusoidal 是因為它可以<strong>外推到更長的序列</strong>（超過訓練時見過的長度）。</p>
      </div>

      <div class="analogy">
        <h4>🤖 AI 體驗連結：為什麼 ChatGPT 有「基礎版」和「Plus 版」？</h4>
        <p>表 3 告訴我們：<strong>模型大小 vs 性能 vs 成本</strong>的權衡。</p>
        <ul>
          <li><strong>Base Model (65M 參數)：</strong>快速、便宜、性能已經很好（BLEU 25.8）→ 適合「免費版」ChatGPT。</li>
          <li><strong>Big Model (213M 參數)：</strong>更慢、更貴、性能更佳（BLEU 26.4）→ 適合「Plus 版」或「API 付費使用」。</li>
        </ul>
        <p><strong>現代 LLM 的策略：</strong></p>
        <ul>
          <li>GPT-3.5 (基礎版)：便宜、快、能應付大部分任務。</li>
          <li>GPT-4 (高階版)：貴、慢、但能處理更複雜的推理。</li>
        </ul>
        <p>這跟 Transformer 論文中的 base vs big 是同樣的邏輯！</p>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <!-- ============= 6.3 English Constituency Parsing ============= -->
      <h3>3. 泛化能力：英語句法分析 (English Constituency Parsing)</h3>

      <div class="text-pair">
        <div class="original-text">
          To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.
        </div>
        <div class="translation">
          <p>
            為了評估 Transformer 是否能<strong>泛化到其他任務</strong>，我們在英語成分句法分析（constituency parsing）上進行了實驗。這個任務具有特定挑戰：輸出受到<strong>強結構約束</strong>，並且比輸入<strong>明顯更長</strong>。此外，RNN 序列到序列模型在<strong>小數據</strong>情境下無法達到 SOTA 結果。
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>🌳 什麼是 Constituency Parsing（成分句法分析）？</h4>
        <p>這是一個<strong>語言學任務</strong>：將句子分解成樹狀結構，顯示詞語如何組成短語。</p>
        <p><strong>範例：</strong></p>
        <pre><code>輸入句子： "The cat sat on the mat."

輸出樹狀結構：
              (S
                (NP (DT The) (NN cat))
                (VP (VBD sat)
                    (PP (IN on)
                        (NP (DT the) (NN mat))))
                (. .))

解釋：
- S = Sentence（句子）
- NP = Noun Phrase（名詞短語）
- VP = Verb Phrase（動詞短語）
- PP = Prepositional Phrase（介詞短語）
</code></pre>
        <p><strong>為什麼這個任務困難？</strong></p>
        <ul>
          <li>輸出比輸入長很多（句子 6 詞 → 樹狀結構有幾十個符號）</li>
          <li>需要理解深層語法結構</li>
          <li>訓練數據相對較少（只有約 40K 句子）</li>
        </ul>
      </div>

      <div class="figure-container">
        <img src="images/table-4-parsing.png" alt="Table 4: Constituency Parsing Results" />
        <div class="figure-caption">
          <strong>表 4：句法分析結果</strong><br>
          Transformer 在非翻譯任務上也表現出色，證明其架構的通用性。
        </div>
      </div>

      <div class="explanation">
        <h4>🎓 表 4 解讀：Transformer 的泛化能力</h4>
        <p><strong>F1 Score：</strong>衡量分析準確度的指標（0-100，越高越好）。90+ 已經是專業水準。</p>

        <p><strong>WSJ only（僅用華爾街日報數據，約 40K 句子）</strong></p>
        <table>
          <tr>
            <th>模型</th>
            <th>F1 分數</th>
            <th>結論</th>
          </tr>
          <tr>
            <td>Vinyals & Kaiser (2014)</td>
            <td>88.3</td>
            <td>早期 RNN seq2seq</td>
          </tr>
          <tr>
            <td>Petrov et al. (2006)</td>
            <td>90.4</td>
            <td>傳統統計方法</td>
          </tr>
          <tr>
            <td>Dyer et al. (2016)</td>
            <td>91.7</td>
            <td>RNN Grammar（當時 SOTA）</td>
          </tr>
          <tr>
            <td><strong>Transformer (4 layers)</strong></td>
            <td><strong>91.3</strong></td>
            <td>✅ 接近 SOTA，且<strong>沒有任務特定調整</strong>！</td>
          </tr>
        </table>

        <p><strong>Semi-supervised（半監督，使用 17M 額外句子）</strong></p>
        <table>
          <tr>
            <th>模型</th>
            <th>F1 分數</th>
          </tr>
          <tr>
            <td>McClosky et al. (2006)</td>
            <td>92.1</td>
          </tr>
          <tr>
            <td>Vinyals & Kaiser (2014)</td>
            <td>92.1</td>
          </tr>
          <tr>
            <td><strong>Transformer (4 layers)</strong></td>
            <td><strong>92.7</strong></td>
          </tr>
          <tr>
            <td>Luong et al. (2015) multi-task</td>
            <td>93.0</td>
          </tr>
          <tr>
            <td>Dyer et al. (2016) generative</td>
            <td>93.3</td>
          </tr>
        </table>
        <p><strong>Transformer 在半監督設定下達到 92.7，超越多數專門方法！</strong></p>

        <p><strong>🔑 關鍵發現：</strong></p>
        <ul>
          <li>Transformer <strong>不需任何任務特定設計</strong>，就能在句法分析上表現優異。</li>
          <li>這證明了 Transformer 是一個<strong>通用架構</strong>（General-purpose architecture）。</li>
          <li>對比 RNN：在小數據情境下無法達到好結果，但 Transformer 可以！</li>
        </ul>
      </div>

      <div class="analogy">
        <h4>🤖 AI 體驗連結：為什麼同一個 ChatGPT 能做這麼多事？</h4>
        <p><strong>表 4 的啟示：Transformer 的「通才」特性</strong></p>
        <ul>
          <li><strong>原始設計：</strong>機器翻譯（seq2seq）</li>
          <li><strong>表 4 實驗：</strong>句法分析（輸出比輸入長，強結構約束）</li>
          <li><strong>實際應用：</strong>
            <ul>
              <li>GPT-3/4：文本生成、對話、摘要、程式碼...</li>
              <li>BERT：情感分析、實體識別、問答...</li>
              <li>DALL-E：文字生成圖片</li>
              <li>Whisper：語音識別</li>
            </ul>
          </li>
        </ul>
        <p><strong>這就是為什麼 Transformer 成為「萬用架構」：</strong></p>
        <ul>
          <li>不需要為每個任務設計專門的網路結構</li>
          <li>只要調整輸入輸出格式和訓練數據，就能應用到新任務</li>
          <li>這大大加速了 AI 研究的進展速度</li>
        </ul>
      </div>

      <div class="nav-bar">
        <a href="05-training.html" class="nav-btn">
          ← 上一頁：Training
        </a>
        <a href="07-conclusion.html" class="nav-btn primary">
          下一頁：Conclusion →
        </a>
      </div>

      <div
        style="
          margin-top: 40px;
          padding: 20px;
          background: #ecf0f1;
          border-radius: 5px;
          text-align: center;
        "
      >
        <p><strong>📚 學習檢查點</strong></p>
        <ul style="text-align: left; max-width: 600px; margin: 20px auto">
          <li>✅ <strong>BLEU Score：</strong>Transformer 在翻譯任務上達到 SOTA，且成本遠低於競爭對手。</li>
          <li>✅ <strong>消融實驗：</strong>8 個注意力頭、6 層深度、適當的 Dropout 是最佳配置。</li>
          <li>✅ <strong>泛化能力：</strong>Transformer 在句法分析等非翻譯任務上也表現優異，證明其通用性。</li>
          <li>✅ <strong>工程價值：</strong>低訓練成本 + 高性能 = 可快速迭代 → 推動了整個 AI 領域的爆發式發展。</li>
        </ul>
      </div>
    </div>
  </body>
</html>

