<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Transformer 論文深度解析 - 第3頁：Model Architecture (2/2)</title>
    <link rel="stylesheet" href="../styles/global.css" />
    <link rel="stylesheet" href="../styles/paper-reading.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=JetBrains+Mono:wght@400;700&family=Merriweather:ital,wght@0,400;0,700;1,400&display=swap"
      rel="stylesheet"
    />
    <!-- MathJax for mathematical formulas -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$']],
          displayMath: [['$$', '$$']]
        }
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <div class="container">
      <h1>
        📖 Attention Is All You Need <br /><span
          style="
            font-size: 0.6em;
            color: var(--text-secondary);
            font-weight: normal;
          "
          >深度解析系列</span
        >
      </h1>
      <h2>第 3 頁 (2/2)：Model Architecture - 其他關鍵組件</h2>

      <div class="header-section">
        <div class="paper-meta">
          <div class="meta-item">
            <strong>目前章節</strong>
            3. Model Architecture (Part 2)
          </div>
          <div class="meta-item">
            <strong>核心主題</strong>
            Feed-Forward Networks、Embeddings、Positional Encoding
          </div>
          <div class="meta-item">
            <strong>閱讀難度</strong>
            ⭐⭐⭐⭐⭐ (Positional Encoding 概念較抽象)
          </div>
        </div>
      </div>

      <!-- ============= Feed-Forward Networks ============= -->
      <h3>1. Position-wise Feed-Forward Networks（逐位置前饋網路）</h3>

      <div class="text-pair">
        <div class="original-text">
          In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.
          $$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$
        </div>
        <div class="translation">
          <p>
            除了注意力子層之外，我們的編碼器和解碼器中的每一層都包含一個<strong>全連接前饋網路</strong>，該網路對每個位置<strong>分別且相同地</strong>應用。這由兩個線性變換組成，中間有一個 ReLU 激活函數。
          </p>
          <p style="text-align: center; font-size: 1.2em; margin: 20px 0;">
            $$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$
          </p>
        </div>
      </div>

      <div class="explanation">
        <h4>🧮 FFN 的結構拆解</h4>
        
        <p><strong>完整計算流程：</strong></p>
        <pre><code>輸入：x (維度 512)
    ↓
第一層線性變換：xW₁ + b₁ (512 → 2048)
    ↓
ReLU 激活：max(0, ·) (去除負值)
    ↓
第二層線性變換：·W₂ + b₂ (2048 → 512)
    ↓
輸出：FFN(x) (維度 512)</code></pre>

        <p><strong>關鍵參數（Transformer Base）：</strong></p>
        <table>
          <tr>
            <th>參數</th>
            <th>維度</th>
            <th>說明</th>
          </tr>
          <tr>
            <td>輸入維度 $d_{model}$</td>
            <td>512</td>
            <td>每個詞的表示向量</td>
          </tr>
          <tr>
            <td>隱藏層維度 $d_{ff}$</td>
            <td>2048</td>
            <td>中間層放大 4 倍</td>
          </tr>
          <tr>
            <td>$W_1$</td>
            <td>$(512 \times 2048)$</td>
            <td>第一層權重</td>
          </tr>
          <tr>
            <td>$W_2$</td>
            <td>$(2048 \times 512)$</td>
            <td>第二層權重</td>
          </tr>
          <tr>
            <td>參數總量</td>
            <td>$\approx 2M$</td>
            <td>$(512 \times 2048 + 2048 \times 512)$</td>
          </tr>
        </table>
      </div>

      <div class="text-pair">
        <div class="original-text">
          While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
        </div>
        <div class="translation">
          <p>
            雖然線性變換在不同位置上是<strong>相同的</strong>，但它們在不同層之間使用<strong>不同的參數</strong>。另一種描述方式是將其視為兩個核大小為 1 的卷積。
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>🎯 "Position-wise" 的意義</h4>
        <p><strong>核心特性：</strong>每個位置的詞<strong>獨立處理</strong>，不看鄰居。</p>
        
        <pre><code>例子：輸入序列 ["The", "cat", "sat"]

位置 0: FFN(x₀) = ...  // 只看 "The" 的向量
位置 1: FFN(x₁) = ...  // 只看 "cat" 的向量
位置 2: FFN(x₂) = ...  // 只看 "sat" 的向量

// 三個位置使用「相同的 FFN」但「不互相溝通」</code></pre>

        <p><strong>為什麼要這樣設計？</strong></p>
        <ul>
          <li><strong>並行化：</strong>所有位置可以同時計算（GPU 友好）</li>
          <li><strong>角色分工：</strong>Attention 負責「看全局」，FFN 負責「獨立處理」</li>
        </ul>
      </div>

      <div class="analogy">
        <h4>🔧 雙重類比：Feed-Forward Network</h4>
        
        <h4>💡 生活類比：流水線上的獨立加工站</h4>
        <p><strong>情境：</strong>一條組裝線上有 3 個零件（詞），每個零件經過同一台機器加工。</p>
        <ul>
          <li><strong>Attention（前一步）：</strong>品管員檢查所有零件之間的配合度（全局檢查）。</li>
          <li><strong>FFN（這一步）：</strong>每個零件<strong>單獨進入</strong>同一台機器（相同的 $W_1, W_2$），獨立打磨、拋光、噴漆。</li>
          <li><strong>關鍵：</strong>機器不看其他零件，只專注當前這一個。</li>
        </ul>

        <h4>🔧 工程類比：Map 操作（函數式編程）</h4>
        <pre><code>// Attention: 全局操作（每個元素看所有其他元素）
output = attention(sequence)  // O(n²)

// FFN: 獨立操作（可並行）
output = sequence.map(x => FFN(x))  // O(n)，完全並行

// 偽代碼實作
function FFN(x):
    hidden = ReLU(x @ W1 + b1)  // 512 → 2048
    output = hidden @ W2 + b2    // 2048 → 512
    return output

// 並行執行
results = [FFN(x[0]), FFN(x[1]), ..., FFN(x[511])]</code></pre>
      </div>

      <div class="problem">
        <h4>❓ 為什麼要「放大再縮小」（512 → 2048 → 512）？</h4>
        <p><strong>問題：</strong>為什麼不直接 512 → 512？</p>
        <p><strong>答案：</strong>增加「非線性表達能力」。</p>
        <ol>
          <li><strong>放大 (512 → 2048)：</strong>
            <br>提供更大的「特徵空間」，讓模型學習複雜的非線性組合。
            <br><em>類比：把壓縮的數據解壓到更大的工作區。</em>
          </li>
          <li><strong>ReLU：</strong>
            <br>引入非線性（否則兩個線性層等價於一個線性層）。
          </li>
          <li><strong>縮小 (2048 → 512)：</strong>
            <br>濃縮精華，回到原本的維度，準備進入下一層。
            <br><em>類比：處理完後重新打包。</em>
          </li>
        </ol>
        <p><strong>工程類比：</strong>類似 AutoEncoder 的 bottleneck 設計，先展開再壓縮。</p>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <!-- ============= Embeddings and Softmax ============= -->
      <h3>2. Embeddings and Softmax（詞嵌入與輸出層）</h3>

      <div class="text-pair">
        <div class="original-text">
          Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{model}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by $\sqrt{d_{model}}$.
        </div>
        <div class="translation">
          <p>
            與其他序列轉換模型類似，我們使用<strong>學習的嵌入</strong>將輸入詞元和輸出詞元轉換為維度 $d_{model}$ 的向量。我們還使用通常的學習線性變換和 softmax 函數，將解碼器輸出轉換為預測的下一個詞元的機率。在我們的模型中，我們在兩個嵌入層和 softmax 前的線性變換之間<strong>共享相同的權重矩陣</strong>。在嵌入層中，我們將這些權重乘以 $\sqrt{d_{model}}$。
          </p>
        </div>
      </div>

      <div class="explanation">
        <h4>🏗️ Embeddings 的作用</h4>
        
        <p><strong>Embedding 的任務：</strong>把每個「詞」(token) 轉換成一個 <strong>512 維的向量</strong>（回顧第1頁：我們學過為何需要向量表示）。</p>

        <h5>💡 生活類比：學生證號 → 個人檔案</h5>
        <pre><code>// 輸入：詞的 ID（整數）
token_id = 1234  // "cat" 這個詞的編號

// 查表：從 Embedding 矩陣中取出對應的向量
embedding_matrix = [
    [0.12, -0.34, ...],  // token 0 的向量 (512維)
    [0.45,  0.67, ...],  // token 1 的向量
    ...
    [0.25, -0.67, 0.89, ...],  // token 1234 的向量 ← "cat"
]

// 輸出：512 維向量
vector = embedding_matrix[1234]  // [0.25, -0.67, 0.89, ...]</code></pre>

        <p><strong>就像：</strong></p>
        <ul>
          <li>輸入學號「1234」</li>
          <li>查學生資料庫</li>
          <li>得到完整個人檔案（姓名、年齡、科系、成績...）</li>
        </ul>

        <table>
          <tr>
            <th>詞（Token）</th>
            <th>Token ID</th>
            <th>Embedding 向量 (簡化為 3 維)</th>
            <th>實際維度</th>
          </tr>
          <tr>
            <td>"cat"</td>
            <td>1234</td>
            <td>[0.25, -0.67, 0.89]</td>
            <td>512 維</td>
          </tr>
          <tr>
            <td>"dog"</td>
            <td>5678</td>
            <td>[0.21, -0.71, 0.92]</td>
            <td>512 維</td>
          </tr>
          <tr>
            <td>"car"</td>
            <td>9012</td>
            <td>[-0.88, 0.34, -0.12]</td>
            <td>512 維</td>
          </tr>
        </table>

        <p><strong>觀察：</strong>"cat" 和 "dog" 的向量相似（都是動物），與 "car" 差異大。</p>
        <p><strong>關鍵：</strong>這些向量是<strong>「學習」</strong>出來的，不是人工設計的！訓練過程中，模型會自動調整向量，讓語義相似的詞靠近。</p>
      </div>

      <div class="key-concept">
        <h4>🔗 權重共享（Weight Sharing）機制</h4>
        <p><strong>Transformer 的特殊設計：</strong>三個地方共享同一個權重矩陣 $W_{embed}$。</p>
        
        <table>
          <tr>
            <th>位置</th>
            <th>功能</th>
            <th>使用方式</th>
          </tr>
          <tr>
            <td>Input Embedding</td>
            <td>將輸入詞轉成向量</td>
            <td>$x = W_{embed}[\text{token\_id}] \times \sqrt{512}$</td>
          </tr>
          <tr>
            <td>Output Embedding</td>
            <td>將目標詞轉成向量（訓練時）</td>
            <td>$y = W_{embed}[\text{token\_id}] \times \sqrt{512}$</td>
          </tr>
          <tr>
            <td>Pre-Softmax Linear</td>
            <td>將 Decoder 輸出轉成詞機率</td>
            <td>$\text{logits} = \text{decoder\_output} \times W_{embed}^T$</td>
          </tr>
        </table>

        <p><strong>為什麼要共享？</strong></p>
        <ul>
          <li><strong>減少參數量：</strong>只需要一個 $W_{embed}$（例如 $30000 \times 512$），而非三個。</li>
          <li><strong>語義一致性：</strong>確保「輸入的 "cat"」和「輸出的 "cat"」有相同的表示。</li>
        </ul>
      </div>

      <div class="key-concept">
        <h4>🔬 為什麼要乘以 $\sqrt{d_{model}}$？</h4>
        <p><strong>觀察：</strong>Embedding 向量的數值通常很小（例如 $[-0.1, 0.05, ...]$）。</p>
        <p><strong>問題：</strong>後面要加上 Positional Encoding（數值較大），Embedding 會被「淹沒」。</p>
        <p><strong>解決方案：</strong>將 Embedding 放大 $\sqrt{512} \approx 22.6$ 倍，讓兩者在相同尺度。</p>
        
        <pre><code>例子：
原始 Embedding：      [0.05, -0.03, 0.08]  (小)
Positional Encoding:  [0.84,  0.54, -0.61]  (大)
→ 相加後 PE 主導，Embedding 貢獻太少 ❌

放大後：
Scaled Embedding:     [1.13, -0.68, 1.81]  (x √512)
Positional Encoding:  [0.84,  0.54, -0.61]
→ 相加後兩者平衡 ✅</code></pre>
      </div>

      <div class="analogy">
        <h4>🔧 雙重類比：Embeddings</h4>
        
        <h4>💡 生活類比：地圖上的座標</h4>
        <p><strong>情境：</strong>你要在地圖上標記不同的城市。</p>
        <ul>
          <li><strong>Embedding：</strong>每個城市有一個 (經度, 緯度) 座標。</li>
          <li><strong>相似性：</strong>座標接近的城市地理位置接近（如台北、新北）。</li>
          <li><strong>Weight Sharing：</strong>無論是「查詢城市」還是「標記城市」，都用同一張地圖（同一組座標）。</li>
        </ul>

        <h4>🔧 工程類比：資料庫的 ID → Vector 映射</h4>
        <pre><code>// Embedding Table: 類似 HashMap
embedding_table = {
    "cat":  [0.25, -0.67, 0.89, ...],  // 512 維
    "dog":  [0.21, -0.71, 0.92, ...],
    "car":  [-0.88, 0.34, -0.12, ...],
    // ... 30000 個詞
}

// Input Embedding: 查表
input = "cat"
vector = embedding_table[input] * sqrt(512)

// Output Prediction: 反向查表（計算相似度）
decoder_output = [0.23, -0.65, 0.87, ...]
scores = decoder_output @ embedding_table.T  // 與所有詞比對
probabilities = softmax(scores)  // 轉成機率
predicted_word = argmax(probabilities)  // "cat"</code></pre>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <!-- ============= Positional Encoding ============= -->
      <h3>3. Positional Encoding（位置編碼）⭐⭐⭐</h3>

      <div class="problem">
        <h4>❓ 問題：Transformer 無法分辨詞的順序</h4>
        <p><strong>回顧：</strong>Attention 機制是「集合操作」，不考慮順序。</p>
        <pre><code>例子：
"The cat sat on the mat" 
"sat The on mat cat the"  
→ 對於 Attention 來說，這兩句話是一樣的！（因為詞的集合相同）</code></pre>
        <p><strong>對比 RNN：</strong>RNN 天生有順序（一個一個詞輸入），Transformer 沒有。</p>
        <p><strong>必須解決：</strong>否則無法理解語法（如「狗咬人」vs「人咬狗」）。</p>
      </div>

      <div class="text-pair">
        <div class="original-text">
          Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks.
        </div>
        <div class="translation">
          <p>
            由於我們的模型<strong>不包含遞歸也不包含卷積</strong>，為了讓模型能夠利用序列的順序，我們必須注入一些關於序列中詞元的<strong>相對或絕對位置</strong>的資訊。為此，我們在編碼器和解碼器堆疊的底部，將<strong>「位置編碼」加到輸入嵌入上</strong>。
          </p>
        </div>
      </div>

      <div class="solution">
        <h4>✅ 解決方案：加入位置資訊</h4>
        <p><strong>核心思想：</strong>為每個位置生成一個獨特的「位置向量」，加到詞向量上。</p>
        
        <pre><code>最終輸入 = Embedding + Positional Encoding

位置 0: [詞向量] + [位置0向量]
位置 1: [詞向量] + [位置1向量]
位置 2: [詞向量] + [位置2向量]
...</code></pre>

        <p><strong>關鍵：</strong>位置向量必須滿足：</p>
        <ul>
          <li>✅ 每個位置有<strong>獨特的</strong>向量（區分不同位置）</li>
          <li>✅ 相鄰位置的向量<strong>相似</strong>（反映相對距離）</li>
          <li>✅ 對於任何偏移 $k$，$PE_{pos+k}$ 可以表示為 $PE_{pos}$ 的<strong>線性函數</strong></li>
        </ul>
      </div>

      <div class="text-pair">
        <div class="original-text">
          In this work, we use sine and cosine functions of different frequencies:
          $$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
          $$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
          where $pos$ is the position and $i$ is the dimension.
        </div>
        <div class="translation">
          <p>在這項工作中，我們使用不同頻率的正弦和餘弦函數：</p>
          <p style="text-align: center; font-size: 1.2em; margin: 20px 0;">
            $$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
          </p>
          <p style="text-align: center; font-size: 1.2em; margin: 20px 0;">
            $$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
          </p>
          <p style="text-align: center;">
            其中 $pos$ 是位置，$i$ 是維度索引。
          </p>
        </div>
      </div>

      <div class="explanation">
        <h4>🔬 公式深度拆解</h4>
        
        <p><strong>符號說明：</strong></p>
        <table>
          <tr>
            <th>符號</th>
            <th>意義</th>
            <th>範圍</th>
            <th>例子</th>
          </tr>
          <tr>
            <td>$pos$</td>
            <td>詞在序列中的位置</td>
            <td>$0, 1, 2, ..., n-1$</td>
            <td>$pos = 3$ 表示第 4 個詞</td>
          </tr>
          <tr>
            <td>$i$</td>
            <td>維度索引（配對）</td>
            <td>$0, 1, 2, ..., 255$</td>
            <td>$i=0 \Rightarrow$ 維度 0, 1</td>
          </tr>
          <tr>
            <td>$2i$</td>
            <td>偶數維度</td>
            <td>$0, 2, 4, ..., 510$</td>
            <td>使用 $\sin$</td>
          </tr>
          <tr>
            <td>$2i+1$</td>
            <td>奇數維度</td>
            <td>$1, 3, 5, ..., 511$</td>
            <td>使用 $\cos$</td>
          </tr>
          <tr>
            <td>$d_{model}$</td>
            <td>向量總維度</td>
            <td>512</td>
            <td>固定值</td>
          </tr>
        </table>

        <p><strong>計算範例（$d_{model} = 512$）：</strong></p>
        <pre><code>位置 pos = 5 的 Positional Encoding:

維度 0 (i=0, 偶數):
  PE[5, 0] = sin(5 / 10000^(0/512)) 
           = sin(5 / 1) 
           = sin(5) ≈ -0.959

維度 1 (i=0, 奇數):
  PE[5, 1] = cos(5 / 10000^(0/512))
           = cos(5) ≈ 0.284

維度 2 (i=1, 偶數):
  PE[5, 2] = sin(5 / 10000^(2/512))
           = sin(5 / 1.055) ≈ -0.998

維度 3 (i=1, 奇數):
  PE[5, 3] = cos(5 / 10000^(2/512)) ≈ 0.061

... (總共 512 維)</code></pre>
      </div>

      <div class="key-concept">
        <h4>🌊 不同維度 = 不同頻率的波</h4>
        <p><strong>核心洞察：</strong>每個維度使用不同頻率的正弦波。</p>
        
        <table>
          <tr>
            <th>維度索引 $i$</th>
            <th>波長 $\lambda$</th>
            <th>變化快慢</th>
            <th>擅長區分</th>
          </tr>
          <tr>
            <td>$i = 0$（維度 0, 1）</td>
            <td>$2\pi$（短）</td>
            <td>變化很快</td>
            <td>相鄰位置（如 pos=1 vs pos=2）</td>
          </tr>
          <tr>
            <td>$i = 128$（維度 256, 257）</td>
            <td>中等</td>
            <td>中速變化</td>
            <td>中距離（如 pos=10 vs pos=20）</td>
          </tr>
          <tr>
            <td>$i = 255$（維度 510, 511）</td>
            <td>$10000 \cdot 2\pi$（長）</td>
            <td>變化很慢</td>
            <td>遠距離（如 pos=100 vs pos=500）</td>
          </tr>
        </table>

        <p><strong>類比：</strong>就像手錶的秒針、分針、時針，分別用於區分不同時間尺度。</p>
      </div>

      <div class="analogy">
        <h4>🔧 雙重類比：Positional Encoding</h4>
        
        <h4>💡 生活類比：時鐘的多指針系統</h4>
        <p><strong>情境：</strong>你要用一個數字（向量）表示「現在的時間」。</p>
        <ul>
          <li><strong>秒針：</strong>變化快（每秒），區分相鄰時刻（14:30:05 vs 14:30:06）。</li>
          <li><strong>分針：</strong>變化中速（每分鐘），區分中期時段（14:30 vs 14:45）。</li>
          <li><strong>時針：</strong>變化慢（每小時），區分長期時段（14:00 vs 18:00）。</li>
        </ul>
        <p><strong>對應：</strong></p>
        <ul>
          <li>低維度（$i$ 小）= 秒針 = 高頻波（快速振盪）</li>
          <li>高維度（$i$ 大）= 時針 = 低頻波（緩慢變化）</li>
        </ul>

        <h4>🔧 工程類比：傅立葉分解（Fourier Series）</h4>
        <pre><code>// Positional Encoding = 多頻率正弦波的疊加
PE(pos) = [
    sin(pos / λ₀),   cos(pos / λ₀),   // 高頻（波長短）
    sin(pos / λ₁),   cos(pos / λ₁),   // 中頻
    sin(pos / λ₂),   cos(pos / λ₂),   // 低頻（波長長）
    ...
]

// 類比：音頻的頻譜分析
// - 高頻部分：捕捉局部細節（相鄰位置的差異）
// - 低頻部分：捕捉全局趨勢（遠距離的關係）

// 工程優勢：
// 1. 每個位置有唯一的「頻譜指紋」
// 2. 相似位置的指紋相似（連續性）
// 3. 可以外推到訓練時未見過的長度</code></pre>
      </div>

      <div class="analogy">
        <h4>🤖 AI 體驗連結：為什麼「我愛你」和「你愛我」不一樣？</h4>
        <p>如果你把這兩句話丟給一個沒有位置編碼的 AI：</p>
        <ul>
            <li><strong>Input A:</strong> {我, 愛, 你}</li>
            <li><strong>Input B:</strong> {你, 愛, 我}</li>
        </ul>
        <p>對於模型來說，這兩組詞是<strong>完全一樣的集合 (Set)</strong>！這就是所謂的「詞袋模型」(Bag of Words)。</p>
        <p><strong>Positional Encoding 的作用：</strong></p>
        <p>它給每個詞打上了「座標」：</p>
        <ul>
            <li><strong>Input A:</strong> {我(pos=1), 愛(pos=2), 你(pos=3)}</li>
            <li><strong>Input B:</strong> {你(pos=1), 愛(pos=2), 我(pos=3)}</li>
        </ul>
        <p>這就是為什麼 ChatGPT 能理解語序和邏輯，而不是只會關鍵字匹配。</p>
      </div>

      <div class="text-pair">
        <div class="original-text">
          We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.
        </div>
        <div class="translation">
          <p>
            我們選擇這個函數，因為我們假設它能讓模型<strong>容易學習根據相對位置進行注意</strong>，因為對於任何固定的偏移量 $k$，$PE_{pos+k}$ 可以表示為 $PE_{pos}$ 的<strong>線性函數</strong>。
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>🔗 關鍵特性：線性可表示性</h4>
        <p><strong>數學性質：</strong>正弦函數的加法定理：</p>
        <p style="text-align: center;">
          $$\sin(a + b) = \sin(a)\cos(b) + \cos(a)\sin(b)$$
        </p>
        <p><strong>應用：</strong></p>
        <pre><code>PE[pos+k] = [sin((pos+k)/λ), cos((pos+k)/λ), ...]

根據三角恆等式，這可以寫成：
PE[pos+k] = A × PE[pos] + B × 常數

// 意義：給定 PE[pos]，可以線性推算出 PE[pos+k]
// → 模型容易學習「相對位置」的概念</code></pre>
        <p><strong>為什麼重要？</strong>模型可以學到「往右移 3 個位置」這種相對關係，而不是死記每個絕對位置。</p>
      </div>

      <div class="text-pair">
        <div class="original-text">
          We also experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results. We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
        </div>
        <div class="translation">
          <p>
            我們也實驗了使用<strong>學習的位置嵌入</strong>，發現兩個版本產生的結果幾乎相同。我們選擇正弦版本，因為它可能允許模型<strong>外推到比訓練時更長的序列長度</strong>。
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>⚖️ Sinusoidal vs Learned Positional Encoding</h4>
        <table>
          <tr>
            <th>方案</th>
            <th>Sinusoidal (本論文)</th>
            <th>Learned (可訓練)</th>
          </tr>
          <tr>
            <td><strong>定義</strong></td>
            <td>固定公式（sin/cos）</td>
            <td>隨機初始化，通過訓練學習</td>
          </tr>
          <tr>
            <td><strong>參數量</strong></td>
            <td>0（無需訓練）</td>
            <td>$max\_len \times d_{model}$<br>例：$512 \times 512 \approx 260K$</td>
          </tr>
          <tr>
            <td><strong>外推能力</strong></td>
            <td>✅ 可處理比訓練更長的序列</td>
            <td>❌ 超過訓練長度會失效</td>
          </tr>
          <tr>
            <td><strong>效果</strong></td>
            <td>與 Learned 幾乎相同</td>
            <td>與 Sinusoidal 幾乎相同</td>
          </tr>
          <tr>
            <td><strong>理論優勢</strong></td>
            <td>數學性質清晰（線性可推算）</td>
            <td>更靈活，可學習任意模式</td>
          </tr>
        </table>
        <p><strong>論文選擇：</strong>Sinusoidal，因為效果相同但更省參數且可外推。</p>
      </div>

      <div class="math-box">
        <h4>📐 完整範例：計算位置 0 和位置 1 的前 4 維</h4>
        <p><strong>參數：</strong>$d_{model} = 512$</p>
        
        <p><strong>位置 0（$pos = 0$）：</strong></p>
        <pre><code>PE[0, 0] = sin(0 / 10000^0) = sin(0) = 0
PE[0, 1] = cos(0 / 10000^0) = cos(0) = 1
PE[0, 2] = sin(0 / 10000^(2/512)) = sin(0) = 0
PE[0, 3] = cos(0 / 10000^(2/512)) = cos(0) = 1

→ PE[0] = [0, 1, 0, 1, 0, 1, ..., 0, 1]  (512 維)</code></pre>

        <p><strong>位置 1（$pos = 1$）：</strong></p>
        <pre><code>PE[1, 0] = sin(1 / 10000^0) = sin(1) ≈ 0.841
PE[1, 1] = cos(1 / 10000^0) = cos(1) ≈ 0.540
PE[1, 2] = sin(1 / 10000^(2/512)) ≈ sin(0.948) ≈ 0.813
PE[1, 3] = cos(1 / 10000^(2/512)) ≈ cos(0.948) ≈ 0.582

→ PE[1] = [0.841, 0.540, 0.813, 0.582, ...]</code></pre>

        <p><strong>觀察：</strong></p>
        <ul>
          <li>位置 0 的 PE 是基準（接近 [0, 1, 0, 1, ...]）。</li>
          <li>位置 1 的 PE 與位置 0 不同，但相似（連續性）。</li>
          <li>不同維度的變化幅度不同（多尺度表示）。</li>
        </ul>
      </div>

      <div class="solution">
        <h4>✅ Positional Encoding 總結</h4>
        <ol>
          <li><strong>為什麼需要？</strong>
            <br>Transformer 的 Attention 不考慮順序，必須手動注入位置資訊。
          </li>
          <li><strong>怎麼做？</strong>
            <br>為每個位置生成一個 512 維的向量，加到詞嵌入上。
          </li>
          <li><strong>為什麼用 sin/cos？</strong>
            <br>多頻率波可以同時捕捉局部和全局的位置關係。
          </li>
          <li><strong>關鍵優勢：</strong>
            <ul>
              <li>每個位置有唯一編碼（可區分）</li>
              <li>相鄰位置編碼相似（連續性）</li>
              <li>可線性推算相對位置（易學習）</li>
              <li>可外推到更長序列（泛化）</li>
            </ul>
          </li>
        </ol>
      </div>

      <div class="nav-bar">
        <a href="03-1-model-architecture-attention.html" class="nav-btn">
          ← 上一頁：Attention Mechanism
        </a>
        <a href="04-why-self-attention.html" class="nav-btn primary">
          下一頁：Why Self-Attention →
        </a>
      </div>

      <div
        style="
          margin-top: 40px;
          padding: 20px;
          background: #ecf0f1;
          border-radius: 5px;
          text-align: center;
        "
      >
        <p><strong>📚 學習檢查點</strong></p>
        <p>在進入下一頁之前，請確保你理解：</p>
        <ul style="text-align: left; max-width: 600px; margin: 20px auto">
          <li>✅ FFN 是「逐位置獨立處理」，Attention 是「全局交互」</li>
          <li>✅ Embedding 將詞轉成向量，乘以 $\sqrt{512}$ 是為了與 PE 平衡</li>
          <li>✅ Positional Encoding 解決了 Transformer 無法感知順序的問題</li>
          <li>✅ Sin/Cos 的不同頻率類似「時鐘的多指針」，捕捉多尺度位置關係</li>
        </ul>
      </div>
    </div>
  </body>
</html>

