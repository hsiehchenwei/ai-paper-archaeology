<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Transformer è«–æ–‡æ·±åº¦è§£æ - ç¬¬4é ï¼šWhy Self-Attention</title>
    <link rel="stylesheet" href="../styles/global.css" />
    <link rel="stylesheet" href="../styles/paper-reading.css" />
    <link rel="preconnect" OS="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=JetBrains+Mono:wght@400;700&family=Merriweather:ital,wght@0,400;0,700;1,400&display=swap"
      rel="stylesheet"
    />
    <!-- MathJax for mathematical formulas -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [["$", "$"]],
          displayMath: [["$$", "$$"]],
        },
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <div class="container">
      <h1>
        ğŸ“– Attention Is All You Need <br /><span
          style="
            font-size: 0.6em;
            color: var(--text-secondary);
            font-weight: normal;
          "
          >æ·±åº¦è§£æç³»åˆ—</span
        >
      </h1>
      <h2>ç¬¬ 4 é ï¼šWhy Self-Attentionï¼Ÿç‚ºä»€éº¼é¸æ“‡è‡ªæ³¨æ„åŠ›ï¼Ÿ</h2>

      <div class="header-section">
        <div class="paper-meta">
          <div class="meta-item">
            <strong>ç›®å‰ç« ç¯€</strong>
            4. Why Self-Attention
          </div>
          <div class="meta-item">
            <strong>æ ¸å¿ƒä¸»é¡Œ</strong>
            å°æ¯” Self-Attentionã€RNNã€CNN çš„ä¸‰å¤§æŒ‡æ¨™
          </div>
          <div class="meta-item">
            <strong>é–±è®€é›£åº¦</strong>
            â­â­â­â­ (æ¶‰åŠè¤‡é›œåº¦åˆ†æ)
          </div>
        </div>
      </div>

      <!-- ============= å‹•æ©Ÿèˆ‡è©•ä¼°æ¨™æº– ============= -->
      <h3>1. è«–æ–‡å‹•æ©Ÿï¼šç‚ºä»€éº¼è¦ç”¨ Self-Attentionï¼Ÿ</h3>

      <div class="text-pair">
        <div class="original-text">
          In this section we compare various aspects of self-attention layers to
          the recurrent and convolutional layers commonly used for mapping one
          variable-length sequence of symbol representations $(x_1, ..., x_n)$
          to another sequence of equal length $(z_1, ..., z_n)$, with $x_i, z_i
          \in \mathbb{R}^d$, such as a hidden layer in a typical sequence
          transduction encoder or decoder. Motivating our use of self-attention
          we consider three desiderata.
        </div>
        <div class="translation">
          <p>
            åœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘æ¯”è¼ƒè‡ªæ³¨æ„åŠ›å±¤èˆ‡å¸¸ç”¨çš„<strong>éæ­¸å±¤ï¼ˆRNNï¼‰</strong>å’Œ<strong>å·ç©å±¤ï¼ˆCNNï¼‰</strong>åœ¨å°‡ä¸€å€‹å¯è®Šé•·åº¦çš„ç¬¦è™Ÿè¡¨ç¤ºåºåˆ—
            $(x_1, ..., x_n)$ æ˜ å°„åˆ°å¦ä¸€å€‹ç­‰é•·åºåˆ— $(z_1, ..., z_n)$ï¼ˆå…¶ä¸­ $x_i,
            z_i \in
            \mathbb{R}^d$ï¼‰æ™‚çš„å„å€‹æ–¹é¢ï¼Œä¾‹å¦‚å…¸å‹åºåˆ—è½‰æ›ç·¨ç¢¼å™¨æˆ–è§£ç¢¼å™¨ä¸­çš„éš±è—å±¤ã€‚ç‚ºäº†è­‰æ˜ä½¿ç”¨è‡ªæ³¨æ„åŠ›çš„åˆç†æ€§ï¼Œæˆ‘å€‘è€ƒæ…®<strong>ä¸‰å€‹æ¨™æº–</strong>ã€‚
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>ğŸ¯ ä¸‰å¤§è©•ä¼°æ¨™æº– (Three Desiderata)</h4>
        <p><strong>å•é¡Œï¼š</strong>é¸æ“‡å“ªç¨®å±¤é¡å‹ä¾†å»ºæ§‹ Seq2Seq æ¨¡å‹ï¼Ÿ</p>
        <p><strong>è©•ä¼°æ¨™æº–ï¼š</strong></p>
        <ol>
          <li>
            <strong>è¨ˆç®—è¤‡é›œåº¦ (Computational Complexity per Layer)</strong>
            <br />å–®å±¤çš„ç¸½è¨ˆç®—é‡æ˜¯å¤šå°‘ï¼Ÿ <br />â†’ è¶Šä½è¶Šå¥½ï¼ˆç¯€çœé‹ç®—è³‡æºï¼‰ã€‚
          </li>
          <li>
            <strong>ä¸¦è¡Œèƒ½åŠ› (Amount of Parallelized Computation)</strong>
            <br />æœ€å°‘éœ€è¦å¤šå°‘ã€Œé †åºæ“ä½œã€ï¼ˆsequential operationsï¼‰ï¼Ÿ <br />â†’
            è¶Šå°‘è¶Šå¥½ï¼ˆæ›´æ˜“æ–¼ä¸¦è¡ŒåŒ–ï¼Œè¨“ç·´æ›´å¿«ï¼‰ã€‚
          </li>
          <li>
            <strong>é•·è·é›¢ä¾è³´çš„è·¯å¾‘é•·åº¦ (Maximum Path Length)</strong>
            <br />è¨Šè™Ÿåœ¨å…©å€‹ä»»æ„ä½ç½®é–“å‚³éçš„æœ€é•·è·¯å¾‘æ˜¯å¤šå°‘ï¼Ÿ <br />â†’
            è¶ŠçŸ­è¶Šå¥½ï¼ˆæ›´å®¹æ˜“å­¸ç¿’é•·è·é›¢ä¾è³´ï¼‰ã€‚
          </li>
        </ol>
      </div>

      <div class="text-pair">
        <div class="original-text">
          One is the total computational complexity per layer. Another is the
          amount of computation that can be parallelized, as measured by the
          minimum number of sequential operations required.
        </div>
        <div class="translation">
          <p>
            ç¬¬ä¸€å€‹æ˜¯<strong>æ¯å±¤çš„ç¸½è¨ˆç®—è¤‡é›œåº¦</strong>ã€‚ç¬¬äºŒå€‹æ˜¯<strong>å¯ä¸¦è¡ŒåŒ–çš„è¨ˆç®—é‡</strong>ï¼Œé€šéæ‰€éœ€çš„æœ€å°é †åºæ“ä½œæ•¸ä¾†è¡¡é‡ã€‚
          </p>
        </div>
      </div>

      <div class="text-pair">
        <div class="original-text">
          The third is the path length between long-range dependencies in the
          network. Learning long-range dependencies is a key challenge in many
          sequence transduction tasks. One key factor affecting the ability to
          learn such dependencies is the length of the paths forward and
          backward signals have to traverse in the network. The shorter these
          paths between any combination of positions in the input and output
          sequences, the easier it is to learn long-range dependencies [12].
          Hence we also compare the maximum path length between any two input
          and output positions in networks composed of the different layer
          types.
        </div>
        <div class="translation">
          <p>
            ç¬¬ä¸‰å€‹æ˜¯ç¶²è·¯ä¸­<strong>é•·è·é›¢ä¾è³´ä¹‹é–“çš„è·¯å¾‘é•·åº¦</strong>ã€‚å­¸ç¿’é•·è·é›¢ä¾è³´æ˜¯è¨±å¤šåºåˆ—è½‰æ›ä»»å‹™çš„é—œéµæŒ‘æˆ°ã€‚å½±éŸ¿å­¸ç¿’é€™äº›ä¾è³´èƒ½åŠ›çš„ä¸€å€‹é—œéµå› ç´ æ˜¯å‰å‘å’Œå¾Œå‘è¨Šè™Ÿå¿…é ˆåœ¨ç¶²è·¯ä¸­ç©¿è¶Šçš„è·¯å¾‘é•·åº¦ã€‚åœ¨è¼¸å…¥å’Œè¼¸å‡ºåºåˆ—ä¸­ä»»æ„ä½ç½®çµ„åˆä¹‹é–“çš„è·¯å¾‘è¶ŠçŸ­ï¼Œå­¸ç¿’é•·è·é›¢ä¾è³´å°±è¶Šå®¹æ˜“
            [12]ã€‚å› æ­¤ï¼Œæˆ‘å€‘ä¹Ÿæ¯”è¼ƒäº†ç”±ä¸åŒå±¤é¡å‹çµ„æˆçš„ç¶²è·¯ä¸­ä»»æ„å…©å€‹è¼¸å…¥å’Œè¼¸å‡ºä½ç½®ä¹‹é–“çš„<strong>æœ€å¤§è·¯å¾‘é•·åº¦</strong>ã€‚
          </p>
        </div>
      </div>

      <div class="analogy">
        <h4>ğŸ”§ é›™é‡é¡æ¯”ï¼šé•·è·é›¢ä¾è³´å•é¡Œ</h4>

        <h4>ğŸ’¡ ç”Ÿæ´»é¡æ¯”ï¼šé›»è©±éŠæˆ²ï¼ˆå‚³è©±éŠæˆ²ï¼‰</h4>
        <p>
          <strong>æƒ…å¢ƒï¼š</strong>10 å€‹äººæ’æˆä¸€åˆ—å‚³è©±ï¼Œç¬¬ 1 å€‹äººèªªã€Œä»Šå¤©æ™šä¸Š 7
          é»åœ¨åœ–æ›¸é¤¨è¦‹ã€ã€‚
        </p>
        <ul>
          <li>
            <strong>å•é¡Œï¼š</strong>å‚³åˆ°ç¬¬ 10
            å€‹äººæ™‚ï¼Œè¨Šæ¯å¯èƒ½è®Šæˆã€Œä»Šå¤©æ™šä¸Šåœ¨å“ªè£¡è¦‹ï¼Ÿã€
          </li>
          <li>
            <strong>åŸå› ï¼š</strong>æ¯ç¶“éä¸€å€‹äººï¼Œè¨Šæ¯å°±å¤±çœŸä¸€æ¬¡ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰ã€‚
          </li>
          <li>
            <strong>è§£æ±ºæ–¹æ¡ˆï¼š</strong>è®“ç¬¬ 1 å€‹äºº<strong
              >ç›´æ¥å°è‘—ç¬¬ 10 å€‹äººèªª</strong
            >ï¼ˆSelf-Attentionï¼‰ï¼è·¯å¾‘é•·åº¦ = 1ã€‚
          </li>
        </ul>

        <h4>ğŸ”§ å·¥ç¨‹é¡æ¯”ï¼šè³‡æ–™åº« JOIN æ“ä½œ</h4>
        <pre><code>// RNN: éˆå¼ä¾è³´ï¼ˆå¿…é ˆæŒ‰é †åºè™•ç†ï¼‰
h1 = f(x1)
h2 = f(x2, h1)       // ä¾è³´ h1
h3 = f(x3, h2)       // ä¾è³´ h2
...
h10 = f(x10, h9)     // ä¾è³´ h9
// â†’ è·¯å¾‘ï¼šx1 â†’ h1 â†’ h2 â†’ ... â†’ h10 (é•·åº¦ = 10)

// Self-Attention: å…¨é€£æ¥ï¼ˆæ‰€æœ‰ä½ç½®åŒæ™‚çœ‹åˆ°ï¼‰
attention_matrix = softmax(Q @ K.T / sqrt(d_k))
output = attention_matrix @ V
// â†’ è·¯å¾‘ï¼šä»»æ„ xi ç›´æ¥é€£æ¥åˆ°ä»»æ„ xj (é•·åº¦ = 1)</code></pre>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <!-- ============= Table 1 ============= -->
      <h3>2. æ ¸å¿ƒå°æ¯”è¡¨ï¼šSelf-Attention vs RNN vs CNN</h3>

      <div class="key-concept">
        <h4>ğŸ“Š Table 1ï¼šä¸‰ç¨®å±¤é¡å‹çš„æ€§èƒ½å°æ¯”</h4>
        <p><strong>ç¬¦è™Ÿèªªæ˜ï¼š</strong></p>
        <ul>
          <li>$n$ï¼šåºåˆ—é•·åº¦ï¼ˆä¾‹å¦‚ä¸€å¥è©±æœ‰å¤šå°‘å€‹è©ï¼‰</li>
          <li>$d$ï¼šè¡¨ç¤ºç¶­åº¦ï¼ˆä¾‹å¦‚æ¯å€‹è©æ˜¯ 512 ç¶­çš„å‘é‡ï¼‰</li>
          <li>$k$ï¼šå·ç©æ ¸å¤§å°</li>
          <li>$r$ï¼šå—é™è‡ªæ³¨æ„åŠ›çš„é„°åŸŸå¤§å°</li>
        </ul>
      </div>

      <div style="overflow-x: auto">
        <table>
          <thead>
            <tr>
              <th>Layer Type<br />(å±¤é¡å‹)</th>
              <th>Complexity per Layer<br />(æ¯å±¤è¤‡é›œåº¦)</th>
              <th>Sequential Operations<br />(é †åºæ“ä½œæ•¸)</th>
              <th>Maximum Path Length<br />(æœ€å¤§è·¯å¾‘é•·åº¦)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Self-Attention</strong></td>
              <td>$O(n^2 \cdot d)$</td>
              <td>$O(1)$</td>
              <td>$O(1)$</td>
            </tr>
            <tr>
              <td><strong>Recurrent</strong></td>
              <td>$O(n \cdot d^2)$</td>
              <td>$O(n)$</td>
              <td>$O(n)$</td>
            </tr>
            <tr>
              <td><strong>Convolutional</strong></td>
              <td>$O(k \cdot n \cdot d^2)$</td>
              <td>$O(1)$</td>
              <td>$O(\log_k(n))$</td>
            </tr>
            <tr>
              <td><strong>Self-Attention (restricted)</strong></td>
              <td>$O(r \cdot n \cdot d)$</td>
              <td>$O(1)$</td>
              <td>$O(n/r)$</td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="explanation">
        <h4>ğŸ”¬ é€è¡Œæ‹†è§£ï¼šæ¯ç¨®å±¤é¡å‹çš„ç‰¹æ€§</h4>

        <div class="key-concept">
          <h5>ğŸ§® å…ˆç†è§£ç¬¦è™Ÿï¼š$n$ å’Œ $d$ æ˜¯ä»€éº¼ï¼Ÿ</h5>
          <p><strong>å›é¡§ä¹‹å‰å­¸åˆ°çš„ï¼š</strong></p>
          <ul>
            <li>
              <strong>$n$ï¼ˆåºåˆ—é•·åº¦ï¼‰ï¼š</strong>ä¸€å¥è©±æœ‰å¤šå°‘å€‹è©ï¼ˆtokenï¼‰
              <br />ä¾‹å¦‚ï¼š"The cat sits" â†’ n = 3
            </li>
            <li>
              <strong>$d$ï¼ˆå‘é‡ç¶­åº¦ï¼‰ï¼š</strong>æ¯å€‹è©ç”¨å¤šå°‘å€‹æ•¸å­—è¡¨ç¤º <br />åœ¨
              Transformer ä¸­ï¼š<strong>$d = d_{model} = 512$</strong>
            </li>
          </ul>

          <h5>ğŸ’¡ ç”Ÿæ´»é¡æ¯”ï¼šå­¸ç”Ÿåå–®</h5>
          <table>
            <tr>
              <th>ç¬¦è™Ÿ</th>
              <th>æ„ç¾©</th>
              <th>é¡æ¯”</th>
            </tr>
            <tr>
              <td>$n$ (åºåˆ—é•·åº¦)</td>
              <td>æœ‰å¤šå°‘å€‹è©</td>
              <td>ç­ä¸Šæœ‰å¤šå°‘å­¸ç”Ÿï¼ˆä¾‹å¦‚ 30 äººï¼‰</td>
            </tr>
            <tr>
              <td>$d$ (å‘é‡ç¶­åº¦)</td>
              <td>æ¯å€‹è©æœ‰å¤šå°‘ç¶­åº¦</td>
              <td>æ¯å€‹å­¸ç”Ÿçš„æª”æ¡ˆæœ‰å¤šå°‘æ¬„ä½ï¼ˆå§“åã€å¹´é½¡ã€ç§‘ç³»...å…± 512 æ¬„ï¼‰</td>
            </tr>
            <tr>
              <td>$n \times d$ çŸ©é™£</td>
              <td>æ•´å€‹è¼¸å…¥</td>
              <td>30 å€‹å­¸ç”Ÿ Ã— 512 å€‹æ¬„ä½ = å®Œæ•´èŠ±åå†Š</td>
            </tr>
          </table>

          <h5>ğŸ”§ ç‚ºä»€éº¼è¤‡é›œåº¦å…¬å¼è£¡æœ‰ $n$ å’Œ $d$ï¼Ÿ</h5>
          <p><strong>Self-Attention çš„è¨ˆç®—ï¼š$O(n^2 \cdot d)$</strong></p>
          <pre><code>// è¨ˆç®— Attention åˆ†æ•¸çŸ©é™£ï¼šQ Ã— K^T
for i in range(n):        // ç¬¬ä¸€å€‹ nï¼šæ¯å€‹ Query
    for j in range(n):    // ç¬¬äºŒå€‹ nï¼šæ¯å€‹ Key
        score[i][j] = dot_product(Q[i], K[j])  // dï¼šå‘é‡å…§ç©éœ€è¦ d æ¬¡ä¹˜æ³•
        
// ç¸½è¨ˆç®—é‡ï¼šn Ã— n Ã— d = nÂ² Ã— d</code></pre>

          <p><strong>ç‚ºä»€éº¼è¦ä¹˜ä»¥ $d$ï¼Ÿ</strong></p>
          <ul>
            <li>å‘é‡å…§ç© (dot product) éœ€è¦é€å€‹å…ƒç´ ç›¸ä¹˜å†ç›¸åŠ </li>
            <li>Q[i] å’Œ K[j] éƒ½æ˜¯ 512 ç¶­ â†’ éœ€è¦ 512 æ¬¡ä¹˜æ³•å’Œ 511 æ¬¡åŠ æ³•</li>
            <li>æ‰€ä»¥è¤‡é›œåº¦æ˜¯ $O(d)$</li>
          </ul>

          <pre><code>ä¾‹å­ï¼šè¨ˆç®—å…©å€‹å‘é‡çš„å…§ç© (d=4 ç°¡åŒ–ç‰ˆ)
Q[i] = [0.1, 0.2, 0.3, 0.4]
K[j] = [0.5, 0.6, 0.7, 0.8]

score = 0.1Ã—0.5 + 0.2Ã—0.6 + 0.3Ã—0.7 + 0.4Ã—0.8
      = 0.05 + 0.12 + 0.21 + 0.32
      = 0.70

// éœ€è¦ 4 æ¬¡ä¹˜æ³• + 3 æ¬¡åŠ æ³• â†’ O(d)</code></pre>
        </div>

        <h5>1. Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰</h5>
        <ul>
          <li>
            <strong>è¤‡é›œåº¦ $O(n^2 \cdot d)$ï¼š</strong> <br />è¨ˆç®— $n \times n$
            çš„æ³¨æ„åŠ›çŸ©é™£ï¼ˆæ¯å€‹ä½ç½®çœ‹æ‰€æœ‰å…¶ä»–ä½ç½®ï¼‰ï¼Œæ¯å€‹å…ƒç´ æ¶‰åŠ $d$ ç¶­è¨ˆç®—ã€‚
            <br /><em>ç¼ºé»ï¼š</em>ç•¶åºåˆ—å¾ˆé•·æ™‚ï¼ˆ$n$ å¤§ï¼‰ï¼Œ$n^2$ æœƒå¾ˆå¤§ã€‚
          </li>
          <li>
            <strong>é †åºæ“ä½œ $O(1)$ï¼š</strong>
            <br />æ‰€æœ‰ä½ç½®å¯ä»¥<strong>å®Œå…¨ä¸¦è¡Œ</strong>è¨ˆç®—ï¼ˆGPU å‹å¥½ï¼‰ã€‚
          </li>
          <li>
            <strong>è·¯å¾‘é•·åº¦ $O(1)$ï¼š</strong>
            <br />ä»»æ„å…©å€‹ä½ç½®<strong>ç›´æ¥é€£æ¥</strong>ï¼ˆä¸€æ­¥åˆ°é”ï¼‰ã€‚
            <br /><strong>é€™æ˜¯æœ€å¤§å„ªå‹¢ï¼</strong>
          </li>
        </ul>

        <h5>2. Recurrentï¼ˆRNN / LSTM / GRUï¼‰</h5>
        <ul>
          <li>
            <strong>è¤‡é›œåº¦ $O(n \cdot d^2)$ï¼š</strong> <br />æ¯å€‹æ™‚é–“æ­¥è¨ˆç®— $d
            \times d$ çš„çŸ©é™£ä¹˜æ³•ï¼Œå…± $n$ æ­¥ã€‚ <br /><em>å°æ¯”ï¼š</em>ç•¶ $n < d$
            æ™‚ï¼ˆå¸¸è¦‹æƒ…æ³ï¼Œå¦‚ $n=100, d=512$ï¼‰ï¼ŒRNN æ¯” Self-Attention æ›´å¿«ã€‚
          </li>
          <li>
            <strong>é †åºæ“ä½œ $O(n)$ï¼š</strong>
            <br />å¿…é ˆ<strong>æŒ‰é †åº</strong>è¨ˆç®—ï¼ˆ$h_t$ ä¾è³´
            $h_{t-1}$ï¼‰ï¼Œ<strong>ç„¡æ³•ä¸¦è¡Œ</strong>ã€‚ <br /><strong
              >é€™æ˜¯æœ€å¤§ç¼ºé»ï¼</strong
            >è¨“ç·´æ…¢ã€‚
          </li>
          <li>
            <strong>è·¯å¾‘é•·åº¦ $O(n)$ï¼š</strong> <br />ä½ç½® 1 åˆ°ä½ç½® $n$ éœ€è¦ç¶“é
            $n-1$ å€‹ä¸­é–“æ­¥é©Ÿï¼ˆæ¢¯åº¦æ¶ˆå¤±å•é¡Œï¼‰ã€‚
          </li>
        </ul>

        <h5>3. Convolutionalï¼ˆCNNï¼‰</h5>
        <ul>
          <li>
            <strong>è¤‡é›œåº¦ $O(k \cdot n \cdot d^2)$ï¼š</strong>
            <br />æ¯å€‹ä½ç½®ç”¨æ ¸å¤§å° $k$ çš„å·ç©ï¼Œæ¶‰åŠ $d \times d$ é‹ç®—ã€‚
            <br /><em>å°æ¯”ï¼š</em>ç•¶ $k$ è¼ƒå°æ™‚ï¼Œæ¯” RNN æ›´æ…¢ï¼ˆå› ç‚º $d^2$ é …ï¼‰ã€‚
          </li>
          <li>
            <strong>é †åºæ“ä½œ $O(1)$ï¼š</strong>
            <br />å·ç©æ“ä½œå¯ä¸¦è¡Œï¼ˆæ‰€æœ‰ä½ç½®åŒæ™‚è¨ˆç®—ï¼‰ã€‚
          </li>
          <li>
            <strong>è·¯å¾‘é•·åº¦ $O(\log_k(n))$ï¼š</strong>
            <br />ä½¿ç”¨ã€Œè†¨è„¹å·ç©ã€ï¼ˆdilated convolutionï¼‰æ™‚ï¼Œéœ€è¦ $\log_k(n)$
            å±¤æ‰èƒ½è¦†è“‹æ•´å€‹åºåˆ—ã€‚ <br /><em
              >å„ªæ–¼ RNNï¼Œä½†ä»ä¸å¦‚ Self-Attention çš„ $O(1)$ã€‚</em
            >
          </li>
        </ul>

        <h5>4. Self-Attention (Restricted)ï¼ˆå—é™è‡ªæ³¨æ„åŠ›ï¼‰</h5>
        <ul>
          <li>
            <strong>è¤‡é›œåº¦ $O(r \cdot n \cdot d)$ï¼š</strong>
            <br />æ¯å€‹ä½ç½®åªçœ‹é„°åŸŸå…§çš„ $r$ å€‹ä½ç½®ï¼ˆè€Œéå…¨éƒ¨ $n$ å€‹ï¼‰ã€‚ <br /><em
              >å„ªå‹¢ï¼š</em
            >ç•¶åºåˆ—å¾ˆé•·æ™‚ï¼Œå¯ä»¥å¤§å¹…é™ä½è¨ˆç®—é‡ã€‚
          </li>
          <li><strong>é †åºæ“ä½œ $O(1)$ï¼š</strong> <br />ä»å¯ä¸¦è¡Œã€‚</li>
          <li>
            <strong>è·¯å¾‘é•·åº¦ $O(n/r)$ï¼š</strong>
            <br />éœ€è¦å¤šå±¤æ‰èƒ½è®“è¨Šæ¯å‚³éåˆ°é è·é›¢ä½ç½®ã€‚
          </li>
        </ul>
      </div>

      <div class="problem">
        <h4>â“ å•é¡Œï¼šå“ªç¨®æƒ…æ³ä¸‹ RNN æ›´å¿«ï¼Ÿ</h4>
        <p><strong>é—œéµï¼š</strong>æ¯”è¼ƒ $O(n^2 \cdot d)$ vs $O(n \cdot d^2)$</p>
        <pre><code>Self-Attention: nÂ² Ã— d
Recurrent:      n Ã— dÂ²

ç•¶ n < d æ™‚ï¼ŒSelf-Attention æ›´å¿«
ç•¶ n > d æ™‚ï¼ŒRecurrent æ›´å¿«

å¯¦éš›æƒ…æ³ï¼ˆNLP ä»»å‹™ï¼‰:
- Word-piece è¡¨ç¤ºï¼šn â‰ˆ 100-200ï¼Œd â‰ˆ 512
- çµæœï¼šn < d â†’ Self-Attention æ›´å¿«ï¼</code></pre>
      </div>

      <div class="solution">
        <h4>âœ… Self-Attention çš„å‹åˆ©æ¢ä»¶</h4>
        <table>
          <tr>
            <th>è©•ä¼°æ¨™æº–</th>
            <th>Self-Attention</th>
            <th>RNN</th>
            <th>CNN</th>
            <th>å‹è€…</th>
          </tr>
          <tr>
            <td><strong>è¤‡é›œåº¦</strong><br />(ç•¶ $n < d$)</td>
            <td>$O(n^2 \cdot d)$</td>
            <td>$O(n \cdot d^2)$</td>
            <td>$O(k \cdot n \cdot d^2)$</td>
            <td>âœ… Self-Attention</td>
          </tr>
          <tr>
            <td><strong>ä¸¦è¡Œèƒ½åŠ›</strong></td>
            <td>$O(1)$ (å®Œå…¨ä¸¦è¡Œ)</td>
            <td>$O(n)$ (é †åº)</td>
            <td>$O(1)$ (ä¸¦è¡Œ)</td>
            <td>âœ… Self-Attention / CNN</td>
          </tr>
          <tr>
            <td><strong>é•·è·é›¢ä¾è³´</strong></td>
            <td>$O(1)$ (ç›´æ¥é€£æ¥)</td>
            <td>$O(n)$ (ç·šæ€§å‚³é)</td>
            <td>$O(\log_k(n))$ (éšå±¤å‚³é)</td>
            <td>âœ… Self-Attention</td>
          </tr>
        </table>
        <p>
          <strong>çµè«–ï¼š</strong>Self-Attention
          åœ¨ã€Œä¸¦è¡Œèƒ½åŠ›ã€å’Œã€Œé•·è·é›¢ä¾è³´ã€ä¸Šéƒ½æ˜¯æœ€å„ªçš„ï¼
        </p>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <!-- ============= æ·±å…¥åˆ†æ ============= -->
      <h3>3. è«–æ–‡çš„è©³ç´°åˆ†æ</h3>

      <div class="text-pair">
        <div class="original-text">
          As noted in Table 1, a self-attention layer connects all positions
          with a constant number of sequentially executed operations, whereas a
          recurrent layer requires $O(n)$ sequential operations. In terms of
          computational complexity, self-attention layers are faster than
          recurrent layers when the sequence length $n$ is smaller than the
          representation dimensionality $d$, which is most often the case with
          sentence representations used by state-of-the-art models in machine
          translation, such as word-piece [38] and byte-pair [31]
          representations.
        </div>
        <div class="translation">
          <p>
            å¦‚è¡¨ 1
            æ‰€ç¤ºï¼Œè‡ªæ³¨æ„åŠ›å±¤é€šé<strong>å¸¸æ•¸å€‹é †åºåŸ·è¡Œæ“ä½œ</strong>é€£æ¥æ‰€æœ‰ä½ç½®ï¼Œè€Œéæ­¸å±¤éœ€è¦
            $O(n)$ å€‹é †åºæ“ä½œã€‚åœ¨è¨ˆç®—è¤‡é›œåº¦æ–¹é¢ï¼Œç•¶åºåˆ—é•·åº¦ $n$
            <strong>å°æ–¼è¡¨ç¤ºç¶­åº¦ $d$</strong>
            æ™‚ï¼Œè‡ªæ³¨æ„åŠ›å±¤æ¯”éæ­¸å±¤æ›´å¿«ï¼Œé€™æ˜¯æ©Ÿå™¨ç¿»è­¯ä¸­æœ€å…ˆé€²æ¨¡å‹æ‰€ä½¿ç”¨çš„å¥å­è¡¨ç¤ºï¼ˆå¦‚
            word-piece [38] å’Œ byte-pair [31]
            è¡¨ç¤ºï¼‰<strong>æœ€å¸¸è¦‹çš„æƒ…æ³</strong>ã€‚
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>ğŸ”¢ å¯¦éš›æ•¸å€¼ç¯„ä¾‹</h4>
        <p><strong>æ©Ÿå™¨ç¿»è­¯ä»»å‹™ï¼ˆWMT 2014ï¼‰ï¼š</strong></p>
        <pre><code>å¥å­é•·åº¦ï¼šn â‰ˆ 30-100 è©
Word-piece è¡¨ç¤ºå¾Œï¼šn â‰ˆ 100-200 tokens
æ¨¡å‹ç¶­åº¦ï¼šd = 512 (base) æˆ– 1024 (big)

â†’ n < d çš„æƒ…æ³ä½”çµ•å¤§å¤šæ•¸ï¼
â†’ Self-Attention è¤‡é›œåº¦å„ªå‹¢ï¼š
   O(100Â² Ã— 512) vs O(100 Ã— 512Â²)
   = 5.1M vs 26.2M (RNN æ…¢ 5 å€ï¼)</code></pre>
      </div>

      <div class="analogy">
        <h4>ğŸ¤– AI é«”é©—é€£çµï¼šç‚ºä»€éº¼ Context Window ä¸èƒ½ç„¡é™å¤§ï¼Ÿ</h4>
        <p>
          ä½ å¯èƒ½ç™¼ç¾ ChatGPT çš„ Context Windowï¼ˆå¯è¨˜æ†¶é•·åº¦ï¼‰æœ‰ä¸Šé™ï¼Œè€Œä¸”è¶Šé•·çš„
          Context è¶Šè²´ã€‚
        </p>
        <p><strong>åŸå› å°±åœ¨é€™è£¡ï¼šè¤‡é›œåº¦ $O(n^2 \cdot d)$</strong></p>

        <h5>ğŸ”¢ å…·é«”æ•¸å­—èªªæ˜</h5>
        <p>å‡è¨­ ChatGPT çš„è¨­å®šï¼š</p>
        <ul>
          <li>
            <strong>$d = 12,288$</strong>ï¼ˆGPT-4 çš„æ¨¡å‹ç¶­åº¦ï¼Œæ¯” Transformer çš„
            512 å¤§å¾—å¤šï¼ï¼‰
          </li>
          <li><strong>$n = 8,192$</strong>ï¼ˆContext Window é•·åº¦ï¼‰</li>
        </ul>

        <pre><code>è¨ˆç®— Attention çš„æˆæœ¬ï¼š
nÂ² Ã— d = 8192Â² Ã— 12288
       = 67,108,864 Ã— 12288
       = 824,633,720,832 æ¬¡é‹ç®—ï¼ï¼ˆ8åƒå„„æ¬¡ï¼‰

å¦‚æœ Context Window åŠ å€åˆ° 16,384ï¼š
nÂ² Ã— d = 16384Â² Ã— 12288
       = 268,435,456 Ã— 12288
       = 3,298,534,883,328 æ¬¡é‹ç®—ï¼ˆ3.3å…†æ¬¡ï¼‰
â†’ è®Šæˆ 4 å€ï¼</code></pre>

        <ul>
          <li>
            å¦‚æœæ–‡ç« é•·åº¦ ($n$) è®Šæˆ 2 å€ï¼Œè¨ˆç®—é‡æœƒè®Šæˆ
            <strong>4 å€</strong> ($2^2$)ï¼
          </li>
          <li>
            å¦‚æœæ–‡ç« é•·åº¦è®Šæˆ 10 å€ï¼Œè¨ˆç®—é‡æœƒè®Šæˆ <strong>100 å€</strong>ï¼
          </li>
          <li><strong>è€Œä¸” $d$ è¶Šå¤§ï¼ˆæ¨¡å‹è¶Šå¤§ï¼‰ï¼Œæˆæœ¬å°±è¶Šé«˜ï¼</strong></li>
        </ul>
        <p>
          é€™å°±æ˜¯ç‚ºä»€éº¼è¦æ“´å¤§ Context Window
          é€™éº¼å›°é›£ï¼Œå› ç‚ºè¨ˆç®—æˆæœ¬æ˜¯<strong>å¹³æ–¹ç´šå¢é•·</strong>çš„ã€‚
        </p>

        <p>
          <strong>ç¾å¯¦è§£æ³•ï¼š</strong>Flash Attentionã€Sliding Windowã€Sparse
          Attentionï¼ˆè«–æ–‡è£¡æåˆ°çš„ Restricted Self-Attention çš„ç¾ä»£å¯¦ä½œç‰ˆæœ¬ï¼‰ã€‚
        </p>
      </div>

      <div class="text-pair">
        <div class="original-text">
          To improve computational performance for tasks involving very long
          sequences, self-attention could be restricted to considering only a
          neighborhood of size $r$ in the input sequence centered around the
          respective output position. This would increase the maximum path
          length to $O(n/r)$. We plan to investigate this approach further in
          future work.
        </div>
        <div class="translation">
          <p>
            ç‚ºäº†æé«˜æ¶‰åŠ<strong>éå¸¸é•·åºåˆ—</strong>çš„ä»»å‹™çš„è¨ˆç®—æ€§èƒ½ï¼Œè‡ªæ³¨æ„åŠ›å¯ä»¥è¢«é™åˆ¶ç‚ºåƒ…è€ƒæ…®ä»¥ç›¸æ‡‰è¼¸å‡ºä½ç½®ç‚ºä¸­å¿ƒçš„è¼¸å…¥åºåˆ—ä¸­å¤§å°ç‚º
            $r$ çš„é„°åŸŸã€‚é€™å°‡ä½¿æœ€å¤§è·¯å¾‘é•·åº¦å¢åŠ åˆ°
            $O(n/r)$ã€‚æˆ‘å€‘è¨ˆåŠƒåœ¨æœªä¾†çš„å·¥ä½œä¸­é€²ä¸€æ­¥ç ”ç©¶é€™ç¨®æ–¹æ³•ã€‚
          </p>
        </div>
      </div>

      <div class="analogy">
        <h4>ğŸ”§ é›™é‡é¡æ¯”ï¼šRestricted Self-Attention</h4>

        <h4>ğŸ’¡ ç”Ÿæ´»é¡æ¯”ï¼šæœƒè­°å®¤åº§ä½å®‰æ’</h4>
        <p>
          <strong>æƒ…å¢ƒï¼š</strong>100
          äººçš„å¤§æœƒè­°ï¼Œæ¯å€‹äººéƒ½æƒ³è·Ÿæ‰€æœ‰äººäº¤æµï¼ˆSelf-Attentionï¼‰ã€‚
        </p>
        <ul>
          <li>
            <strong>å•é¡Œï¼š</strong>100 Ã— 100 = 10,000 æ¬¡å°è©±ï¼ˆè¤‡é›œåº¦çˆ†ç‚¸ï¼‰ã€‚
          </li>
          <li>
            <strong>Restricted æ–¹æ¡ˆï¼š</strong>æ¯å€‹äººåªè·Ÿ<strong
              >å·¦å³ 10 å€‹äºº</strong
            >äº¤æµï¼ˆ$r=10$ï¼‰ã€‚
          </li>
          <li>
            <strong>çµæœï¼š</strong>100 Ã— 10 = 1,000 æ¬¡å°è©±ï¼ˆé™ä½ 10 å€ï¼‰ã€‚
          </li>
          <li>
            <strong>ä»£åƒ¹ï¼š</strong>é ç«¯çš„äººéœ€è¦é€éã€Œä¸­ä»‹ã€å‚³éè¨Šæ¯ï¼ˆè·¯å¾‘é•·åº¦
            $O(n/r) = 10$ï¼‰ã€‚
          </li>
        </ul>

        <h4>ğŸ”§ å·¥ç¨‹é¡æ¯”ï¼šSliding Window Attention</h4>
        <pre><code>// Full Self-Attention (O(nÂ²))
for i in range(n):
    for j in range(n):
        attention[i][j] = compute(Q[i], K[j])

// Restricted Self-Attention (O(nÃ—r))
for i in range(n):
    start = max(0, i - r//2)
    end = min(n, i + r//2)
    for j in range(start, end):  // åªçœ‹é„°åŸŸ
        attention[i][j] = compute(Q[i], K[j])

// è¤‡é›œåº¦ï¼šO(nÂ²) â†’ O(nÃ—r)
// è·¯å¾‘é•·åº¦ï¼šO(1) â†’ O(n/r)</code></pre>
      </div>

      <div class="text-pair">
        <div class="original-text">
          A single convolutional layer with kernel width $k < n$ does not
          connect all pairs of input and output positions. Doing so requires a
          stack of $O(n/k)$ convolutional layers in the case of contiguous
          kernels, or $O(\log_k(n))$ in the case of dilated convolutions [18],
          increasing the length of the longest paths between any two positions
          in the network.
        </div>
        <div class="translation">
          <p>
            å–®å±¤æ ¸å¯¬åº¦ $k < n$
            çš„å·ç©å±¤<strong>ç„¡æ³•é€£æ¥æ‰€æœ‰è¼¸å…¥å’Œè¼¸å‡ºä½ç½®å°</strong>ã€‚è¦åšåˆ°é€™ä¸€é»ï¼Œå°æ–¼é€£çºŒæ ¸ï¼Œéœ€è¦<strong
              >å †ç–Š $O(n/k)$ å€‹å·ç©å±¤</strong
            >ï¼Œæˆ–è€…å°æ–¼è†¨è„¹å·ç© [18]ï¼Œéœ€è¦ $O(\log_k(n))$
            å±¤ï¼Œå¾è€Œå¢åŠ äº†ç¶²è·¯ä¸­ä»»æ„å…©å€‹ä½ç½®ä¹‹é–“çš„æœ€é•·è·¯å¾‘é•·åº¦ã€‚
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>ğŸ—ï¸ CNN çš„å±¤ç´šçµæ§‹</h4>
        <p>
          <strong>å•é¡Œï¼š</strong>å–®å±¤ CNN åªèƒ½çœ‹åˆ°å±€éƒ¨ï¼ˆæ ¸å¤§å°
          $k$ï¼‰ï¼Œå¦‚ä½•çœ‹åˆ°å…¨åŸŸï¼Ÿ
        </p>
        <p><strong>è§£æ±ºæ–¹æ¡ˆï¼š</strong>å †ç–Šå¤šå±¤ï¼</p>
        <pre><code>å±¤ 1ï¼šæ¯å€‹ä½ç½®çœ‹ k å€‹é„°å±…
å±¤ 2ï¼šæ¯å€‹ä½ç½®çœ‹ kÂ² å€‹é„°å±… (é€éå±¤ 1)
å±¤ 3ï¼šæ¯å€‹ä½ç½®çœ‹ kÂ³ å€‹é„°å±…
...
å±¤ log_k(n)ï¼šæ¯å€‹ä½ç½®çœ‹æ•´å€‹åºåˆ—

ä¾‹å­ï¼šk=3, n=27
- éœ€è¦ logâ‚ƒ(27) = 3 å±¤æ‰èƒ½è¦†è“‹å…¨åŸŸ
- Self-Attentionï¼šåªéœ€ 1 å±¤ï¼</code></pre>
      </div>

      <div class="text-pair">
        <div class="original-text">
          Convolutional layers are generally more expensive than recurrent
          layers, by a factor of $k$. Separable convolutions [6], however,
          decrease the complexity considerably, to $O(k \cdot n \cdot d + n
          \cdot d^2)$. Even with $k = n$, however, the complexity of a separable
          convolution is equal to the combination of a self-attention layer and
          a point-wise feed-forward layer, the approach we take in our model.
        </div>
        <div class="translation">
          <p>
            å·ç©å±¤é€šå¸¸æ¯”éæ­¸å±¤<strong>æ›´æ˜‚è²´</strong>ï¼Œä¿‚æ•¸ç‚º $k$ã€‚ç„¶è€Œï¼Œ<strong
              >å¯åˆ†é›¢å·ç©</strong
            >
            [6] å¯ä»¥é¡¯è‘—é™ä½è¤‡é›œåº¦ï¼Œé™è‡³ $O(k \cdot n \cdot d + n \cdot
            d^2)$ã€‚ä½†å³ä½¿ $k =
            n$ï¼Œå¯åˆ†é›¢å·ç©çš„è¤‡é›œåº¦ä¹Ÿç­‰æ–¼<strong>è‡ªæ³¨æ„åŠ›å±¤åŠ ä¸Šé€é»å‰é¥‹å±¤</strong>çš„çµ„åˆï¼Œé€™æ­£æ˜¯æˆ‘å€‘åœ¨æ¨¡å‹ä¸­æ¡å–çš„æ–¹æ³•ã€‚
          </p>
        </div>
      </div>

      <div class="solution">
        <h4>âœ… æœ€çµ‚é¸æ“‡ï¼šSelf-Attention + FFN</h4>
        <p><strong>è¨­è¨ˆæ±ºç­–ï¼š</strong></p>
        <ul>
          <li>
            âœ… <strong>Self-Attentionï¼š</strong>è™•ç†é•·è·é›¢ä¾è³´ï¼ˆ$O(1)$ è·¯å¾‘ï¼‰+
            å®Œå…¨ä¸¦è¡Œï¼ˆ$O(1)$ é †åºæ“ä½œï¼‰
          </li>
          <li>
            âœ…
            <strong>FFNï¼ˆFeed-Forward Networkï¼‰ï¼š</strong
            >å¢åŠ éç·šæ€§è¡¨é”èƒ½åŠ›ï¼ˆé¡ä¼¼ CNN çš„é€é»è®Šæ›ï¼‰
          </li>
        </ul>
        <p>
          <strong>çµæœï¼š</strong>Transformer = Self-Attention +
          FFNï¼Œå…¼å…·ã€Œå…¨åŸŸè¦–é‡ã€å’Œã€Œè¡¨é”èƒ½åŠ›ã€ï¼
        </p>
      </div>

      <div class="text-pair">
        <div class="original-text">
          As side benefit, self-attention could yield more interpretable models.
          We inspect attention distributions from our models and present and
          discuss examples in the appendix. Not only do individual attention
          heads clearly learn to perform different tasks, many appear to exhibit
          behavior related to the syntactic and semantic structure of the
          sentences.
        </div>
        <div class="translation">
          <p>
            ä½œç‚ºé™„å¸¶å¥½è™•ï¼Œè‡ªæ³¨æ„åŠ›å¯ä»¥<strong>ç”¢ç”Ÿæ›´å¯è§£é‡‹çš„æ¨¡å‹</strong>ã€‚æˆ‘å€‘æª¢æŸ¥äº†æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†ä½ˆï¼Œä¸¦åœ¨é™„éŒ„ä¸­å‘ˆç¾å’Œè¨è«–ç¤ºä¾‹ã€‚ä¸åƒ…å„å€‹æ³¨æ„åŠ›é ­æ˜é¡¯å­¸æœƒåŸ·è¡Œä¸åŒçš„ä»»å‹™ï¼Œè¨±å¤šé‚„è¡¨ç¾å‡ºèˆ‡å¥å­çš„<strong>å¥æ³•å’Œèªç¾©çµæ§‹ç›¸é—œ</strong>çš„è¡Œç‚ºã€‚
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>ğŸ” å¯è§£é‡‹æ€§å„ªå‹¢ï¼ˆInterpretabilityï¼‰</h4>
        <p><strong>Self-Attention çš„é¡å¤–å¥½è™•ï¼š</strong>å¯è¦–åŒ–æ³¨æ„åŠ›æ¬Šé‡ï¼</p>
        <ul>
          <li>
            <strong>RNN/CNNï¼š</strong
            >éš±è—ç‹€æ…‹æ˜¯ã€Œé»‘ç›’å­ã€ï¼Œé›£ä»¥ç†è§£æ¨¡å‹åœ¨ã€Œçœ‹ã€ä»€éº¼ã€‚
          </li>
          <li>
            <strong>Self-Attentionï¼š</strong>å¯ä»¥ç•«å‡ºã€Œèª°åœ¨çœ‹èª°ã€çš„æ³¨æ„åŠ›çŸ©é™£ã€‚
          </li>
        </ul>
        <p><strong>ç™¼ç¾ï¼š</strong></p>
        <ul>
          <li>æŸäº› head å°ˆæ³¨æ–¼<strong>å¥æ³•é—œä¿‚</strong>ï¼ˆå¦‚ä¸»è©-å‹•è©ï¼‰ã€‚</li>
          <li>
            æŸäº› head å°ˆæ³¨æ–¼<strong>èªç¾©é—œä¿‚</strong>ï¼ˆå¦‚ã€Œmaking ... more
            difficultã€çš„é•·è·é›¢ä¾è³´ï¼‰ã€‚
          </li>
        </ul>
        <p><em>â†’ è«–æ–‡é™„éŒ„æœ‰è©³ç´°çš„æ³¨æ„åŠ›å¯è¦–åŒ–ç¯„ä¾‹ï¼ˆFigure 3-5ï¼‰ã€‚</em></p>
      </div>

      <div class="nav-bar">
        <a href="03-2-model-architecture-components.html" class="nav-btn">
          â† ä¸Šä¸€é ï¼šModel Architecture (2/2)
        </a>
        <a href="05-training.html" class="nav-btn primary">
          ä¸‹ä¸€é ï¼šTraining â†’
        </a>
      </div>

      <div
        style="
          margin-top: 40px;
          padding: 20px;
          background: #ecf0f1;
          border-radius: 5px;
          text-align: center;
        "
      >
        <p><strong>ğŸ“š å­¸ç¿’æª¢æŸ¥é»</strong></p>
        <p>åœ¨é€²å…¥ä¸‹ä¸€é ä¹‹å‰ï¼Œè«‹ç¢ºä¿ä½ ç†è§£ï¼š</p>
        <ul style="text-align: left; max-width: 600px; margin: 20px auto">
          <li>
            âœ… Self-Attention åœ¨ã€Œä¸¦è¡Œèƒ½åŠ›ã€($O(1)$) å’Œã€Œé•·è·é›¢ä¾è³´ã€($O(1)$)
            ä¸Šéƒ½æ˜¯æœ€å„ªçš„
          </li>
          <li>âœ… ç•¶ $n < d$ æ™‚ï¼ˆNLP å¸¸è¦‹æƒ…æ³ï¼‰ï¼ŒSelf-Attention æ¯” RNN æ›´å¿«</li>
          <li>âœ… RNN çš„è‡´å‘½ç¼ºé»æ˜¯ã€Œé †åºæ“ä½œã€($O(n)$)ï¼Œç„¡æ³•ä¸¦è¡Œè¨“ç·´</li>
          <li>âœ… CNN éœ€è¦å¤šå±¤å †ç–Šæ‰èƒ½è¦†è“‹å…¨åŸŸï¼Œè€Œ Self-Attention åªéœ€ä¸€å±¤</li>
        </ul>
      </div>
    </div>
  </body>
</html>
