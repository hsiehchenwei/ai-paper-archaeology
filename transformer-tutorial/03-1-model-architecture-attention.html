<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Transformer 論文深度解析 - 第3頁：Model Architecture (1/2)</title>
    <link rel="stylesheet" href="../styles/global.css" />
    <link rel="stylesheet" href="../styles/paper-reading.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=JetBrains+Mono:wght@400;700&family=Merriweather:ital,wght@0,400;0,700;1,400&display=swap"
      rel="stylesheet"
    />
    <!-- MathJax for mathematical formulas -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$']],
          displayMath: [['$$', '$$']]
        }
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <div class="container">
      <h1>
        📖 Attention Is All You Need <br /><span
          style="
            font-size: 0.6em;
            color: var(--text-secondary);
            font-weight: normal;
          "
          >深度解析系列</span
        >
      </h1>
      <h2>第 3 頁 (1/2)：Model Architecture - Attention 機制</h2>

      <div class="header-section">
        <div class="paper-meta">
          <div class="meta-item">
            <strong>目前章節</strong>
            3. Model Architecture (Part 1)
          </div>
          <div class="meta-item">
            <strong>核心主題</strong>
            Encoder-Decoder、Scaled Dot-Product Attention、Multi-Head
          </div>
          <div class="meta-item">
            <strong>閱讀難度</strong>
            ⭐⭐⭐⭐⭐ (涉及數學公式，會逐步拆解)
          </div>
        </div>
      </div>

      <!-- ============= 整體架構 ============= -->
      <h3>1. Encoder-Decoder 整體架構</h3>

      <div class="text-pair">
        <div class="original-text">
          Most competitive neural sequence transduction models have an
          encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input
          sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of
          continuous representations $\mathbf{z} = (z_1, ..., z_n)$. Given
          $\mathbf{z}$, the decoder then generates an output sequence $(y_1,
          ..., y_m)$ one element at a time.
        </div>
        <div class="translation">
          <p>
            大多數具有競爭力的神經序列轉換模型都採用<strong>編碼器-解碼器結構</strong> [5, 2, 35]。在這裡，編碼器將輸入的符號表示序列 $(x_1, ..., x_n)$ 映射到連續表示序列 $\mathbf{z} = (z_1, ..., z_n)$。給定 $\mathbf{z}$，解碼器然後<strong>一次生成一個元素</strong>，產生輸出序列 $(y_1, ..., y_m)$。
          </p>
        </div>
      </div>

      <div class="explanation">
        <h4>🏗️ Encoder-Decoder 架構解析</h4>
        <p><strong>核心概念：兩段式處理</strong></p>
        <table>
          <tr>
            <th>階段</th>
            <th>輸入</th>
            <th>輸出</th>
            <th>功能</th>
          </tr>
          <tr>
            <td><strong>Encoder (編碼器)</strong></td>
            <td>離散符號序列<br>$(x_1, x_2, ..., x_n)$<br><em>例：[The, cat, sat]</em></td>
            <td>連續向量序列<br>$\mathbf{z} = (z_1, ..., z_n)$<br><em>每個 $z_i \in \mathbb{R}^{512}$</em></td>
            <td>理解輸入的語義</td>
          </tr>
          <tr>
            <td><strong>Decoder (解碼器)</strong></td>
            <td>Encoder 的輸出 $\mathbf{z}$<br>+ 已生成的前綴<br>$(y_1, ..., y_{i-1})$</td>
            <td>下一個符號 $y_i$<br><em>一次生成一個詞</em></td>
            <td>根據語義生成輸出</td>
          </tr>
        </table>
      </div>

      <div class="analogy">
        <h4>🔧 雙重類比：Encoder-Decoder</h4>
        
        <h4>💡 生活類比：翻譯員的工作流程</h4>
        <p><strong>情境：</strong>一位翻譯員要把英文演講翻譯成中文。</p>
        <ul>
          <li><strong>Encoder (理解階段)</strong>：翻譯員先<strong>完整聽完</strong>英文演講，在腦海中形成對整段話的理解（語義表示 $\mathbf{z}$）。</li>
          <li><strong>Decoder (表達階段)</strong>：基於理解，<strong>一句一句</strong>用中文表達出來，每說一句都參考前面已經說過的內容。</li>
        </ul>

        <h4>🤖 AI 體驗類比：與 ChatGPT 對話</h4>
        <p><strong>情境：</strong>你問 ChatGPT「請幫我寫一首關於秋天的詩」。</p>
        <ul>
          <li><strong>Encoder (讀題)</strong>：模型先讀取你的整個 Prompt，將「寫詩」、「秋天」等需求轉換成向量理解。</li>
          <li><strong>Decoder (生成)</strong>：模型開始<strong>逐字生成</strong>回答，「秋」、「風」、「吹」、「起」... 每個字都是基於前面的字和你原本的 Prompt 生成的。</li>
        </ul>

        <h4>🔧 工程類比：編譯器的兩階段處理</h4>
        <pre><code>// Encoder: Lexer + Parser (理解原始碼)
source_code = "int x = 5;"
tokens = lexer(source_code)        // [INT, IDENTIFIER, ASSIGN, NUMBER]
AST = parser(tokens)                // 抽象語法樹 (類比為 z)

// Decoder: Code Generator (生成目標碼)
assembly = []
for node in AST:
    assembly.append(generate(node, previous_context))  // 逐行生成
output = "\n".join(assembly)</code></pre>
      </div>

      <div class="text-pair">
        <div class="original-text">
          At each step the model is auto-regressive [10], consuming the
          previously generated symbols as additional input when generating the
          next.
        </div>
        <div class="translation">
          <p>
            在每一步，模型都是<strong>自回歸的</strong>（auto-regressive） [10]，在生成下一個符號時，會將<span class="highlight">先前生成的符號作為額外輸入</span>。
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>🎯 Auto-Regressive（自回歸）機制</h4>
        <p><strong>定義：</strong>下一個輸出依賴於前面所有已生成的輸出。</p>
        
        <p><strong>生成過程範例（英翻中）：</strong></p>
        <table>
          <tr>
            <th>步驟</th>
            <th>已生成內容</th>
            <th>預測下一個詞</th>
          </tr>
          <tr>
            <td>1</td>
            <td>[START]</td>
            <td>→ "這"</td>
          </tr>
          <tr>
            <td>2</td>
            <td>[START, 這]</td>
            <td>→ "隻"</td>
          </tr>
          <tr>
            <td>3</td>
            <td>[START, 這, 隻]</td>
            <td>→ "貓"</td>
          </tr>
          <tr>
            <td>4</td>
            <td>[START, 這, 隻, 貓]</td>
            <td>→ "坐"</td>
          </tr>
        </table>
        <p><strong>關鍵：</strong>每一步都「回顧」(regress) 前面的生成結果。</p>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <!-- ============= Figure 2: Attention Mechanisms ============= -->
      <div class="key-concept">
        <h4>📐 Figure 2: Attention 機制視覺化</h4>
        <img 
          src="images/figure-2-attention-mechanisms.png" 
          alt="Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention" 
          style="width: 100%; max-width: 900px; display: block; margin: 20px auto; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" 
        />
        <p style="text-align: center; color: var(--text-secondary); font-size: 0.9em; margin-top: 10px;">
          <strong>圖 2：</strong>（左）Scaled Dot-Product Attention 的計算流程；（右）Multi-Head Attention 由多個並行的 Attention 層組成
        </p>
      </div>

      <div class="explanation">
        <h4>🔍 圖片詳細解說</h4>
        
        <h5>📍 左圖：Scaled Dot-Product Attention（縮放點積注意力）</h5>
        <p><strong>由下往上的計算流程：</strong></p>
        
        <div style="margin: 30px 0; text-align: center;">
            <img src="../images/user_generate_image_20260101095407_9f2c.png" 
                 alt="Self-Attention 視覺化" 
                 style="max-width: 100%; border-radius: var(--radius-lg); box-shadow: var(--shadow-lg);">
            <p class="caption">
                <strong>🔍 Self-Attention 運作視覺化</strong><br>
                句子 "The cat sat on the mat" 的注意力權重<br>
                線條粗細代表注意力強度 (cat→sat, sat→on, on→mat)<br>
                顏色從藍(弱)到紅(強),展示每個詞如何關注其他詞！
            </p>
        </div>
        
        <ol>
          <li><strong>輸入：Q, K, V</strong>
            <ul>
              <li>Q (Query)：查詢向量，「我想找什麼」</li>
              <li>K (Key)：鍵向量，「我能提供什麼資訊」</li>
              <li>V (Value)：值向量，「我實際的內容」</li>
            </ul>
          </li>
          <li><strong>MatMul（矩陣乘法）：Q × K</strong>
            <ul>
              <li>計算 Query 和 Key 的相似度</li>
              <li>結果：注意力分數矩陣（每個位置對每個位置的「關注度」）</li>
            </ul>
          </li>
          <li><strong>Scale（縮放）：除以 √d_k</strong>
            <ul>
              <li>d_k 是 Key 的維度（例如 64）</li>
              <li>目的：防止點積結果過大，避免 softmax 梯度消失</li>
            </ul>
          </li>
          <li><strong>Mask (opt.)（遮罩，可選）</strong>
            <ul>
              <li>用於 Decoder 的 Masked Attention</li>
              <li>將「未來」位置的分數設為 -∞，確保生成時不會偷看</li>
            </ul>
          </li>
          <li><strong>SoftMax（歸一化）</strong>
            <ul>
              <li>將注意力分數轉換為機率分佈（總和為 1）</li>
              <li>結果：注意力權重矩陣</li>
            </ul>
          </li>
          <li><strong>MatMul（加權求和）：Attention × V</strong>
            <ul>
              <li>用注意力權重對 Value 進行加權平均</li>
              <li>結果：最終的 Attention 輸出</li>
            </ul>
          </li>
        </ol>

        <h5>📍 右圖：Multi-Head Attention（多頭注意力）</h5>
        
        <div style="margin: 30px 0; text-align: center;">
            <img src="../images/user_generate_image_20260101095428_02ce.png" 
                 alt="Multi-Head Attention 視覺化" 
                 style="max-width: 100%; border-radius: var(--radius-lg); box-shadow: var(--shadow-lg);">
            <p class="caption">
                <strong>🌈 Multi-Head Attention 並行處理</strong><br>
                8 個注意力頭同時處理相同輸入,每個用不同顏色表示<br>
                Head 1:語法關係 | Head 2:語義關係 | Head 3:位置關係...<br>
                最後合併所有頭的輸出 → 更豐富的表達！
            </p>
        </div>
        
        <p><strong>由下往上的計算流程：</strong></p>
        <ol>
          <li><strong>輸入：V, K, Q</strong>（注意順序：Value, Key, Query）</li>
          <li><strong>Linear（線性投影）× 3</strong>
            <ul>
              <li>將 Q, K, V 各自通過 h 個不同的線性變換（h = 頭數，論文中 h=8）</li>
              <li>每個頭使用不同的權重矩陣 W^Q, W^K, W^V</li>
              <li>結果：每個頭得到自己的 Q_i, K_i, V_i（維度降低為 d_model/h = 64）</li>
            </ul>
          </li>
          <li><strong>Scaled Dot-Product Attention × h</strong>
            <ul>
              <li>h 個頭<strong>並行</strong>計算各自的 Attention（使用左圖的流程）</li>
              <li>每個頭學習不同的「注意力模式」（例如：語法關係、語義關係）</li>
            </ul>
          </li>
          <li><strong>Concat（拼接）</strong>
            <ul>
              <li>將 h 個頭的輸出拼接起來</li>
              <li>結果：恢復到原始維度 d_model = 512</li>
            </ul>
          </li>
          <li><strong>Linear（最終投影）</strong>
            <ul>
              <li>通過一個線性層融合所有頭的資訊</li>
              <li>權重矩陣 W^O</li>
            </ul>
          </li>
        </ol>

        <h5>📍 為什麼需要 Multi-Head？</h5>
        <ul>
          <li>✅ <strong>多角度關注：</strong>不同的頭可以學習不同類型的依賴關係
            <ul>
              <li>Head 1：可能關注「主詞-動詞」的語法關係</li>
              <li>Head 2：可能關注「形容詞-名詞」的修飾關係</li>
              <li>Head 3：可能關注長距離的語義關係</li>
            </ul>
          </li>
          <li>✅ <strong>表達能力更強：</strong>單一 Attention 可能會「平均化」所有資訊，Multi-Head 可以保留更多細節</li>
          <li>✅ <strong>計算效率：</strong>雖然有 h 個頭，但每個頭的維度降為 d_k = d_model/h，總計算量與單頭相當</li>
        </ul>

        <h5>📍 實際參數（論文設定）</h5>
        <table>
          <tr>
            <th>參數</th>
            <th>數值</th>
            <th>說明</th>
          </tr>
          <tr>
            <td>h（頭數）</td>
            <td>8</td>
            <td>並行運行 8 個 Attention</td>
          </tr>
          <tr>
            <td>d_model</td>
            <td>512</td>
            <td>模型維度</td>
          </tr>
          <tr>
            <td>d_k（Key 維度）</td>
            <td>64</td>
            <td>= d_model / h = 512/8</td>
          </tr>
          <tr>
            <td>d_v（Value 維度）</td>
            <td>64</td>
            <td>= d_model / h = 512/8</td>
          </tr>
        </table>
      </div>

      <div class="analogy">
        <h4>🤖 AI 體驗連結：ChatGPT 如何理解代名詞？</h4>
        <p>當你對 ChatGPT 說：<strong>「蘋果很好吃，但我最近不常吃它。」</strong></p>
        <p><strong>Self-Attention 在做什麼？</strong></p>
        <ul>
            <li>當模型讀到<strong>「它」</strong>這個字時，它的 Query 向量會去搜尋前面所有字的 Key。</li>
            <li><strong>「蘋果」</strong>的 Key 會和「它」的 Query 最匹配（分數最高）。</li>
            <li>因此，「它」的 Value 就會吸收大量「蘋果」的資訊。</li>
        </ul>
        <p>這就是為什麼 ChatGPT 知道「它」指的就是蘋果，而不是別的東西。這就是 <strong>Coreference Resolution（指代消解）</strong>。</p>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <!-- ============= Scaled Dot-Product Attention ============= -->
      <h3>2. Scaled Dot-Product Attention（縮放點積注意力）</h3>

      <div class="key-concept">
        <h4>❓ 等等...Q/K/V 是怎麼來的？</h4>
        <p><strong>疑問：</strong>上面說輸入是「向量」，但這裡突然出現 Q、K、V 三組向量？</p>
        <p><strong>答案：</strong>它們是從原始輸入向量<strong>「轉換」</strong>出來的！</p>

        <h5>💡 生活類比：一人分飾三角</h5>
        <p>想像你在圖書館：</p>
        <ul>
          <li><strong>作為讀者：</strong>你拿著「需求清單」(Query) 去找書</li>
          <li><strong>作為書架標籤：</strong>你自己也被貼上「關鍵字」(Key) 供別人搜尋</li>
          <li><strong>作為書本內容：</strong>你自己也有「實際內容」(Value) 可被提取</li>
        </ul>
        <p><strong>關鍵：</strong>同一個「詞向量」要扮演三種角色！</p>

        <h5>💡 補充類比：8 位專家看同一份履歷</h5>
        <p><strong>情境：</strong>你投履歷應徵工作，8 位面試官（8 個 heads）同時審查你的履歷。</p>
        <ul>
          <li><strong>你的履歷：</strong>一份完整的 512 項資料（學歷、經歷、技能...）</li>
          <li><strong>面試官 1（Head 1）：</strong>
            <br>只關注「技術能力」相關的 64 項特徵
            <br>→ 從你的 512 項資料中，用自己的「篩選標準」(W_1^Q, W_1^K, W_1^V) 提取出 64 項
          </li>
          <li><strong>面試官 2（Head 2）：</strong>
            <br>只關注「團隊合作」相關的 64 項特徵
            <br>→ 同樣是你的 512 項資料，但用不同的「篩選標準」(W_2^Q, W_2^K, W_2^V) 提取出另外 64 項
          </li>
          <li><strong>面試官 3~8：</strong>各自關注不同面向（領導力、創新力、穩定性...）</li>
        </ul>
        <p><strong>關鍵洞察：</strong></p>
        <ul>
          <li>❌ <strong>不是</strong>把履歷切成 8 份，每人看 1/8</li>
          <li>✅ <strong>而是</strong>8 位面試官都看<strong>完整的</strong>履歷，但各自關注不同面向</li>
          <li>✅ 每位面試官用自己的「評估標準」從 512 項中提取關鍵的 64 項</li>
          <li>✅ 最後綜合 8 位面試官的意見（8×64=512）做出決策</li>
        </ul>

        <h5>🔧 工程實作：矩陣轉換</h5>
        <p><strong>步驟 0：原始輸入向量是什麼？</strong></p>
        <p>在真實情境中，「原始輸入向量」指的是：</p>
        <pre><code>// 真實例子：處理句子 "The cat sat"
句子：["The", "cat", "sat"]

// 步驟 1：Embedding（第 3-2 頁會詳細解釋）
"The" → embedding_table["The"] → [0.12, -0.34, 0.56, ..., 0.78]  // 512 個數字
"cat" → embedding_table["cat"] → [0.23, 0.45, -0.12, ..., 0.91]  // 512 個數字
"sat" → embedding_table["sat"] → [-0.11, 0.67, 0.34, ..., -0.22] // 512 個數字

// 步驟 2：加上 Positional Encoding（告訴模型位置資訊）
"The" → [0.12, -0.34, ...] + [pos_0] = x_0 (512 維)
"cat" → [0.23, 0.45, ...] + [pos_1] = x_1 (512 維)
"sat" → [-0.11, 0.67, ...] + [pos_2] = x_2 (512 維)

// 這就是「原始輸入向量」！每個詞都是 512 維的向量</code></pre>

        <p><strong>步驟 1：Multi-Head 如何處理？（以 "cat" 為例）</strong></p>
        <pre><code>// 現在我們有 "cat" 的 512 維向量
x_cat = [0.23, 0.45, -0.12, ..., 0.91]  // 512 個數字

// ❓ 問題：這 512 維會被「拆開」嗎？
// ❌ 答案：不是！每個 head 都會「複製」這 512 維，然後各自投影

// Head 1 的處理（有自己專屬的投影矩陣）：
Q_1 = x_cat × W_1^Q  // (1×512) × (512×64) = (1×64)
K_1 = x_cat × W_1^K  // (1×512) × (512×64) = (1×64)
V_1 = x_cat × W_1^V  // (1×512) × (512×64) = (1×64)
// Head 1 得到：Q_1(64維), K_1(64維), V_1(64維)

// Head 2 的處理（有不同的投影矩陣）：
Q_2 = x_cat × W_2^Q  // (1×512) × (512×64) = (1×64)
K_2 = x_cat × W_2^K  // (1×512) × (512×64) = (1×64)
V_2 = x_cat × W_2^V  // (1×512) × (512×64) = (1×64)
// Head 2 得到：Q_2(64維), K_2(64維), V_2(64維)

// ... Head 3, 4, 5, 6, 7, 8 同理

// 關鍵：每個 head 都從「同樣的 512 維輸入」各自投影出自己的 Q/K/V
// 不是「把 512 維切成 8 份」，而是「8 個 head 各自看同一個 512 維」</code></pre>

        <p><strong>步驟 2：每個 Head 獨立計算 Attention</strong></p>
        <pre><code>// Head 1 計算自己的 Attention
output_1 = Attention(Q_1, K_1, V_1)  // 輸出 64 維

// Head 2 計算自己的 Attention
output_2 = Attention(Q_2, K_2, V_2)  // 輸出 64 維

// ... Head 3~8 同理

// 步驟 3：拼接所有 head 的輸出
concat = [output_1 | output_2 | ... | output_8]
// (64) + (64) + ... + (64) = 512 維

// 步驟 4：最終投影
final_output = concat × W^O  // (512) × (512×512) = 512 維</code></pre>

        <p><strong>💡 視覺化理解：</strong></p>
        <pre><code>           512 維的 "cat"
                 │
        ┌────────┼────────┐
        │        │        │
    Head 1   Head 2 ... Head 8
        │        │        │
    投影到   投影到    投影到
    Q₁K₁V₁  Q₂K₂V₂   Q₈K₈V₈
    (各64維) (各64維) (各64維)
        │        │        │
    算Attn  算Attn   算Attn
    輸出64   輸出64   輸出64
        │        │        │
        └────────┴────────┘
                 │
          拼接成 512 維</code></pre>

        <p><strong>🎯 關鍵理解：</strong></p>
        <ul>
          <li>✅ <strong>原始輸入：</strong>每個詞是 512 維（來自 Embedding + Positional Encoding）</li>
          <li>✅ <strong>每個 head：</strong>都從這 512 維投影出自己的 Q(64維)、K(64維)、V(64維)</li>
          <li>✅ <strong>不是拆分：</strong>不是把 512 維「切成 8 份」，而是每個 head 都「看同一個 512 維，但用不同的投影矩陣」</li>
          <li>✅ <strong>為什麼 64？</strong>設計選擇：512 ÷ 8 = 64，讓最後拼接時剛好回到 512 維</li>
        </ul>

        <p><strong>為什麼要這樣做？</strong></p>
        <table>
          <tr>
            <th>原因</th>
            <th>說明</th>
          </tr>
          <tr>
            <td><strong>角色分離</strong></td>
            <td>
              同一個詞在「查詢時」和「被查詢時」可能需要不同的表示。
              <br>例如：「it」作為 Query 想找指代對象，作為 Key 時提供自己的特徵。
            </td>
          </tr>
          <tr>
            <td><strong>可學習性</strong></td>
            <td>
              $W^Q, W^K, W^V$ 是訓練過程中「學習」出來的。
              <br>模型會自動學會：「什麼樣的 Query 該去匹配什麼樣的 Key」。
            </td>
          </tr>
          <tr>
            <td><strong>降維效率</strong></td>
            <td>
              在 Multi-Head Attention 中，每個 head 只處理部分維度。
              <br>從 512 維降到 64 維（每個 head），減少單個 head 的計算量。
              <br>8 個 head × 64 維 = 512 維（總維度不變，但並行化提升效率）。
            </td>
          </tr>
          <tr>
            <td><strong>彈性變換</strong></td>
            <td>
              不同的投影矩陣可以學習提取不同的「特徵子空間」。
              <br>例如：W_1^Q 可能學習提取「語法特徵」，W_2^Q 學習提取「語義特徵」。
            </td>
          </tr>
        </table>

        <p><strong>💡 類比：資料庫的索引轉換</strong></p>
        <pre><code>// 原始資料：一篇文章（512 個屬性）
article = {title, content, tags, author, date, ...}  // 512 個欄位

// 轉換成不同用途的索引
search_index (Query)：  只保留「關鍵字、摘要」
match_index (Key)：     只保留「分類、標籤」
content_cache (Value)： 保留「完整內容」

// 為什麼不直接用原始資料？
// → 因為「搜尋」和「匹配」需要不同的特徵提取方式！</code></pre>

        <h5>📌 重要補充：徹底釐清「拆分」vs「投影」</h5>
        <table>
          <tr>
            <th>常見誤解 ❌</th>
            <th>實際情況 ✅</th>
          </tr>
          <tr>
            <td>
              <strong>誤解：</strong>把 512 維「切成」8 份<br>
              Head 1 拿到 [0~63] 維度<br>
              Head 2 拿到 [64~127] 維度<br>
              ...
            </td>
            <td>
              <strong>正確：</strong>每個 head 都看「完整」的 512 維<br>
              Head 1: 512 維 →<strong>投影</strong>→ 64 維 (用 W_1)<br>
              Head 2: 512 維 →<strong>投影</strong>→ 64 維 (用 W_2)<br>
              每個 head 用不同的投影矩陣提取不同特徵
            </td>
          </tr>
          <tr>
            <td>
              <strong>誤解：</strong>512 維被拆成 Q/K/V<br>
              Q = [0~170] 維<br>
              K = [171~340] 維<br>
              V = [341~511] 維
            </td>
            <td>
              <strong>正確：</strong>512 維<strong>複製</strong>三次，各自投影<br>
              Q = 512 維 →<strong>投影</strong>→ 64 維 (用 W^Q)<br>
              K = 512 維 →<strong>投影</strong>→ 64 維 (用 W^K)<br>
              V = 512 維 →<strong>投影</strong>→ 64 維 (用 W^V)<br>
              同一個 512 維輸入，但扮演三種不同角色
            </td>
          </tr>
        </table>

        <h5>🎯 真實數據流（以 "cat" 為例）</h5>
        <pre><code>輸入："cat" 的 512 維向量
      x_cat = [0.23, 0.45, -0.12, ..., 0.91]  (512 個數字)

┌─────────────────────────────────────────────────────────┐
│                    Multi-Head Attention                  │
│                                                          │
│  Head 1:                Head 2:              Head 8:    │
│  x_cat × W_1^Q = Q_1    x_cat × W_2^Q = Q_2  ...        │
│  x_cat × W_1^K = K_1    x_cat × W_2^K = K_2  ...        │
│  x_cat × W_1^V = V_1    x_cat × W_2^V = V_2  ...        │
│  (各64維)              (各64維)           (各64維)      │
│      ↓                     ↓                  ↓         │
│  Attention(Q_1,K_1,V_1) Attention(...) Attention(...)   │
│  output_1(64維)        output_2(64維)  output_8(64維)  │
└─────────────────────────────────────────────────────────┘
                              ↓
              Concat: [out_1|out_2|...|out_8] = 512 維
                              ↓
                   Linear: 512 維 × W^O = 512 維

輸出："cat" 經過 Multi-Head Attention 後的新表示 (512 維)</code></pre>

        <p><strong>🔑 記住三個關鍵：</strong></p>
        <ol>
          <li><strong>複製，不是切割：</strong>每個 head 都看完整的 512 維輸入</li>
          <li><strong>投影，不是選取：</strong>用矩陣乘法「壓縮」成 64 維，不是直接取某幾個維度</li>
          <li><strong>並行，不是串行：</strong>8 個 head 同時計算，互不干擾</li>
        </ol>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <div class="text-pair">
        <div class="original-text">
          We call our particular attention "Scaled Dot-Product Attention". The
          input consists of queries and keys of dimension $d_k$, and values of
          dimension $d_v$. We compute the dot products of the query with all
          keys, divide each by $\sqrt{d_k}$, and apply a softmax function to
          obtain the weights on the values.
        </div>
        <div class="translation">
          <p>
            我們將我們的特定注意力機制稱為<strong>「縮放點積注意力」</strong>。輸入包括維度為 $d_k$ 的查詢（queries）和鍵（keys），以及維度為 $d_v$ 的值（values）。我們計算查詢與所有鍵的<strong>點積</strong>（dot products），然後除以 $\sqrt{d_k}$，並應用 <strong>softmax 函數</strong>來獲得值的權重。
          </p>
        </div>
      </div>

      <div class="text-pair">
        <div class="original-text">
          In practice, we compute the attention function on a set of queries
          simultaneously, packed together into a matrix $Q$. The keys and values
          are also packed together into matrices $K$ and $V$. We compute the
          matrix of outputs as:
          $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
        </div>
        <div class="translation">
          <p>
            在實踐中，我們同時計算一組查詢的注意力函數，將它們打包成矩陣 $Q$。鍵和值也分別打包成矩陣 $K$ 和 $V$。我們計算輸出矩陣為：
          </p>
          <p style="text-align: center; font-size: 1.2em; margin: 20px 0;">
            $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>🔬 公式深度拆解：每個符號的意義</h4>
        
        <table>
          <tr>
            <th>符號</th>
            <th>維度</th>
            <th>意義</th>
            <th>類比</th>
          </tr>
          <tr>
            <td>$Q$</td>
            <td>$(n \times d_k)$</td>
            <td><strong>查詢矩陣</strong><br>每一行是一個查詢向量</td>
            <td>「我想找什麼？」<br>$n$ 個搜尋需求</td>
          </tr>
          <tr>
            <td>$K$</td>
            <td>$(m \times d_k)$</td>
            <td><strong>鍵矩陣</strong><br>每一行是一個鍵向量</td>
            <td>「這些東西是什麼？」<br>$m$ 個資料的標籤</td>
          </tr>
          <tr>
            <td>$V$</td>
            <td>$(m \times d_v)$</td>
            <td><strong>值矩陣</strong><br>每一行是一個值向量</td>
            <td>「實際的內容」<br>$m$ 個資料本體</td>
          </tr>
          <tr>
            <td>$d_k$</td>
            <td>純量</td>
            <td>鍵/查詢的維度<br><em>Transformer: $d_k = 64$</em></td>
            <td>標籤的「特徵數」</td>
          </tr>
          <tr>
            <td>$d_v$</td>
            <td>純量</td>
            <td>值的維度<br><em>Transformer: $d_v = 64$</em></td>
            <td>內容的「特徵數」</td>
          </tr>
        </table>

        <p><strong>典型數值（Transformer Base）：</strong></p>
        <ul>
          <li>序列長度：$n = m = 512$ (最多 512 個詞)</li>
          <li>$d_k = d_v = 64$ (每個 head 的維度)</li>
          <li>$Q$: $(512 \times 64)$，$K$: $(512 \times 64)$，$V$: $(512 \times 64)$</li>
        </ul>
      </div>

      <div class="explanation">
        <h4>🧮 計算步驟拆解（一步一步）</h4>
        
        <p><strong>步驟 1：計算相似度 $QK^T$</strong></p>
        <ul>
          <li>操作：矩陣乘法 $(n \times d_k) \times (d_k \times m) = (n \times m)$</li>
          <li>意義：每個查詢與每個鍵的<strong>點積</strong>（相似度）</li>
          <li>輸出：相似度矩陣 $(n \times m)$，每個元素是一個相似度分數</li>
        </ul>
        <pre><code>例子：
Q[0] · K[0] = q₁k₁ + q₂k₂ + ... + q₆₄k₆₄  (點積)
結果越大 = 越相似</code></pre>

        <p><strong>步驟 2：縮放 $\frac{QK^T}{\sqrt{d_k}}$</strong></p>
        <ul>
          <li>操作：逐元素除以 $\sqrt{d_k}$ (例如 $\sqrt{64} = 8$)</li>
          <li>意義：防止點積值過大，穩定梯度</li>
          <li><strong>為什麼要縮放？</strong>當 $d_k$ 很大時，點積值會變得很大，導致 softmax 梯度消失</li>
        </ul>

        <p><strong>步驟 3：Softmax 歸一化</strong></p>
        <ul>
          <li>操作：對每一行（每個查詢）應用 softmax</li>
          <li>意義：將相似度轉換成<strong>機率分佈</strong>（總和為 1）</li>
          <li>公式：$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$</li>
        </ul>
        <pre><code>例子：
相似度：[2.0, 1.0, 0.5]
→ softmax → [0.66, 0.24, 0.10]  (總和 = 1)</code></pre>

        <p><strong>步驟 4：加權求和 $\times V$</strong></p>
        <ul>
          <li>操作：$(n \times m) \times (m \times d_v) = (n \times d_v)$</li>
          <li>意義：根據<strong>注意力權重</strong>，加權提取值向量</li>
          <li>輸出：最終的注意力輸出 $(n \times d_v)$</li>
        </ul>
        <pre><code>例子：
權重：[0.66, 0.24, 0.10]
值向量：V[0], V[1], V[2]
→ 輸出 = 0.66 × V[0] + 0.24 × V[1] + 0.10 × V[2]</code></pre>
      </div>

      <div class="analogy">
        <h4>🔧 雙重類比：Scaled Dot-Product Attention</h4>
        
        <h4>🤖 AI 體驗類比：ChatGPT 的知識檢索</h4>
        <p><strong>情境：</strong>你問 AI「蘋果的營養價值」。</p>
        <ol>
          <li><strong>Query ($Q$)</strong>：你的問題「蘋果 營養」。</li>
          <li><strong>Keys ($K$)</strong>：AI 資料庫中所有文章的標籤（如「水果」、「科技公司」、「電影」...）。</li>
          <li><strong>$QK^T$ (相似度匹配)</strong>：
            <br>「蘋果」與「水果」標籤匹配度高 (90%)。
            <br>「蘋果」與「科技公司」標籤匹配度中等 (50%)（因為有 Apple 公司）。
            <br>「蘋果」與「電影」匹配度低 (1%)。
          </li>
          <li><strong>Softmax & Values ($V$)</strong>：AI 主要提取「水果」類別下的營養知識，但也可能稍微提及 Apple 公司的資訊（如果沒有明確上下文），最終混合成回答。
          </li>
        </ol>

        <h4>💡 生活類比：餐廳點餐系統</h4>
        <p><strong>情境：</strong>你去一家有 100 道菜的餐廳點餐。</p>
        <ol>
          <li><strong>$QK^T$ (相似度計算)</strong>：
            <br>你的口味偏好（Query）與每道菜的特徵（Keys）比對。
            <br>例：你喜歡「辣、麻、重口味」→ 麻辣鍋得分 95，清湯得分 20。
          </li>
          <li><strong>$\div \sqrt{d_k}$ (縮放)</strong>：
            <br>分數標準化（避免分數差距過大）。
            <br>例：原本 [95, 20] → 縮放後 [11.9, 2.5]。
          </li>
          <li><strong>Softmax (機率化)</strong>：
            <br>轉換成點菜比例（總和 100%）。
            <br>例：[11.9, 2.5] → [99%, 1%]。
          </li>
          <li><strong>$\times V$ (加權提取)</strong>：
            <br>按比例「混合」不同菜色的特色。
            <br>例：99% 麻辣鍋的香辣 + 1% 清湯的清爽 = 你最終吃到的風味。
          </li>
        </ol>

        <h4>🔧 工程類比：加權資料庫查詢</h4>
        <pre><code>// 偽代碼
function attention(query, keys, values):
    // Step 1: 計算相似度
    scores = query.dot(keys.T)  // (1 x d) × (d x m) = (1 x m)
    
    // Step 2: 縮放（數值穩定性）
    scores = scores / sqrt(d_k)
    
    // Step 3: 歸一化為機率
    weights = softmax(scores)   // [0.66, 0.24, 0.10]
    
    // Step 4: 加權提取
    output = weights.dot(values)  // (1 x m) × (m x d_v) = (1 x d_v)
    return output

// 複雜度分析
// QK^T: O(n × m × d_k) 
// Softmax: O(n × m)
// 總計：O(n × m × d_k) ≈ O(n²) 當 n=m 時</code></pre>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <!-- ============= Multi-Head Attention ============= -->
      <h3>3. Multi-Head Attention（多頭注意力）</h3>

      <div class="text-pair">
        <div class="original-text">
          Instead of performing a single attention function with $d_{model}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively.
        </div>
        <div class="translation">
          <p>
            我們發現，與其使用 $d_{model}$ 維度的鍵、值和查詢執行單一的注意力函數，不如將查詢、鍵和值<strong>線性投影</strong> $h$ 次，使用不同的、學習到的線性投影，分別投影到 $d_k$、$d_k$ 和 $d_v$ 維度。
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>🎯 Multi-Head 的核心思想</h4>
        <p><strong>問題：</strong>單一 Attention 會「平均化」，細節模糊（回顧第 2 頁：降低有效解析度）。</p>
        <p><strong>解決方案：</strong>用 $h$ 個不同的「視角」(heads) 同時看，每個 head 專注於不同的語義關係。</p>
        
        <table>
          <tr>
            <th>單頭 Attention</th>
            <th>多頭 Attention (h=8)</th>
          </tr>
          <tr>
            <td>1 個視角<br>看到「平均化」的關係</td>
            <td>8 個平行視角<br>每個 head 專注不同特徵</td>
          </tr>
          <tr>
            <td>$d_{model} = 512$ 維的 Q, K, V</td>
            <td>每個 head: $d_k = d_v = 64$ 維<br>$(512 \div 8 = 64)$</td>
          </tr>
          <tr>
            <td>輸出：512 維</td>
            <td>8 個 head 各輸出 64 維<br>拼接後：$8 \times 64 = 512$ 維</td>
          </tr>
        </table>
      </div>

      <div class="text-pair">
        <div class="original-text">
          On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values.
          $$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$
          where $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$
        </div>
        <div class="translation">
          <p>
            在每個投影版本的查詢、鍵和值上，我們並行執行注意力函數，產生 $d_v$ 維的輸出值。這些輸出被<strong>拼接</strong>（concatenate）起來，然後再次投影，得到最終值。
          </p>
          <p style="text-align: center; font-size: 1.2em; margin: 20px 0;">
            $$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$
          </p>
          <p style="text-align: center;">
            其中 $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$
          </p>
        </div>
      </div>

      <div class="explanation">
        <h4>🔬 Multi-Head Attention 完整流程</h4>
        
        <p><strong>步驟 1：線性投影（Linear Projections）</strong></p>
        <p>將原始的 $Q, K, V$ 分別投影成 $h$ 個不同版本：</p>
        <pre><code>for i in 1 to h:
    Q_i = Q × W_i^Q   // (n × 512) × (512 × 64) = (n × 64)
    K_i = K × W_i^K   // (m × 512) × (512 × 64) = (m × 64)
    V_i = V × W_i^V   // (m × 512) × (512 × 64) = (m × 64)</code></pre>
        <p><strong>關鍵：</strong>每個 head 的 $W_i^Q, W_i^K, W_i^V$ 都是<strong>不同的、可學習的</strong>參數。</p>

        <p><strong>步驟 2：並行計算 Attention</strong></p>
        <p>對每個 head 的 $(Q_i, K_i, V_i)$ 執行 Scaled Dot-Product Attention：</p>
        <pre><code>head_1 = Attention(Q_1, K_1, V_1)  // 輸出 (n × 64)
head_2 = Attention(Q_2, K_2, V_2)  // 輸出 (n × 64)
...
head_8 = Attention(Q_8, K_8, V_8)  // 輸出 (n × 64)</code></pre>
        <p><strong>重要：</strong>這 8 個 Attention 可以<strong>完全並行計算</strong>（GPU 友好）。</p>

        <p><strong>步驟 3：拼接（Concatenate）</strong></p>
        <pre><code>concat_output = [head_1 | head_2 | ... | head_8]
// (n × 64) + (n × 64) + ... + (n × 64) = (n × 512)</code></pre>

        <p><strong>步驟 4：最終投影</strong></p>
        <pre><code>output = concat_output × W^O  // (n × 512) × (512 × 512) = (n × 512)</code></pre>
        <p><strong>目的：</strong>讓模型學習如何「混合」這 8 個 heads 的資訊。</p>
      </div>

      <div class="analogy">
        <h4>🔧 雙重類比：Multi-Head Attention</h4>
        
        <h4>🤖 AI 體驗類比：ChatGPT 的多角度思考</h4>
        <p><strong>情境：</strong>當 ChatGPT 回答你的問題時，它不是只有「單一思維」，而是同時考慮多個面向：</p>
        <table>
          <tr>
            <th>Head (思維模式)</th>
            <th>關注點</th>
            <th>功能</th>
          </tr>
          <tr>
            <td><strong>Head 1 (語法腦)</strong></td>
            <td>主詞與動詞的搭配</td>
            <td>確保語句通順，沒有語病。</td>
          </tr>
          <tr>
            <td><strong>Head 2 (上下文腦)</strong></td>
            <td>代名詞 "it" 指代什麼</td>
            <td>確保理解你說的「它」是指前面的「蘋果」而不是「公司」。</td>
          </tr>
          <tr>
            <td><strong>Head 3 (風格腦)</strong></td>
            <td>語氣與風格</td>
            <td>判斷你是在開玩笑還是在問嚴肅問題，調整回答語氣。</td>
          </tr>
        </table>
        <p><strong>結果：</strong>ChatGPT 將這 8 個腦袋（Heads）的思考結果<strong>結合起來</strong>，產生一個既通順、又合邏輯、語氣又恰當的回答。</p>

        <h4>💡 生活類比：專家委員會評估</h4>
        <p><strong>情境：</strong>你要選擇一部電影來看，找了 8 位不同領域的專家給建議。</p>
        <table>
          <tr>
            <th>專家（Head）</th>
            <th>關注重點</th>
            <th>輸出</th>
          </tr>
          <tr>
            <td>Head 1：劇情專家</td>
            <td>故事邏輯、劇情轉折</td>
            <td>「劇情 8/10」</td>
          </tr>
          <tr>
            <td>Head 2：視覺專家</td>
            <td>攝影、特效、美術</td>
            <td>「視覺 9/10」</td>
          </tr>
          <tr>
            <td>Head 3：音樂專家</td>
            <td>配樂、音效設計</td>
            <td>「音樂 7/10」</td>
          </tr>
          <tr>
            <td>...</td>
            <td>...</td>
            <td>...</td>
          </tr>
        </table>
        <p><strong>最後：</strong>你綜合這 8 位專家的意見（Concat + 線性投影），做出最終決策。</p>
        <p><strong>關鍵洞察：</strong>每位專家用不同的「評估標準」($W_i^Q, W_i^K, W_i^V$)，看到不同的面向。</p>

        <h4>🔧 工程類比：Ensemble Learning（集成學習）</h4>
        <pre><code>// Multi-Head = 8 個不同的弱分類器並行
models = []
for i in 1 to 8:
    model_i = train_classifier(data, random_features_subset_i)
    models.append(model_i)

// 預測時：每個模型給出自己的預測
predictions = []
for model in models:
    pred = model.predict(input)
    predictions.append(pred)

// 最後：加權組合所有預測（類比 Concat + W^O）
final_output = weighted_average(predictions, learned_weights)

// 類比對應
// - 每個 model = 一個 Attention head
// - random_features_subset = W_i^Q, W_i^K, W_i^V（不同的投影）
// - weighted_average = Concat + W^O</code></pre>
      </div>

      <div class="key-concept">
        <h4>📊 Transformer 的 Multi-Head 配置</h4>
        <table>
          <tr>
            <th>參數</th>
            <th>Transformer Base</th>
            <th>Transformer Big</th>
          </tr>
          <tr>
            <td>Head 數量 $h$</td>
            <td>8</td>
            <td>16</td>
          </tr>
          <tr>
            <td>模型維度 $d_{model}$</td>
            <td>512</td>
            <td>1024</td>
          </tr>
          <tr>
            <td>每個 head 的維度 $d_k = d_v$</td>
            <td>64 $(512 \div 8)$</td>
            <td>64 $(1024 \div 16)$</td>
          </tr>
          <tr>
            <td>總參數量（單層 Multi-Head）</td>
            <td>$512 \times 512 \times 4 \approx 1M$</td>
            <td>$1024 \times 1024 \times 4 \approx 4M$</td>
          </tr>
        </table>
        <p><strong>設計哲學：</strong>保持每個 head 的維度較小 ($d_k = 64$)，增加 heads 數量，比單一大維度 Attention 更有效。</p>
      </div>

      <div class="solution">
        <h4>✅ Multi-Head 解決了什麼問題？</h4>
        <ol>
          <li><strong>避免平均化模糊</strong>：
            <br>單頭 Attention 會把所有資訊混在一起。
            <br>多頭讓不同的語義關係保持分離（如語法 vs 語義）。
          </li>
          <li><strong>增強表達能力</strong>：
            <br>8 個 heads 可以學習 8 種不同的「注意力模式」。
            <br>例：Head 1 關注主詞-動詞，Head 2 關注修飾語-名詞。
          </li>
          <li><strong>計算效率</strong>：
            <br>雖然有 8 個 heads，但每個 head 的維度只有原本的 1/8。
            <br>總計算量與單頭相當：$8 \times (64^2) \approx 512^2$。
          </li>
        </ol>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <!-- ============= Applications of Attention ============= -->
      <h3>4. Attention 在模型中的三種應用 (Applications)</h3>

      <div class="text-pair">
        <div class="original-text">
          The Transformer uses multi-head attention in three different ways:
          <ul>
            <li>In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.</li>
            <li>The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place.</li>
            <li>Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.</li>
          </ul>
        </div>
        <div class="translation">
          <p>Transformer 以三種不同的方式使用多頭注意力機制：</p>
          <ul>
            <li>在<strong>「編碼器-解碼器注意力」</strong>層中，查詢來自前一個解碼器層，而記憶鍵和值來自編碼器的輸出。</li>
            <li>編碼器包含<strong>自注意力</strong>層。在自注意力層中，所有的鍵、值和查詢都來自同一個地方（編碼器的前一層輸出）。</li>
            <li>同樣地，解碼器中的自注意力層允許解碼器中的每個位置關注解碼器中直到並包括該位置的所有位置。我們需要防止解碼器中的向左信息流，以保持自回歸屬性。</li>
          </ul>
        </div>
      </div>

      <div class="explanation">
        <h4>🏗️ 三種 Attention 機制的角色分工</h4>
        
        <table>
          <tr>
            <th>類型</th>
            <th>Q 來源</th>
            <th>K, V 來源</th>
            <th>功能描述</th>
            <th>AI / 生活類比</th>
          </tr>
          <tr>
            <td><strong>Encoder Self-Attention</strong></td>
            <td>Encoder</td>
            <td>Encoder</td>
            <td><strong>理解上下文</strong><br>每個詞都看整句話的其他詞</td>
            <td><strong>閱讀理解</strong>：<br>ChatGPT 讀你的問題時，搞懂「它」是指前面的哪個人名。</td>
          </tr>
          <tr>
            <td><strong>Masked Decoder Self-Attention</strong></td>
            <td>Decoder</td>
            <td>Decoder</td>
            <td><strong>生成過程的回顧</strong><br>只能看已生成的詞，不能偷看後面</td>
            <td><strong>逐字生成</strong>：<br>像 ChatGPT 打字一樣，它寫下一個字時，只能看到已經寫出來的字，不能預知未來要寫什麼。</td>
          </tr>
          <tr>
            <td><strong>Encoder-Decoder Attention</strong></td>
            <td>Decoder</td>
            <td>Encoder</td>
            <td><strong>跨語言對齊</strong><br>生成譯文時，回頭查閱原文</td>
            <td><strong>翻譯查閱</strong>：<br>ChatGPT 在翻譯時，寫到「貓」這個字，會回頭「瞄一眼」英文原文的 "cat"。</td>
          </tr>
        </table>
      </div>

      <div class="key-concept">
        <h4>🚫 什麼是 Masking（遮罩）？</h4>
        <p><strong>問題：</strong>訓練時我們有完整的正確答案（Ground Truth），如果不擋住，Decoder 會直接「偷看」到下一個字，學不到預測能力。</p>
        <p><strong>解決方案：</strong>將矩陣中代表「未來」的位置設為 $-\infty$。</p>
        <ul>
            <li>Softmax($-\infty$) = 0</li>
            <li>結果：模型對未來的注意力權重為 0（完全看不到）。</li>
        </ul>
        <pre><code>[ 1, 0, 0 ]  <- 第一個字只能看自己
[ 1, 1, 0 ]  <- 第二個字能看前兩個
[ 1, 1, 1 ]  <- 第三個字能看前三個
(右上角被 Mask 掉)</code></pre>
      </div>

      <div class="nav-bar">
        <a href="02-background.html" class="nav-btn">
          ← 上一頁：Background
        </a>
        <a href="03-2-model-architecture-components.html" class="nav-btn primary">
          下一頁：Positional Encoding →
        </a>
      </div>

      <div
        style="
          margin-top: 40px;
          padding: 20px;
          background: #ecf0f1;
          border-radius: 5px;
          text-align: center;
        "
      >
        <p><strong>📚 學習檢查點</strong></p>
        <p>在進入下一頁之前，請確保你理解：</p>
        <ul style="text-align: left; max-width: 600px; margin: 20px auto">
          <li>✅ Encoder 負責「理解」，Decoder 負責「生成」</li>
          <li>✅ $QK^T$ 計算相似度，Softmax 轉成機率，$\times V$ 加權提取</li>
          <li>✅ $\div \sqrt{d_k}$ 是為了數值穩定性（防止梯度消失）</li>
          <li>✅ 整個過程就像「根據口味偏好，加權混合菜色」</li>
        </ul>
      </div>
    </div>
  </body>
</html>
