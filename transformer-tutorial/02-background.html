<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Transformer è«–æ–‡æ·±åº¦è§£æ - ç¬¬2é ï¼šBackground</title>
    <link rel="stylesheet" href="../styles/global.css" />
    <link rel="stylesheet" href="../styles/paper-reading.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=JetBrains+Mono:wght@400;700&family=Merriweather:ital,wght@0,400;0,700;1,400&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <div class="container">
      <h1>
        ğŸ“– Attention Is All You Need <br /><span
          style="
            font-size: 0.6em;
            color: var(--text-secondary);
            font-weight: normal;
          "
          >æ·±åº¦è§£æç³»åˆ—</span
        >
      </h1>
      <h2>ç¬¬ 2 é ï¼šBackgroundï¼ˆèƒŒæ™¯ï¼‰</h2>

      <div class="header-section">
        <div class="paper-meta">
          <div class="meta-item">
            <strong>ç›®å‰ç« ç¯€</strong>
            2. Background
          </div>
          <div class="meta-item">
            <strong>æ ¸å¿ƒä¸»é¡Œ</strong>
            CNN é™åˆ¶ã€è¨ˆç®—è¤‡é›œåº¦ã€Self-Attention å®šç¾©
          </div>
          <div class="meta-item">
            <strong>é–±è®€é›£åº¦</strong>
            â­â­â­ (æ¶‰åŠå·ç©é‹ç®—æ¦‚å¿µ)
          </div>
        </div>
      </div>

      <!-- ============= CNN vs Transformer ============= -->
      <h3>1. æ¸›å°‘åºåˆ—é‹ç®—çš„å˜—è©¦</h3>

      <div class="text-pair">
        <div class="original-text">
          The goal of reducing sequential computation also forms the foundation of
          the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of
          which use convolutional neural networks as basic building block,
          computing hidden representations in parallel for all input and output
          positions.
        </div>
        <div class="translation">
          <p>
            æ¸›å°‘åºåˆ—é‹ç®—çš„ç›®æ¨™ä¹Ÿæ˜¯ <strong>Extended Neural GPU</strong> [20]ã€<strong>ByteNet</strong> [15] å’Œ
            <strong>ConvS2S</strong> [8] çš„åŸºç¤ï¼Œé€™äº›æ¨¡å‹éƒ½ä½¿ç”¨<strong>å·ç©ç¥ç¶“ç¶²è·¯ï¼ˆCNNï¼‰</strong>ä½œç‚ºåŸºæœ¬æ§‹å»ºå¡Šï¼Œä¸¦è¡Œç‚ºæ‰€æœ‰è¼¸å…¥å’Œè¼¸å‡ºä½ç½®è¨ˆç®—éš±è—è¡¨ç¤ºã€‚
          </p>
        </div>
      </div>

      <div class="explanation">
        <h4>ğŸ“Š ç‚ºä»€éº¼ä½¿ç”¨ CNNï¼Ÿ</h4>
        <p>åœ¨ Transformer å‡ºç¾ä¹‹å‰ï¼Œç‚ºäº†æ”¹å–„ RNN ç„¡æ³•ä¸¦è¡Œçš„å•é¡Œï¼Œç ”ç©¶è€…è½‰å‘äº† CNNï¼š</p>
        <ul>
            <li><strong>ä¸¦è¡Œæ€§</strong>ï¼šCNN å¯ä»¥åŒæ™‚è™•ç†ä¸åŒçš„çª—å£ï¼ˆWindowï¼‰ï¼Œä¸åƒ RNN éœ€è¦ç­‰å‰ä¸€å€‹æ™‚é–“æ­¥ã€‚</li>
            <li><strong>å±€éƒ¨ç‰¹å¾µ</strong>ï¼šCNN å¾ˆæ“…é•·æ•æ‰é„°è¿‘è©å½™çš„é—œä¿‚ï¼ˆå¦‚ "New" + "York"ï¼‰ã€‚</li>
        </ul>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <h3>2. é•·è·é›¢ä¾è³´çš„æŒ‘æˆ°</h3>

      <div class="text-pair">
        <div class="original-text">
          In these models, the number of operations required to relate signals
          from two arbitrary input or output positions grows in the distance
          between positions, linearly for ConvS2S and logarithmically for
          ByteNet. This makes it more difficult to learn dependencies between
          distant positions [11].
        </div>
        <div class="translation">
          <p>
            åœ¨é€™äº›æ¨¡å‹ä¸­ï¼Œé—œè¯ä¾†è‡ªå…©å€‹ä»»æ„è¼¸å…¥æˆ–è¼¸å‡ºä½ç½®çš„ä¿¡è™Ÿæ‰€éœ€çš„æ“ä½œæ¬¡æ•¸ï¼Œæœƒéš¨è‘—ä½ç½®ä¹‹é–“çš„è·é›¢å¢é•·è€Œå¢åŠ ï¼šå°æ–¼
            ConvS2S æ˜¯<strong>ç·šæ€§å¢é•·</strong>ï¼Œå°æ–¼ ByteNet æ˜¯<strong>å°æ•¸å¢é•·</strong>ã€‚é€™ä½¿å¾—å­¸ç¿’<span class="highlight">é è·é›¢ä½ç½®ä¹‹é–“çš„ä¾è³´é—œä¿‚è®Šå¾—æ›´åŠ å›°é›£</span> [11]ã€‚
          </p>
        </div>
      </div>

      <div class="problem">
        <h4>âŒ CNN çš„è¦–é‡é™åˆ¶ï¼ˆReceptive Fieldï¼‰</h4>
        <p>CNN å°±åƒé€éä¸€å€‹å°çª—å£çœ‹ä¸–ç•Œã€‚å¦‚æœè¦çœ‹å…©å€‹è·é›¢å¾ˆé çš„è©ï¼ˆä¾‹å¦‚å¥é¦–çš„ "The" å’Œå¥å°¾çš„ "."ï¼‰ï¼ŒCNN éœ€è¦å †ç–Šå¾ˆå¤šå±¤æ‰èƒ½è®“å®ƒå€‘çš„ã€Œè¦–é‡ã€é‡ç–Šã€‚</p>
        <table>
            <tr>
                <th>æ¨¡å‹æ¶æ§‹</th>
                <th>é—œè¯è·é›¢ç‚º N çš„å…©å€‹è©æ‰€éœ€å±¤æ•¸</th>
                <th>è¤‡é›œåº¦</th>
            </tr>
            <tr>
                <td>ConvS2S (CNN)</td>
                <td>éœ€è¦ N/k å±¤ (k=kernel size)</td>
                <td>O(N) - ç·šæ€§</td>
            </tr>
            <tr>
                <td>ByteNet (Dilated CNN)</td>
                <td>éœ€è¦ log(N) å±¤</td>
                <td>O(log N) - å°æ•¸</td>
            </tr>
            <tr>
                <td><strong>Transformer</strong></td>
                <td><strong>åªéœ€è¦ 1 å±¤</strong></td>
                <td><strong>O(1) - å¸¸æ•¸</strong></td>
            </tr>
        </table>
      </div>

      <div class="analogy">
        <h4>ğŸ”§ å·¥ç¨‹é¡æ¯”ï¼šå‚³éè¨Šæ¯çš„æˆæœ¬</h4>
        <p><strong>æƒ…å¢ƒï¼šå…¬å¸è£¡çš„è¨Šæ¯å‚³é</strong></p>
        <ul>
            <li>
                <strong>RNN (åºåˆ—)</strong>ï¼š
                <br>A å‚³çµ¦ Bï¼ŒB å‚³çµ¦ Cï¼ŒC å‚³çµ¦ D... æœ€å¾Œå‚³çµ¦ Zã€‚
                <br>âŒ æ…¢ï¼Œä¸”å‚³åˆ° Z æ™‚è¨Šæ¯å¯èƒ½å·²ç¶“å¤±çœŸã€‚
            </li>
            <li>
                <strong>CNN (å±¤ç´šçµæ§‹)</strong>ï¼š
                <br>åŸºå±¤å“¡å·¥å‚³çµ¦çµ„é•·ï¼Œçµ„é•·å‚³çµ¦ç¶“ç†ï¼Œç¶“ç†å‚³çµ¦ç¸½ç›£...
                <br>âŒ è·é›¢è¶Šé ï¼ˆè·¨éƒ¨é–€ï¼‰ï¼Œéœ€è¦å‘ä¸ŠåŒ¯å ±çš„å±¤ç´šè¶Šé«˜ï¼Œè·¯å¾‘è¶Šé•·ã€‚
            </li>
            <li>
                <strong>Transformer (Attention)</strong>ï¼š
                <br>æ‰€æœ‰äººåœ¨åŒä¸€å€‹ç¾¤çµ„èŠå¤©å®¤ (Slack/Discord)ã€‚
                <br>âœ… A å¯ä»¥ç›´æ¥ @Zï¼Œç„¡è«– Z åœ¨å“ªè£¡ï¼Œæˆæœ¬éƒ½æ˜¯ä¸€æ¨£çš„ (O(1))ã€‚
            </li>
        </ul>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <h3>3. Transformer çš„æ¬Šè¡¡èˆ‡è§£æ±ºæ–¹æ¡ˆ</h3>

      <div class="text-pair">
        <div class="original-text">
          In the Transformer this is reduced to a constant number of operations,
          albeit at the cost of reduced effective resolution due to averaging
          attention-weighted positions, an effect we counteract with Multi-Head
          Attention.
        </div>
        <div class="translation">
          <p>
            åœ¨ Transformer ä¸­ï¼Œé€™è¢«æ¸›å°‘åˆ°äº†<strong>å¸¸æ•¸æ¬¡æ“ä½œ</strong>ï¼Œå„˜ç®¡é€™æ˜¯ä»¥<span class="highlight">é™ä½æœ‰æ•ˆè§£æåº¦ï¼ˆeffective resolutionï¼‰</span>ç‚ºä»£åƒ¹çš„ï¼Œå› ç‚ºæˆ‘å€‘æœƒå°æ³¨æ„åŠ›åŠ æ¬Šä½ç½®é€²è¡Œå¹³å‡ã€‚æˆ‘å€‘é€é<strong>å¤šé ­æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆMulti-Head Attentionï¼‰</strong>ä¾†æŠµæ¶ˆé€™ç¨®å½±éŸ¿ã€‚
          </p>
        </div>
      </div>

      <div class="analogy">
        <h4>ğŸ¤– AI é«”é©—é€£çµï¼šChatGPT çš„ã€Œé•·è¨˜æ†¶ã€å„ªå‹¢</h4>
        <p><strong>å°æ¯”èˆŠæ™‚ä»£ Chatbot (RNN)ï¼š</strong></p>
        <ul>
            <li>èŠäº† 10 å¥ä¹‹å¾Œï¼Œå¿˜è¨˜ä½ ä¸€é–‹å§‹èªªçš„å…§å®¹ï¼ˆè·¯å¾‘å¤ªé•·ï¼Œè¨Šè™Ÿè¡°æ¸›ï¼‰</li>
        </ul>
        <p><strong>Transformer (ChatGPT)ï¼š</strong></p>
        <ul>
            <li>å¯ä»¥è²¼ä¸Šä¸€æ•´ç¯‡æ–‡ç« ï¼Œå•å®ƒç¬¬ä¸€æ®µçš„ç´°ç¯€ï¼Œå®ƒéƒ½èƒ½å›ç­”</li>
            <li>åŸå› ï¼šO(1) å¸¸æ•¸æ“ä½œï¼Œã€Œéš¨æ™‚ã€éƒ½èƒ½å›é ­çœ‹é–‹é ­</li>
        </ul>
        <p><strong>é€™å°±æ˜¯ã€Œé•·è·é›¢ä¾è³´ã€è™•ç†èƒ½åŠ›çš„å·®ç•°ï¼</strong></p>
      </div>

      <div class="explanation">
        <h4>ğŸ’¡ ä»€éº¼æ˜¯ã€Œé™ä½æœ‰æ•ˆè§£æåº¦ã€ï¼Ÿ</h4>
        <p>Attention æ©Ÿåˆ¶æœ¬è³ªä¸Šæ˜¯å°‡æ‰€æœ‰è¼¸å…¥<strong>å‘é‡</strong>é€²è¡Œ<strong>åŠ æ¬Šå¹³å‡ (Weighted Average)</strong>ã€‚</p>
        
        <h5>ğŸ§® å‘é‡çš„åŠ æ¬Šå¹³å‡ï¼ˆæ•¸å­¸ç›´è¦ºï¼‰</h5>
        <p>å‡è¨­æœ‰ä¸‰å€‹å­—çš„å‘é‡ï¼ˆç°¡åŒ–ç‚º 3 ç¶­ï¼‰ï¼š</p>
        <pre><code>ã€ŒTheã€  = [1.0, 0.2, 0.3]
ã€Œcatã€  = [0.1, 0.9, 0.4]
ã€Œsleepsã€= [0.2, 0.3, 1.0]

# Attention æ¬Šé‡ (ç›¸é—œåº¦åˆ†æ•¸)
weights = [0.1, 0.7, 0.2]  # ã€Œcatã€æœ€ç›¸é—œ

# åŠ æ¬Šå¹³å‡ (æ··åˆå‘é‡)
output = 0.1Ã—[1.0,0.2,0.3] + 0.7Ã—[0.1,0.9,0.4] + 0.2Ã—[0.2,0.3,1.0]
       = [0.24, 0.71, 0.51]  # æ–°çš„æ··åˆå‘é‡</code></pre>
        
        <ul>
            <li><strong>å¹³å‡åŒ–çš„å‰¯ä½œç”¨</strong>ï¼šç•¶ä½ æŠŠå¤šå€‹å‘é‡æ··åˆåœ¨ä¸€èµ·æ™‚ï¼ŒåŸæœ¬å„è‡ªæ¸…æ™°çš„ç‰¹å¾µå¯èƒ½æœƒè®Šå¾—æ¨¡ç³Šã€‚</li>
            <li><strong>ä¾‹å­</strong>ï¼šæŠŠä¸‰ç¨®é¡è‰² (RGBå‘é‡) æ··åˆ â†’ è®Šæˆç°è‰²ï¼ˆç´°ç¯€ä¸Ÿå¤±ï¼‰</li>
        </ul>
        
        <h4>âœ… è§£æ±ºæ–¹æ¡ˆï¼šMulti-Head Attention</h4>
        <p>æ—¢ç„¶ä¸€å€‹è¦–è§’æœƒæ¨¡ç³Šï¼Œé‚£å°±ç”¨<strong>å¤šå€‹è¦–è§’ï¼ˆHeadsï¼‰</strong>åŒæ™‚çœ‹ï¼</p>
        <ul>
            <li>Head 1 å°ˆæ³¨æ–¼ï¼š<strong>èªæ³•çµæ§‹</strong>ï¼ˆä¸»è©-å‹•è©é—œä¿‚ï¼‰</li>
            <li>Head 2 å°ˆæ³¨æ–¼ï¼š<strong>æŒ‡ä»£é—œä¿‚</strong>ï¼ˆit æŒ‡ä»£ä»€éº¼ï¼‰</li>
            <li>Head 3 å°ˆæ³¨æ–¼ï¼š<strong>æ™‚æ…‹ä¿¡æ¯</strong></li>
        </ul>
        <p>æœ€å¾Œå†æŠŠé€™äº›æ¸…æ™°çš„å±€éƒ¨è¦–è§’æ‹¼æ¥èµ·ä¾†ï¼Œæ¢å¾©é«˜è§£æåº¦çš„ç†è§£ã€‚</p>
        <p><em>ğŸ’¡ è©³ç´°æ©Ÿåˆ¶å°‡åœ¨ç¬¬ 3 ç«  Section 3.2 è©³ç´°èªªæ˜ï¼ˆåŒ…å«å®Œæ•´æ•¸å­¸å…¬å¼ï¼‰ã€‚</em></p>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <h3>4. Self-Attention èˆ‡ Memory Networks</h3>

      <div class="text-pair">
        <div class="original-text">
          Self-attention, sometimes called intra-attention is an attention
          mechanism relating different positions of a single sequence in order
          to compute a representation of the sequence.
        </div>
        <div class="translation">
          <p>
            <strong>è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆSelf-attentionï¼‰</strong>ï¼Œæœ‰æ™‚ç¨±ç‚ºå…§éƒ¨æ³¨æ„åŠ›ï¼ˆintra-attentionï¼‰ï¼Œæ˜¯ä¸€ç¨®é—œè¯<strong>å–®å€‹åºåˆ—</strong>ä¸­ä¸åŒä½ç½®ä»¥è¨ˆç®—è©²åºåˆ—è¡¨ç¤ºçš„æ³¨æ„åŠ›æ©Ÿåˆ¶ã€‚
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>ğŸ¯ é—œéµå®šç¾©ï¼šSelf-Attention</h4>
        <p><strong>Attention vs Self-Attention:</strong></p>
        <ul>
            <li><strong>Attention (ä¸€èˆ¬)</strong>ï¼šSource (Encoder) å° Target (Decoder) çš„é—œæ³¨ã€‚
                <br><em>ä¾‹å­ï¼šç¿»è­¯æ™‚ï¼Œçœ‹è‘—è‹±æ–‡åŸæ–‡ï¼ˆSourceï¼‰å¯«ä¸­æ–‡ï¼ˆTargetï¼‰ã€‚</em>
            </li>
            <li><strong>Self-Attention (è‡ªæ³¨æ„åŠ›)</strong>ï¼šSource å° Source è‡ªå·±å…§éƒ¨çš„é—œæ³¨ã€‚
                <br><em>ä¾‹å­ï¼šé–±è®€è‹±æ–‡å¥å­æ™‚ï¼Œç†è§£å–®å­—ä¹‹é–“çš„é—œä¿‚ï¼ˆå¦‚ "The bank" çš„ bank æ˜¯æŒ‡éŠ€è¡Œé‚„æ˜¯æ²³å²¸ï¼Œå–æ±ºæ–¼åŒå¥å­è£¡çš„å…¶ä»–è©ï¼‰ã€‚</em>
            </li>
        </ul>
      </div>

      <div class="text-pair">
        <div class="original-text">
          Self-attention has been used successfully in a variety of tasks
          including reading comprehension, abstractive summarization, textual
          entailment and learning task-independent sentence representations [4,
          27, 28, 22].
        </div>
        <div class="translation">
          <p>
            è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶å·²ç¶“æˆåŠŸæ‡‰ç”¨æ–¼å¤šç¨®ä»»å‹™ï¼ŒåŒ…æ‹¬<strong>é–±è®€ç†è§£</strong>ï¼ˆreading comprehensionï¼‰ã€<strong>æŠ½è±¡æ‘˜è¦ç”Ÿæˆ</strong>ï¼ˆabstractive summarizationï¼‰ã€<strong>æ–‡æœ¬è˜Šå«</strong>ï¼ˆtextual entailmentï¼‰ä»¥åŠ<strong>å­¸ç¿’èˆ‡ä»»å‹™ç„¡é—œçš„å¥å­è¡¨ç¤º</strong> [4, 27, 28, 22]ã€‚
          </p>
        </div>
      </div>

      <div class="explanation">
        <h4>ğŸ“Š Self-Attention çš„éå¾€æˆ°ç¸¾</h4>
        <p>åœ¨ Transformer å‡ºç¾ä¹‹å‰ï¼ŒSelf-Attention å·²ç¶“åœ¨é€™äº›ä»»å‹™ä¸­å±•ç¾å¯¦åŠ›ï¼š</p>
        <table>
            <tr>
                <th>ä»»å‹™</th>
                <th>æè¿°</th>
                <th>Self-Attention çš„ä½œç”¨</th>
            </tr>
            <tr>
                <td>é–±è®€ç†è§£</td>
                <td>è®€å®Œæ–‡ç« å¾Œå›ç­”å•é¡Œ</td>
                <td>æ‰¾åˆ°å•é¡Œé—œéµè©èˆ‡æ–‡ç« æ®µè½çš„å°æ‡‰é—œä¿‚</td>
            </tr>
            <tr>
                <td>æŠ½è±¡æ‘˜è¦ç”Ÿæˆ</td>
                <td>ç”¨è‡ªå·±çš„è©±ç¸½çµé•·æ–‡</td>
                <td>è­˜åˆ¥æ–‡ç« ä¸­æœ€é‡è¦çš„å¥å­èˆ‡æ¦‚å¿µ</td>
            </tr>
            <tr>
                <td>æ–‡æœ¬è˜Šå«</td>
                <td>åˆ¤æ–·å¥å­ A æ˜¯å¦æš—ç¤ºå¥å­ B</td>
                <td>æ¯”å°å…©å€‹å¥å­çš„èªç¾©é‡ç–Šèˆ‡é‚è¼¯é—œä¿‚</td>
            </tr>
        </table>
        <p><strong>é—œéµæ´å¯Ÿ</strong>ï¼šSelf-Attention ä¸æ˜¯ Transformer ç™¼æ˜çš„ï¼Œä½† Transformer <strong>é¦–æ¬¡è®“å®ƒæˆç‚ºå”¯ä¸€çš„ä¸»è§’</strong>ã€‚</p>
      </div>

      <div class="text-pair">
        <div class="original-text">
          End-to-end memory networks are based on a recurrent attention
          mechanism instead of sequence-aligned recurrence and have been shown
          to perform well on simple-language question answering and language
          modeling tasks.
        </div>
        <div class="translation">
          <p>
            <strong>ç«¯åˆ°ç«¯è¨˜æ†¶ç¶²è·¯ï¼ˆEnd-to-end memory networksï¼‰</strong>åŸºæ–¼éè¿´æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œè€Œä¸æ˜¯åºåˆ—å°é½Šçš„éè¿´ï¼Œä¸¦ä¸”å·²è¢«è­‰æ˜åœ¨ç°¡å–®èªè¨€å•ç­”å’Œèªè¨€å»ºæ¨¡ä»»å‹™ä¸Šè¡¨ç¾è‰¯å¥½ã€‚
          </p>
        </div>
      </div>

      <div class="explanation">
        <h4>ğŸ“š å­¸è¡“æ·µæº</h4>
        <p>é€™ä¸€æåˆ° Memory Networks æ˜¯ç‚ºäº†è‡´æ•¬å‰äººçš„ç ”ç©¶ï¼Œèªªæ˜ Transformer çš„æ€æƒ³ä¸¦éæ†‘ç©ºè€Œä¾†ï¼Œè€Œæ˜¯å»ºç«‹åœ¨ Memory Network å°‡ã€Œè¨˜æ†¶ï¼ˆå…¨åŸŸè³‡è¨Šï¼‰ã€å­˜å–èˆ‡ã€Œè¨ˆç®—ã€åˆ†é›¢çš„åŸºç¤ä¸Šã€‚</p>
      </div>

      <hr style="border: 0; border-top: 1px dashed #ddd; margin: 40px 0" />

      <h3>5. Transformer çš„æ­·å²å®šä½</h3>

      <div class="text-pair">
        <div class="original-text">
          To the best of our knowledge, however, the Transformer is the first
          transduction model relying entirely on self-attention to compute
          representations of its input and output without using sequence-aligned
          RNNs or convolution. In the following sections, we will describe the
          Transformer, motivate self-attention and discuss its advantages over
          models such as [17, 18] and [9].
        </div>
        <div class="translation">
          <p>
            ç„¶è€Œï¼Œ<span class="highlight">æ“šæˆ‘å€‘æ‰€çŸ¥ï¼ŒTransformer æ˜¯ç¬¬ä¸€å€‹å®Œå…¨ä¾è³´è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶</span>ä¾†è¨ˆç®—è¼¸å…¥å’Œè¼¸å‡ºè¡¨ç¤ºçš„<strong>åºåˆ—è½‰æ›æ¨¡å‹</strong>ï¼Œ<strong>ä¸ä½¿ç”¨åºåˆ—å°é½Šçš„ RNN æˆ–å·ç©</strong>ã€‚åœ¨æ¥ä¸‹ä¾†çš„ç« ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡æè¿° Transformerã€è§£é‡‹è‡ªæ³¨æ„åŠ›çš„å‹•æ©Ÿï¼Œä¸¦è¨è«–å®ƒç›¸å°æ–¼ [17, 18] å’Œ [9] ç­‰æ¨¡å‹çš„å„ªå‹¢ã€‚
          </p>
        </div>
      </div>

      <div class="key-concept">
        <h4>ğŸ† Transformer çš„æ ¸å¿ƒè²¢ç»é™³è¿°</h4>
        <p><strong>é€™å¥è©±æ˜¯æ•´å€‹ Background ç« ç¯€çš„é«˜æ½®ï¼</strong></p>
        <ul>
            <li>âœ… <strong>ç¬¬ä¸€å€‹ (First)</strong>ï¼šå‰ç„¡å¤äººçš„å‰µæ–°ã€‚</li>
            <li>âœ… <strong>å®Œå…¨ä¾è³´ Self-Attention (Relying Entirely)</strong>ï¼šä¸æ˜¯ã€Œæ··åˆä½¿ç”¨ã€ï¼Œè€Œæ˜¯ã€Œç´”ç²¹ã€ã€‚</li>
            <li>âœ… <strong>ä¸ç”¨ RNN æˆ–å·ç© (Without RNN or Convolution)</strong>ï¼šæ–¬æ–·èˆŠä¸–ç•Œçš„é–éˆã€‚</li>
        </ul>
        
        <h4>ğŸ”® ä½œè€…çš„æ‰¿è«¾ï¼ˆRoadmapï¼‰</h4>
        <p>è«–æ–‡æ¥ä¸‹ä¾†å°‡åšä¸‰ä»¶äº‹ï¼š</p>
        <ol>
            <li><strong>æè¿° Transformer</strong>ï¼šç¬¬ 3 ç« çš„æ¶æ§‹åœ–èˆ‡æ•¸å­¸å…¬å¼ã€‚</li>
            <li><strong>è§£é‡‹å‹•æ©Ÿ</strong>ï¼šç‚ºä»€éº¼è¦ç”¨ Self-Attentionï¼Ÿï¼ˆç¬¬ 4 ç« ï¼‰</li>
            <li><strong>æ¯”è¼ƒå„ªå‹¢</strong>ï¼šèˆ‡ ByteNet/ConvS2S çš„é‡åŒ–å°æ¯”ï¼ˆè¡¨æ ¼ï¼‰ã€‚</li>
        </ol>
      </div>

      <div class="solution">
        <h4>ğŸš€ ç¬¬äºŒé ç¸½çµï¼šTransformer çš„æˆ°ç•¥åœ°ä½</h4>
        <p>æœ¬é ç¢ºç«‹äº† Transformer åœ¨ç¥ç¶“ç¶²è·¯æ¼”åŒ–å²ä¸Šçš„ä½ç½®ï¼š</p>
        <ol>
            <li><strong>RNN</strong>ï¼šåºåˆ—è™•ç†ï¼Œç„¡æ³•ä¸¦è¡Œ (Sequential)ã€‚</li>
            <li><strong>CNN (ByteNet/ConvS2S)</strong>ï¼šå˜—è©¦ä¸¦è¡Œï¼Œä½†é•·è·é›¢ä¾è³´è™•ç†å›°é›£ (Hierarchical)ã€‚</li>
            <li><strong>Transformer</strong>ï¼šå®Œå…¨ä¸¦è¡Œ + O(1) é•·è·é›¢ä¾è³´ï¼Œä¸¦ç”¨ Multi-Head è§£æ±ºè§£æåº¦å•é¡Œã€‚</li>
        </ol>
      </div>

      <div class="nav-bar">
        <a href="01-abstract-and-introduction.html" class="nav-btn">
            â† ä¸Šä¸€é ï¼šAbstract
        </a>
        <a href="03-1-model-architecture-attention.html" class="nav-btn primary">
            ä¸‹ä¸€é ï¼šModel Architecture â†’
        </a>
      </div>

      <div
        style="
          margin-top: 40px;
          padding: 20px;
          background: #ecf0f1;
          border-radius: 5px;
          text-align: center;
        "
      >
        <p><strong>ğŸ“š å­¸ç¿’å»ºè­°</strong></p>
        <p>ä¸‹ä¸€ç« æ˜¯æ•´ç¯‡è«–æ–‡æœ€æ ¸å¿ƒã€æœ€å›°é›£çš„éƒ¨åˆ†ï¼š<strong>æ¨¡å‹æ¶æ§‹</strong>ã€‚</p>
        <p>åœ¨é€²å…¥ä¸‹ä¸€é ä¹‹å‰ï¼Œè«‹ç¢ºä¿ä½ è…¦ä¸­æœ‰é€™å€‹ç•«é¢ï¼š</p>
        <ul style="text-align: left; max-width: 600px; margin: 20px auto">
          <li>âœ… CNN åƒé‡‘å­—å¡”ï¼Œéœ€è¦ç–Šå¾ˆé«˜æ‰èƒ½çœ‹åˆ°é è™•ã€‚</li>
          <li>âœ… Transformer åƒå…¨é«”è¦–è¨Šæœƒè­°ï¼Œæ¯å€‹äººéƒ½èƒ½ç›´æ¥å°è©±ã€‚</li>
          <li>âœ… Multi-Head å°±åƒå¤šå€‹å°ˆå®¶å¾ä¸åŒè§’åº¦å¯©è¦–åŒä¸€ä»½æ–‡ä»¶ã€‚</li>
        </ul>
      </div>
    </div>
  </body>
</html>
