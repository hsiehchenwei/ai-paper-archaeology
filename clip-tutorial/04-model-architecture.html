<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CLIP 第 4 章：模型架構與擴展 - 雙編碼器設計</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="hero-section" style="background-image: url('images/chapter04_hero.png'); height: 80vh;">
        <div class="hero-overlay"></div>
        <div class="hero-content">
            <h1>模型架構與擴展</h1>
            <p class="hero-subtitle">ResNet vs Vision Transformer · 從 50M 到 400M 參數</p>
            <p class="hero-meta">CLIP 深度解析 · 第 4 章</p>
        </div>
    </div>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">🏠 首頁</a>
            <span>/</span>
            <a href="index.html">CLIP 教學</a>
            <span>/</span>
            <span class="current">第 4 章</span>
        </div>

        <div class="story-container">
            <p class="story-lead drop-cap">
                CLIP 採用<strong>雙編碼器架構</strong>：一個圖像編碼器和一個文字編碼器。
                本章將深入探討這兩種編碼器的設計選擇，以及如何從小模型擴展到大模型。
            </p>

            <div class="section-divider"><span>✦</span></div>

            <p>
                CLIP 的架構設計遵循<strong>簡潔與效率</strong>的原則：
            </p>
            <ul>
                <li><strong>圖像編碼器</strong>：ResNet 或 Vision Transformer (ViT)</li>
                <li><strong>文字編碼器</strong>：Transformer（類似 GPT-2）</li>
                <li><strong>投影層</strong>：簡單的線性投影到共同嵌入空間</li>
                <li><strong>擴展策略</strong>：同時增加寬度、深度和解析度</li>
            </ul>
        </div>

        <h2>🖼️ 圖像編碼器：ResNet vs ViT</h2>

        <div class="paper-section">
            <div class="original-quote">
                <strong>原文</strong>
                <p>
                    We consider two different architectures for the image encoder. For the first, we use ResNet-50 as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNet-D improvements and the antialiased rect-2 blur pooling. We also replace the global average pooling layer with an attention pooling mechanism.
                </p>
                <p>
                    For the second architecture, we experiment with the recently introduced Vision Transformer (ViT). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer.
                </p>
            </div>

            <div class="translation">
                <h4>📝 中文翻譯</h4>
                <p>
                    我們考慮兩種不同的圖像編碼器架構。
                    對於第一種，我們使用 ResNet-50 作為圖像編碼器的基礎架構，
                    因為它被廣泛採用且性能經過驗證。
                    我們對原始版本進行了幾處修改，使用 ResNet-D 改進和抗鋸齒 rect-2 模糊池化。
                    我們還用注意力池化機制替換了全局平均池化層。
                </p>
                <p>
                    對於第二種架構，我們實驗了最近引入的 Vision Transformer (ViT)。
                    我們緊密遵循其實現，只做了小修改：
                    在 Transformer 之前對組合的 patch 和位置嵌入添加額外的層正規化。
                </p>
            </div>

            <div class="explanation">
                <h4>🔍 兩種架構的對比</h4>
                
                <div style="overflow-x: auto; margin: 20px 0;">
                    <table style="width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e5e7eb; border-radius: 12px; overflow: hidden; font-family: 'Inter', sans-serif;">
                        <thead>
                            <tr style="background: #f8fafc; color: #1e293b;">
                                <th style="padding: 16px; text-align: left; font-weight: 700; border-bottom: 2px solid #e2e8f0;">特性</th>
                                <th style="padding: 16px; text-align: left; font-weight: 700; border-bottom: 2px solid #e2e8f0;">ResNet</th>
                                <th style="padding: 16px; text-align: left; font-weight: 700; border-bottom: 2px solid #e2e8f0;">Vision Transformer</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr style="background: #fff; border-bottom: 1px solid #f1f5f9;">
                                <td style="padding: 16px; font-weight: 600; color: #334155;">基礎架構</td>
                                <td style="padding: 16px; color: #475569;">卷積神經網路</td>
                                <td style="padding: 16px; color: #475569;">Transformer</td>
                            </tr>
                            <tr style="background: #f8fafc; border-bottom: 1px solid #f1f5f9;">
                                <td style="padding: 16px; font-weight: 600; color: #334155;">輸入處理</td>
                                <td style="padding: 16px; color: #475569;">卷積層提取特徵</td>
                                <td style="padding: 16px; color: #475569;">將圖像切分為 patches</td>
                            </tr>
                            <tr style="background: #fff; border-bottom: 1px solid #f1f5f9;">
                                <td style="padding: 16px; font-weight: 600; color: #334155;">池化機制</td>
                                <td style="padding: 16px; color: #475569;">注意力池化</td>
                                <td style="padding: 16px; color: #475569;">CLS token</td>
                            </tr>
                            <tr style="background: #eff6ff;">
                                <td style="padding: 16px; font-weight: 600; color: #1e3a8a;">最終性能</td>
                                <td style="padding: 16px; color: #1e3a8a;">良好</td>
                                <td style="padding: 16px; color: #1e40af; font-weight: 700;">最佳（ViT-L/14@336px）</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h5>🔧 ResNet 的改進</h5>
                <ul>
                    <li><strong>ResNet-D</strong>：改進的殘差連接，提升性能</li>
                    <li><strong>抗鋸齒池化</strong>：減少下採樣時的混疊效應</li>
                    <li><strong>注意力池化</strong>：用 Transformer 風格的注意力機制替代全局平均池化</li>
                </ul>

                <h5>🔧 Vision Transformer 的優勢</h5>
                <ul>
                    <li><strong>統一架構</strong>：圖像和文字都使用 Transformer</li>
                    <li><strong>更好的擴展性</strong>：在大規模數據上表現更好</li>
                    <li><strong>最終選擇</strong>：ViT-L/14@336px 是 CLIP 的最佳模型</li>
                </ul>
            </div>
        </div>

        <h2>📝 文字編碼器：Transformer</h2>

        <div class="paper-section">
            <div class="original-quote">
                <strong>原文</strong>
                <p>
                    The text encoder is a Transformer with the architecture modifications described in Radford et al. (2019). As a base size we use a 63M-parameter 12-layer 512-wide model with 8 attention heads. The transformer operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size. For computational efficiency, the max sequence length was capped at 76.
                </p>
            </div>

            <div class="translation">
                <h4>📝 中文翻譯</h4>
                <p>
                    文字編碼器是一個 Transformer，架構修改如 Radford 等人（2019）所述。
                    作為基礎大小，我們使用一個 6300 萬參數的 12 層 512 寬模型，有 8 個注意力頭。
                    Transformer 在文字的小寫字節對編碼（BPE）表徵上操作，詞彙大小為 49,152。
                    為了計算效率，最大序列長度限制為 76。
                </p>
            </div>

            <div class="explanation">
                <h4>🔍 文字編碼器的設計</h4>
                
                <div class="key-concept">
                    <h6>📊 架構規格</h6>
                    <ul>
                        <li><strong>參數</strong>：63M（基礎模型）</li>
                        <li><strong>層數</strong>：12 層</li>
                        <li><strong>隱藏維度</strong>：512</li>
                        <li><strong>注意力頭</strong>：8 個</li>
                        <li><strong>詞彙大小</strong>：49,152（BPE）</li>
                        <li><strong>最大長度</strong>：76 tokens</li>
                    </ul>
                </div>

                <h5>🔧 關鍵設計選擇</h5>
                <ul>
                    <li><strong>BPE 編碼</strong>：與 GPT-2 相同的分詞方式</li>
                    <li><strong>小寫</strong>：所有文字轉為小寫，簡化學習</li>
                    <li><strong>特殊 token</strong>：[SOS] 和 [EOS] 標記序列開始和結束</li>
                    <li><strong>輸出</strong>：使用 [EOS] token 的表示作為文字表徵</li>
                    <li><strong>掩碼注意力</strong>：保留與預訓練語言模型兼容的可能性</li>
                </ul>
            </div>
        </div>

        <h2>📈 模型擴展策略</h2>

        <div class="paper-section">
            <div class="original-quote">
                <strong>原文</strong>
                <p>
                    While previous computer vision research has often scaled models by increasing the width or depth in isolation, for the ResNet image encoders we adapt the approach of EfficientNet which found that allocating additional compute across all of width, depth, and resolution outperforms only allocating it to only one dimension of the model. We use a simple baseline of allocating additional compute equally to increasing the width, depth, and resolution of the model.
                </p>
                <p>
                    For the text encoder, we only scale the width of the model to be proportional to the calculated increase in width of the ResNet and do not scale the depth at all, as we found CLIP's performance to be less sensitive to the capacity of the text encoder.
                </p>
            </div>

            <div class="translation">
                <h4>📝 中文翻譯</h4>
                <p>
                    雖然先前的計算機視覺研究經常通過單獨增加寬度或深度來擴展模型，
                    但對於 ResNet 圖像編碼器，我們採用 EfficientNet 的方法，
                    該方法發現將額外計算分配給寬度、深度和解析度的所有維度，
                    優於僅將其分配給模型的一個維度。
                    我們使用一個簡單的基線：將額外計算平均分配給增加模型的寬度、深度和解析度。
                </p>
                <p>
                    對於文字編碼器，我們只按比例縮放模型的寬度以對應 ResNet 寬度的計算增加，
                    完全不縮放深度，因為我們發現 CLIP 的性能對文字編碼器的容量不太敏感。
                </p>
            </div>

            <div class="explanation">
                <h4>📊 CLIP 的模型系列</h4>
                
                <h5>ResNet 系列</h5>
                <div style="overflow-x: auto; margin: 20px 0;">
                    <table style="width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e5e7eb; border-radius: 12px; overflow: hidden; font-family: 'Inter', sans-serif;">
                        <thead>
                            <tr style="background: #f8fafc; color: #1e293b;">
                                <th style="padding: 16px; text-align: left; font-weight: 700; border-bottom: 2px solid #e2e8f0;">模型</th>
                                <th style="padding: 16px; text-align: center; font-weight: 700; border-bottom: 2px solid #e2e8f0;">計算量</th>
                                <th style="padding: 16px; text-align: left; font-weight: 700; border-bottom: 2px solid #e2e8f0;">說明</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr style="background: #fff; border-bottom: 1px solid #f1f5f9;">
                                <td style="padding: 16px; font-weight: 600; color: #334155;">ResNet-50</td>
                                <td style="padding: 16px; text-align: center; color: #64748b;">1x</td>
                                <td style="padding: 16px; color: #475569;">基準模型</td>
                            </tr>
                            <tr style="background: #f8fafc; border-bottom: 1px solid #f1f5f9;">
                                <td style="padding: 16px; font-weight: 600; color: #334155;">ResNet-101</td>
                                <td style="padding: 16px; text-align: center; color: #64748b;">~2x</td>
                                <td style="padding: 16px; color: #475569;">增加深度</td>
                            </tr>
                            <tr style="background: #fff; border-bottom: 1px solid #f1f5f9;">
                                <td style="padding: 16px; font-weight: 600; color: #334155;">RN50x4</td>
                                <td style="padding: 16px; text-align: center; color: #64748b;">4x</td>
                                <td style="padding: 16px; color: #475569;">EfficientNet 風格擴展</td>
                            </tr>
                            <tr style="background: #f8fafc; border-bottom: 1px solid #f1f5f9;">
                                <td style="padding: 16px; font-weight: 600; color: #334155;">RN50x16</td>
                                <td style="padding: 16px; text-align: center; color: #64748b;">16x</td>
                                <td style="padding: 16px; color: #475569;">EfficientNet 風格擴展</td>
                            </tr>
                            <tr style="background: #eff6ff;">
                                <td style="padding: 16px; font-weight: 600; color: #1e3a8a;">RN50x64</td>
                                <td style="padding: 16px; text-align: center; color: #1e40af; font-weight: 700;">64x</td>
                                <td style="padding: 16px; color: #1e3a8a;">最大 ResNet 模型</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h5>Vision Transformer 系列</h5>
                <div style="overflow-x: auto; margin: 20px 0;">
                    <table style="width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e5e7eb; border-radius: 12px; overflow: hidden; font-family: 'Inter', sans-serif;">
                        <thead>
                            <tr style="background: #f8fafc; color: #1e293b;">
                                <th style="padding: 16px; text-align: left; font-weight: 700; border-bottom: 2px solid #e2e8f0;">模型</th>
                                <th style="padding: 16px; text-align: left; font-weight: 700; border-bottom: 2px solid #e2e8f0;">Patch Size</th>
                                <th style="padding: 16px; text-align: left; font-weight: 700; border-bottom: 2px solid #e2e8f0;">說明</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr style="background: #fff; border-bottom: 1px solid #f1f5f9;">
                                <td style="padding: 16px; font-weight: 600; color: #334155;">ViT-B/32</td>
                                <td style="padding: 16px; color: #475569;">32x32</td>
                                <td style="padding: 16px; color: #475569;">Base 模型，大 patch</td>
                            </tr>
                            <tr style="background: #f8fafc; border-bottom: 1px solid #f1f5f9;">
                                <td style="padding: 16px; font-weight: 600; color: #334155;">ViT-B/16</td>
                                <td style="padding: 16px; color: #475569;">16x16</td>
                                <td style="padding: 16px; color: #475569;">Base 模型，小 patch</td>
                            </tr>
                            <tr style="background: #eff6ff;">
                                <td style="padding: 16px; font-weight: 700; color: #1e3a8a; border-left: 4px solid #3b82f6;">ViT-L/14@336px</td>
                                <td style="padding: 16px; color: #1e3a8a;">14x14</td>
                                <td style="padding: 16px; color: #1e40af; font-weight: 700;">最佳模型（最終選擇）</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="key-concept">
                    <h6>💡 擴展策略的關鍵洞察</h6>
                    <ul>
                        <li><strong>三維擴展</strong>：同時增加寬度、深度和解析度，而非單一維度</li>
                        <li><strong>文字編碼器</strong>：只擴展寬度，不擴展深度（性能不敏感）</li>
                        <li><strong>計算效率</strong>：EfficientNet 風格的擴展比單維度擴展更高效</li>
                    </ul>
                </div>
            </div>
        </div>

        <h2>⚙️ 訓練細節</h2>

        <div class="paper-section">
            <div class="original-quote">
                <strong>原文</strong>
                <p>
                    We train all models for 32 epochs. We use the Adam optimizer with decoupled weight decay regularization and decay the learning rate using a cosine schedule. We use a very large minibatch size of 32,768. The largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs.
                </p>
            </div>

            <div class="translation">
                <h4>📝 中文翻譯</h4>
                <p>
                    我們訓練所有模型 32 個 epoch。
                    我們使用 Adam 優化器，帶有解耦權重衰減正規化，
                    並使用餘弦調度衰減學習率。
                    我們使用非常大的小批次大小 32,768。
                    最大的 ResNet 模型 RN50x64 在 592 個 V100 GPU 上訓練了 18 天，
                    而最大的 Vision Transformer 在 256 個 V100 GPU 上訓練了 12 天。
                </p>
            </div>

            <div class="explanation">
                <h4>🔧 訓練配置</h4>
                
                <ul>
                    <li><strong>Epochs</strong>：32（所有模型）</li>
                    <li><strong>優化器</strong>：Adam + 解耦權重衰減</li>
                    <li><strong>學習率</strong>：餘弦調度</li>
                    <li><strong>Batch Size</strong>：32,768（超大批次）</li>
                    <li><strong>混合精度</strong>：加速訓練並節省記憶體</li>
                    <li><strong>梯度檢查點</strong>：節省記憶體</li>
                </ul>

                <div class="key-concept">
                    <h6>💡 超大 Batch Size 的意義</h6>
                    <ul>
                        <li><strong>更多負樣本</strong>：對比學習需要大量負例，大 batch 提供更多</li>
                        <li><strong>更穩定的梯度</strong>：大 batch 提供更準確的梯度估計</li>
                        <li><strong>技術挑戰</strong>：需要分片計算相似度矩陣，跨 GPU 協調</li>
                    </ul>
                </div>
            </div>
        </div>

        <h2>📚 本章小結</h2>

        <div class="paper-section">
            <div class="explanation">
                <p>
                    <strong>CLIP 的架構設計</strong>：
                </p>
                <ul>
                    <li><strong>雙編碼器架構</strong>：由一個圖像編碼器和一個文字編碼器組成（非同時使用兩種圖像編碼器）。</li>
                    <li><strong>圖像編碼器選擇</strong>：論文實驗了 ResNet 和 Vision Transformer 兩種架構。</li>
                    <li><strong>最佳模型</strong>：最終發現 <strong>ViT-L/14@336px</strong> 效果最好，成為官方預設的 CLIP 模型。</li>
                </ul>

                <p>
                    <strong>擴展策略</strong>：
                </p>
                <ul>
                    <li><strong>三維擴展</strong>：同時增加寬度、深度和解析度</li>
                    <li><strong>文字編碼器</strong>：只擴展寬度，不擴展深度</li>
                    <li><strong>8 個模型</strong>：5 個 ResNet + 3 個 ViT，跨越 2 個數量級的計算量</li>
                </ul>

                <div class="key-concept">
                    <h6>💡 下一章預告</h6>
                    <p>
                        在第 5 章，我們將探索<strong>零樣本遷移實驗</strong>：
                        CLIP 如何在 30+ 資料集上不需訓練就能分類？
                        為何零樣本性能能匹敵監督學習？
                    </p>
                </div>
            </div>
        </div>

        <div class="nav-buttons">
            <a href="03-contrastive-learning.html" class="nav-button nav-button-prev">
                ← 上一章：對比式預訓練
            </a>
            <a href="05-zero-shot-transfer.html" class="nav-button nav-button-next">
                下一章：零樣本遷移實驗 →
            </a>
        </div>
    </div>
</body>
</html>


