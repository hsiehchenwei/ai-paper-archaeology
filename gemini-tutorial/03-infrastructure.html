<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è¨“ç·´åŸºç¤è¨­æ–½ - Gemini è«–æ–‡æ·±åº¦è§£æ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <!-- Hero Section -->
        <div class="hero-section" style="background-image: url('images/generated/hero_03_infrastructure.png');">
            <div class="hero-overlay"></div>
            <div class="hero-content">
                <h1>è¶…å¤§è¦æ¨¡è¨“ç·´çš„å·¥ç¨‹å¥‡è¹Ÿ</h1>
                <p class="hero-subtitle">å¾ 85% åˆ° 97% çš„è¨“ç·´æ•ˆç‡æå‡</p>
                <div class="hero-meta">Gemini è«–æ–‡æ·±åº¦è§£æ Â· ç¬¬ 3 ç« </div>
            </div>
        </div>

        <!-- Breadcrumb -->
        <nav class="breadcrumb">
            <a href="../index.html">ğŸ  é¦–é </a>
            <span>/</span>
            <a href="index.html">Gemini æ·±åº¦è§£æ</a>
            <span>/</span>
            <span class="current">ç¬¬ 3 ç« ï¼šè¨“ç·´åŸºç¤è¨­æ–½</span>
        </nav>

        <div class="story-container">
            <!-- Drop Cap é–‹å ´ -->
            <p class="drop-cap">
                è¨“ç·´ Gemini Ultra é€™æ¨£è¦æ¨¡çš„æ¨¡å‹ï¼Œéœ€è¦çš„ä¸åƒ…åƒ…æ˜¯å¼·å¤§çš„ç¡¬é«”ï¼Œæ›´éœ€è¦ä¸€å¥—èƒ½å¤ æ‡‰å°è¶…å¤§è¦æ¨¡æŒ‘æˆ°çš„åŸºç¤è¨­æ–½ã€‚Google ä½¿ç”¨äº†æ•¸åƒå€‹ TPUv4 åŠ é€Ÿå™¨ï¼Œè·¨è¶Šå¤šå€‹è³‡æ–™ä¸­å¿ƒï¼Œé€™å¸¶ä¾†äº†å‰æ‰€æœªæœ‰çš„å·¥ç¨‹æŒ‘æˆ°ã€‚ä½†é€šéå‰µæ–°çš„æ¶æ§‹è¨­è¨ˆå’Œæ•…éšœæ¢å¾©æ©Ÿåˆ¶ï¼Œä»–å€‘å°‡è¨“ç·´æ•ˆç‡å¾ 85% æå‡åˆ°äº† 97%ï¼Œé€™æ˜¯ä¸€å€‹ä»¤äººé©šè‰·çš„å·¥ç¨‹æˆå°±ã€‚
            </p>

            <div class="section-divider"><span>âœ¦</span></div>

            <!-- TPU åŸºç¤è¨­æ–½ -->
            <h2>TPU SuperPodsï¼šè¶…å¤§è¦æ¨¡è¨ˆç®—å–®å…ƒ</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        We trained Gemini models using TPUv5e and TPUv4, depending on their sizes and configuration. Training Gemini Ultra used a large fleet of TPUv4 accelerators owned by Google across multiple datacenters. This represents a significant increase in scale over our prior flagship model PaLM-2 which presented new infrastructure challenges.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        æˆ‘å€‘ä½¿ç”¨ TPUv5e å’Œ TPUv4 è¨“ç·´ Gemini æ¨¡å‹ï¼Œå…·é«”å–æ±ºæ–¼å®ƒå€‘çš„å°ºå¯¸å’Œé…ç½®ã€‚è¨“ç·´ Gemini Ultra ä½¿ç”¨äº† Google æ“æœ‰çš„è·¨å¤šå€‹è³‡æ–™ä¸­å¿ƒçš„å¤§é‡ TPUv4 åŠ é€Ÿå™¨ã€‚é€™ä»£è¡¨è‘—ç›¸è¼ƒæ–¼æˆ‘å€‘ä¹‹å‰çš„æ——è‰¦æ¨¡å‹ PaLM-2ï¼Œè¦æ¨¡æœ‰äº†é¡¯è‘—å¢åŠ ï¼Œé€™å¸¶ä¾†äº†æ–°çš„åŸºç¤è¨­æ–½æŒ‘æˆ°ã€‚
                    </div>
                </div>
            </div>

            <!-- AI ç”Ÿæˆæ¦‚å¿µåœ– -->
            <div class="figure figure-ai">
                <img src="images/generated/concept_infrastructure_03.png" alt="åˆ†æ•£å¼ TPU é›†ç¾¤æ¦‚å¿µåœ–">
                <div class="caption">
                    ğŸ’¡ <strong>AI åœ–è§£ï¼š</strong>è¶…å¤§è¦æ¨¡åˆ†æ•£å¼ TPU é›†ç¾¤
                </div>
            </div>

            <div class="key-concept">
                <h4>ğŸ’¡ TPU SuperPod æ¶æ§‹</h4>
                <p>
                    TPUv4 åŠ é€Ÿå™¨éƒ¨ç½²åœ¨ã€ŒSuperPodsã€ä¸­ï¼Œæ¯å€‹ SuperPod åŒ…å« 4096 å€‹æ™¶ç‰‡ï¼Œé€£æ¥åˆ°å°ˆç”¨çš„å…‰å­¸äº¤æ›æ©Ÿã€‚é€™å€‹äº¤æ›æ©Ÿå¯ä»¥åœ¨ç´„ 10 ç§’å…§å°‡ 4x4x4 çš„æ™¶ç‰‡ç«‹æ–¹é«”å‹•æ…‹é‡æ–°é…ç½®ç‚ºä»»æ„çš„ 3D ç’°é¢æ‹“æ’²ã€‚å°æ–¼ Gemini Ultraï¼Œåœ˜éšŠæ±ºå®šåœ¨æ¯å€‹ SuperPod ä¸­ä¿ç•™å°‘é‡ç«‹æ–¹é«”ï¼Œä»¥æ”¯æŒç†±å‚™ä»½å’Œæ»¾å‹•ç¶­è­·ã€‚
                </p>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <!-- ç¶²è·¯æ¶æ§‹ -->
            <h2>è·¨è³‡æ–™ä¸­å¿ƒçš„ç¶²è·¯é€£æ¥</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        TPU accelerators primarily communicate over the high speed inter-chip-interconnect, but at Gemini Ultra scale, we combine SuperPods in multiple datacenters using Google's intra-cluster and inter-cluster network. Google's network latencies and bandwidths are sufficient to support the commonly used synchronous training paradigm, exploiting model parallelism within superpods and data-parallelism across superpods.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        TPU åŠ é€Ÿå™¨ä¸»è¦é€šéé«˜é€Ÿæ™¶ç‰‡é–“äº’é€£é€²è¡Œé€šä¿¡ï¼Œä½†åœ¨ Gemini Ultra çš„è¦æ¨¡ä¸‹ï¼Œæˆ‘å€‘ä½¿ç”¨ Google çš„é›†ç¾¤å…§å’Œé›†ç¾¤é–“ç¶²è·¯åœ¨å¤šå€‹è³‡æ–™ä¸­å¿ƒä¸­çµ„åˆ SuperPodã€‚Google çš„ç¶²è·¯å»¶é²å’Œé »å¯¬è¶³ä»¥æ”¯æŒå¸¸ç”¨çš„åŒæ­¥è¨“ç·´ç¯„å¼ï¼Œåœ¨ SuperPod å…§åˆ©ç”¨æ¨¡å‹ä¸¦è¡Œï¼Œåœ¨ SuperPod é–“åˆ©ç”¨è³‡æ–™ä¸¦è¡Œã€‚
                    </div>
                </div>
            </div>

            <div class="analogy-section">
                <div class="analogy-card life">
                    <h4>ğŸ  ç”Ÿæ´»é¡æ¯”</h4>
                    <p>
                        æƒ³åƒä¸€å€‹å·¨å¤§çš„äº¤éŸ¿æ¨‚åœ˜ï¼Œæ¨‚æ‰‹å€‘åˆ†ä½ˆåœ¨å¤šå€‹éŸ³æ¨‚å»³ä¸­ï¼Œä½†é€šéå…ˆé€²çš„éŸ³è¨Šç³»çµ±ï¼Œä»–å€‘èƒ½å¤ å®Œç¾åŒæ­¥æ¼”å¥ã€‚æ¯å€‹ SuperPod å°±åƒä¸€å€‹éŸ³æ¨‚å»³ï¼Œè€Œ Google çš„ç¶²è·¯å°±åƒé€£æ¥é€™äº›éŸ³æ¨‚å»³çš„éŸ³è¨Šç³»çµ±ï¼Œç¢ºä¿æ‰€æœ‰ã€Œæ¨‚æ‰‹ã€ï¼ˆTPUï¼‰èƒ½å¤ å”èª¿ä¸€è‡´åœ°å·¥ä½œã€‚
                    </p>
                </div>
                <div class="analogy-card engineering">
                    <h4>âš™ï¸ å·¥ç¨‹é¡æ¯”</h4>
                    <p>
                        é€™é¡ä¼¼æ–¼å¾®æœå‹™æ¶æ§‹ä¸­çš„æœå‹™ç¶²æ ¼ï¼ˆService Meshï¼‰ã€‚æ¯å€‹ SuperPod æ˜¯ä¸€å€‹ç¨ç«‹çš„æœå‹™å–®å…ƒï¼Œé€šéé«˜é€Ÿç¶²è·¯é€£æ¥ï¼Œå¯¦ç¾ï¼š
                        <ul>
                            <li><strong>æ¨¡å‹ä¸¦è¡Œ</strong>ï¼šåœ¨ SuperPod å…§éƒ¨ï¼Œæ¨¡å‹çš„ä¸åŒéƒ¨åˆ†åˆ†ä½ˆåœ¨ä¸åŒ TPU ä¸Š</li>
                            <li><strong>è³‡æ–™ä¸¦è¡Œ</strong>ï¼šä¸åŒçš„ SuperPod è™•ç†ä¸åŒçš„è³‡æ–™æ‰¹æ¬¡</li>
                        </ul>
                    </p>
                </div>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <!-- å–®æ§åˆ¶å™¨ç·¨ç¨‹æ¨¡å‹ -->
            <h2>Jax å’Œ Pathwaysï¼šç°¡åŒ–çš„ç·¨ç¨‹æ¨¡å‹</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        The `single controller' programming model of Jax and Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow. The GSPMD partitioner in the XLA compiler partitions the training step computation, and the MegaScale XLA compiler pass statically schedules appropriate collectives so that they maximally overlap with the computation with very little variation in step time.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        Jax å’Œ Pathways çš„ã€Œå–®æ§åˆ¶å™¨ã€ç·¨ç¨‹æ¨¡å‹å…è¨±å–®å€‹ Python é€²ç¨‹å”èª¿æ•´å€‹è¨“ç·´é‹è¡Œï¼Œå¤§å¹…ç°¡åŒ–é–‹ç™¼å·¥ä½œæµç¨‹ã€‚XLA ç·¨è­¯å™¨ä¸­çš„ GSPMD åˆ†å€å™¨å°è¨“ç·´æ­¥é©Ÿè¨ˆç®—é€²è¡Œåˆ†å€ï¼Œè€Œ MegaScale XLA ç·¨è­¯å™¨å‚³ééœæ…‹èª¿åº¦é©ç•¶çš„é›†åˆæ“ä½œï¼Œä½¿å®ƒå€‘èˆ‡è¨ˆç®—æœ€å¤§ç¨‹åº¦é‡ç–Šï¼Œæ­¥é©Ÿæ™‚é–“è®ŠåŒ–å¾ˆå°ã€‚
                    </div>
                </div>
            </div>

            <div class="key-concept">
                <h4>ğŸ’¡ ç‚ºä»€éº¼å–®æ§åˆ¶å™¨æ¨¡å‹å¾ˆé‡è¦ï¼Ÿ</h4>
                <p>
                    å‚³çµ±çš„åˆ†æ•£å¼è¨“ç·´éœ€è¦è¤‡é›œçš„å”èª¿é‚è¼¯ï¼Œè€Œ Jax å’Œ Pathways çš„å–®æ§åˆ¶å™¨æ¨¡å‹è®“é–‹ç™¼è€…å¯ä»¥ç”¨å–®ä¸€ Python é€²ç¨‹ä¾†æ§åˆ¶æ•´å€‹è¨“ç·´éç¨‹ã€‚é€™å°±åƒï¼š
                </p>
                <ul>
                    <li>å¾ã€ŒæŒ‡æ®å¤šå€‹æ¨‚åœ˜ã€è®Šæˆã€ŒæŒ‡æ®ä¸€å€‹å¤§æ¨‚åœ˜ã€</li>
                    <li>å¾ã€Œç®¡ç†å¤šå€‹å¾®æœå‹™ã€è®Šæˆã€Œç®¡ç†ä¸€å€‹çµ±ä¸€ç³»çµ±ã€</li>
                    <li>å¤§å¹…ç°¡åŒ–äº†èª¿è©¦å’Œé–‹ç™¼æµç¨‹</li>
                </ul>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <!-- æ•…éšœæ¢å¾©æ©Ÿåˆ¶ -->
            <h2>å‰µæ–°çš„æ•…éšœæ¢å¾©ï¼šå¾ 85% åˆ° 97% çš„æ•ˆç‡æå‡</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Maintaining a high goodput at this scale would have been impossible using the conventional approach of periodic checkpointing of weights to persistent cluster storage. For Gemini models, we instead made use of redundant in-memory copies of the model state, and on any unplanned hardware failures, we rapidly recover directly from an intact model replica. Compared to both PaLM and PaLM-2, this provided a substantial speedup in recovery time, despite the significantly larger training resources being used. As a result, the overall goodput for the largest-scale training job increased from 85\% to 97\%.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        åœ¨é€™ç¨®è¦æ¨¡ä¸‹ï¼Œä½¿ç”¨å‚³çµ±çš„å®šæœŸå°‡æ¬Šé‡æª¢æŸ¥é»ä¿å­˜åˆ°æŒä¹…é›†ç¾¤å­˜å„²çš„æ–¹æ³•ï¼Œä¸å¯èƒ½ç¶­æŒé«˜ååé‡ã€‚å°æ–¼ Gemini æ¨¡å‹ï¼Œæˆ‘å€‘æ”¹ç‚ºä½¿ç”¨æ¨¡å‹ç‹€æ…‹çš„å†—é¤˜è¨˜æ†¶é«”å‰¯æœ¬ï¼Œåœ¨ä»»ä½•éè¨ˆåŠƒçš„ç¡¬é«”æ•…éšœæ™‚ï¼Œæˆ‘å€‘ç›´æ¥å¾å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬å¿«é€Ÿæ¢å¾©ã€‚èˆ‡ PaLM å’Œ PaLM-2 ç›¸æ¯”ï¼Œå„˜ç®¡ä½¿ç”¨äº†é¡¯è‘—æ›´å¤§çš„è¨“ç·´è³‡æºï¼Œé€™ä»æä¾›äº†æ¢å¾©æ™‚é–“çš„å¤§å¹…åŠ é€Ÿã€‚çµæœï¼Œæœ€å¤§è¦æ¨¡è¨“ç·´ä½œæ¥­çš„æ•´é«”ååé‡å¾ 85% å¢åŠ åˆ° 97%ã€‚
                    </div>
                </div>
            </div>

            <div class="key-concept">
                <h4>ğŸ’¡ Goodputï¼šçœŸæ­£çš„è¨“ç·´æ•ˆç‡</h4>
                <p>
                    <strong>Goodput</strong> å®šç¾©ç‚ºï¼šåœ¨è¨“ç·´ä½œæ¥­çš„ç¸½æ™‚é–“ä¸­ï¼Œç”¨æ–¼è¨ˆç®—æœ‰ç”¨æ–°æ­¥é©Ÿçš„æ™‚é–“æ¯”ä¾‹ã€‚
                </p>
                <p>
                    å‚³çµ±æ–¹æ³•éœ€è¦å®šæœŸå°‡æ¨¡å‹æ¬Šé‡ä¿å­˜åˆ°ç£ç¢Ÿï¼Œé€™æœƒï¼š
                </p>
                <ul>
                    <li>ä¸­æ–·è¨“ç·´æµç¨‹</li>
                    <li>æ¶ˆè€—å¤§é‡ I/O è³‡æº</li>
                    <li>æ¢å¾©æ™‚éœ€è¦å¾ç£ç¢Ÿè®€å–ï¼Œè€—æ™‚è¼ƒé•·</li>
                </ul>
                <p>
                    Gemini çš„å‰µæ–°æ–¹æ³•ï¼š
                </p>
                <ul>
                    <li>åœ¨è¨˜æ†¶é«”ä¸­ç¶­è­·å†—é¤˜çš„æ¨¡å‹å‰¯æœ¬</li>
                    <li>ç¡¬é«”æ•…éšœæ™‚ç›´æ¥å¾è¨˜æ†¶é«”æ¢å¾©</li>
                    <li>å¤§å¹…æ¸›å°‘æ¢å¾©æ™‚é–“ï¼Œæå‡æ•´é«”æ•ˆç‡</li>
                </ul>
            </div>

            <div class="section-divider"><span>âœ¦</span></div>

            <!-- éœé»˜è³‡æ–™æå£ -->
            <h2>æ‡‰å°éœé»˜è³‡æ–™æå£ï¼ˆSDCï¼‰</h2>

            <div class="original-quote">
                <div class="content">
                    <div class="en">
                        <strong>ğŸ“„ è«–æ–‡åŸæ–‡</strong><br><br>
                        Training at unprecedented scale invariably surfaces new and interesting systems failure modes - and in this instance one of the problems that we needed to address was that of ``Silent Data Corruption (SDC)''. Although these are extremely rare, the scale of Gemini models means that we can expect SDC events to impact training every week or two. Rapidly detecting and removing faulty hardware required several new techniques that exploit deterministic replay to isolate incorrect computations, combined with proactive SDC scanners on idle machines and hot standbys.
                    </div>
                    <div class="zh">
                        <strong>ç¿»è­¯</strong><br><br>
                        åœ¨å²ç„¡å‰ä¾‹çš„è¦æ¨¡ä¸‹é€²è¡Œè¨“ç·´ï¼Œä¸å¯é¿å…åœ°æœƒå‡ºç¾æ–°çš„å’Œæœ‰è¶£çš„ç³»çµ±æ•…éšœæ¨¡å¼â€”â€”åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘éœ€è¦è§£æ±ºçš„å•é¡Œä¹‹ä¸€æ˜¯ã€Œéœé»˜è³‡æ–™æå£ï¼ˆSDCï¼‰ã€ã€‚é›–ç„¶é€™äº›æƒ…æ³æ¥µå…¶ç½•è¦‹ï¼Œä½† Gemini æ¨¡å‹çš„è¦æ¨¡æ„å‘³è‘—æˆ‘å€‘å¯ä»¥é æœŸ SDC äº‹ä»¶æ¯é€±æˆ–æ¯å…©é€±å½±éŸ¿è¨“ç·´ä¸€æ¬¡ã€‚å¿«é€Ÿæª¢æ¸¬å’Œç§»é™¤æ•…éšœç¡¬é«”éœ€è¦å¹¾ç¨®æ–°æŠ€è¡“ï¼Œåˆ©ç”¨ç¢ºå®šæ€§é‡æ”¾ä¾†éš”é›¢ä¸æ­£ç¢ºçš„è¨ˆç®—ï¼Œçµåˆç©ºé–’æ©Ÿå™¨ä¸Šçš„ä¸»å‹• SDC æƒæå™¨å’Œç†±å‚™ä»½ã€‚
                    </div>
                </div>
            </div>

            <div class="key-concept">
                <h4>ğŸ’¡ éœé»˜è³‡æ–™æå£çš„æŒ‘æˆ°</h4>
                <p>
                    åœ¨è¶…å¤§è¦æ¨¡ç³»çµ±ä¸­ï¼Œç¡¬é«”æ•…éšœæ˜¯ä¸å¯é¿å…çš„ã€‚éœé»˜è³‡æ–™æå£ï¼ˆSDCï¼‰æ˜¯ä¸€ç¨®ç‰¹åˆ¥æ£˜æ‰‹çš„å•é¡Œï¼Œå› ç‚ºï¼š
                </p>
                <ul>
                    <li>å®ƒä¸æœƒå°è‡´ç³»çµ±å´©æ½°ï¼Œä½†æœƒç”¢ç”ŸéŒ¯èª¤çš„è¨ˆç®—çµæœ</li>
                    <li>åœ¨ Gemini çš„è¦æ¨¡ä¸‹ï¼Œæ¯é€±æˆ–æ¯å…©é€±å°±æœƒç™¼ç”Ÿä¸€æ¬¡</li>
                    <li>å¦‚æœä¸åŠæ™‚ç™¼ç¾ï¼Œæœƒæ±¡æŸ“æ•´å€‹è¨“ç·´éç¨‹</li>
                </ul>
                <p>
                    Google çš„è§£æ±ºæ–¹æ¡ˆï¼š
                </p>
                <ul>
                    <li><strong>ç¢ºå®šæ€§é‡æ”¾</strong>ï¼šèƒ½å¤ é‡ç¾è¨ˆç®—éç¨‹ï¼Œæ‰¾å‡ºéŒ¯èª¤ä¾†æº</li>
                    <li><strong>ä¸»å‹•æƒæ</strong>ï¼šåœ¨ç©ºé–’æ©Ÿå™¨ä¸Šé‹è¡Œ SDC æƒæå™¨</li>
                    <li><strong>ç†±å‚™ä»½</strong>ï¼šéš¨æ™‚æº–å‚™æ›¿æ›æ•…éšœç¡¬é«”</li>
                </ul>
            </div>

            <!-- Quote Block é‡‘å¥ -->
            <div class="quote-block">
                ã€Œå¾ 85% åˆ° 97% çš„æ•ˆç‡æå‡ï¼Œå±•ç¾äº†å·¥ç¨‹å‰µæ–°çš„åŠ›é‡ã€
            </div>

            <!-- æœ¬ç« é‡é»å›é¡§ -->
            <div class="chapter-summary">
                <h3>ğŸ’¡ æœ¬ç« é‡é»</h3>
                <ul>
                    <li><strong>TPU SuperPods</strong>ï¼šæ¯å€‹ SuperPod åŒ…å« 4096 å€‹ TPU æ™¶ç‰‡ï¼Œæ”¯æŒå‹•æ…‹æ‹“æ’²é‡é…ç½®</li>
                    <li><strong>è·¨è³‡æ–™ä¸­å¿ƒè¨“ç·´</strong>ï¼šä½¿ç”¨ Google çš„é«˜é€Ÿç¶²è·¯é€£æ¥å¤šå€‹è³‡æ–™ä¸­å¿ƒçš„ SuperPod</li>
                    <li><strong>å–®æ§åˆ¶å™¨æ¨¡å‹</strong>ï¼šJax å’Œ Pathways ç°¡åŒ–äº†å¤§è¦æ¨¡è¨“ç·´çš„ç·¨ç¨‹è¤‡é›œåº¦</li>
                    <li><strong>å‰µæ–°æ•…éšœæ¢å¾©</strong>ï¼šä½¿ç”¨è¨˜æ†¶é«”å†—é¤˜å‰¯æœ¬ï¼Œå°‡è¨“ç·´æ•ˆç‡å¾ 85% æå‡åˆ° 97%</li>
                    <li><strong>SDC æ‡‰å°</strong>ï¼šé€šéç¢ºå®šæ€§é‡æ”¾ã€ä¸»å‹•æƒæå’Œç†±å‚™ä»½æ©Ÿåˆ¶ï¼Œå¿«é€Ÿæª¢æ¸¬å’Œè™•ç†éœé»˜è³‡æ–™æå£</li>
                    <li><strong>å®Œå…¨ç¢ºå®šæ€§</strong>ï¼šæ•´å€‹åŸºç¤è¨­æ–½çš„ç¢ºå®šæ€§è¨­è¨ˆï¼Œä½¿æ•…éšœè¨ºæ–·å’Œä¿®å¾©è®Šå¾—å¿«é€Ÿå¯é </li>
                </ul>
            </div>
        </div>

        <!-- Navigation -->
        <div class="chapter-nav">
            <a href="02-architecture.html" class="prev">â† ä¸Šä¸€ç« ï¼šæ¨¡å‹æ¶æ§‹</a>
            <a href="index.html" class="home">ğŸ“‘ ç›®éŒ„</a>
            <a href="04-dataset.html" class="next">ä¸‹ä¸€ç« ï¼šé è¨“ç·´è³‡æ–™é›† â†’</a>
        </div>
    </div>
</body>
</html>
