<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Primary Meta Tags -->
    <title>RLHF è¨“ç·´ä¸‰éƒ¨æ›²ï¼šSFT â†’ RM â†’ PPO | InstructGPT è«–æ–‡æ·±åº¦è§£æ</title>
    <meta name="title" content="RLHF è¨“ç·´ä¸‰éƒ¨æ›²ï¼šSFT â†’ RM â†’ PPO | InstructGPT è«–æ–‡æ·±åº¦è§£æ" />
    <meta name="description" content="æ·±å…¥è§£æ InstructGPT çš„ RLHF (Reinforcement Learning from Human Feedback) ä¸‰éšæ®µè¨“ç·´æµç¨‹ï¼šç›£ç£å¼å¾®èª¿ (SFT)ã€çå‹µæ¨¡å‹ (RM)ã€å¼·åŒ–å­¸ç¿’å„ªåŒ– (PPO)ã€‚é€™æ˜¯ç¾ä»£ LLM è¨“ç·´çš„é»ƒé‡‘æ¨™æº–æ¶æ§‹ï¼Œè¢« LLaMAã€Claude ç­‰æ¨¡å‹æ¡ç”¨ã€‚" />
    <meta name="keywords" content="InstructGPT, RLHF, Reinforcement Learning from Human Feedback, SFT, Supervised Fine-Tuning, Reward Model, PPO, Proximal Policy Optimization, å¼·åŒ–å­¸ç¿’, äººé¡å›é¥‹, ç›£ç£å¼å¾®èª¿, çå‹µæ¨¡å‹, LLaMA, Claude, ChatGPT, å¤§å‹èªè¨€æ¨¡å‹, AI è«–æ–‡, ç¹é«”ä¸­æ–‡" />
    <meta name="author" content="è¬æ‰¿ç·¯ (Chen Wei Hsieh)" />
    <meta name="robots" content="index, follow" />
    <link rel="canonical" href="https://hsiehchenwei.github.io/ai-paper-archaeology/instructgpt-tutorial/02-methodology.html" />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://hsiehchenwei.github.io/ai-paper-archaeology/instructgpt-tutorial/02-methodology.html" />
    <meta property="og:title" content="RLHF è¨“ç·´ä¸‰éƒ¨æ›²ï¼šSFT â†’ RM â†’ PPO | InstructGPT è«–æ–‡æ·±åº¦è§£æ" />
    <meta property="og:description" content="æ·±å…¥è§£æ InstructGPT çš„ RLHF ä¸‰éšæ®µè¨“ç·´æµç¨‹ï¼šç›£ç£å¼å¾®èª¿ã€çå‹µæ¨¡å‹ã€å¼·åŒ–å­¸ç¿’å„ªåŒ–ã€‚ç¾ä»£ LLM è¨“ç·´çš„é»ƒé‡‘æ¨™æº–æ¶æ§‹ã€‚" />
    <meta property="og:image" content="https://hsiehchenwei.github.io/ai-paper-archaeology/instructgpt-tutorial/images/original/InstructGPT_Diagram3.1.png" />
    <meta property="og:locale" content="zh_TW" />
    <meta property="og:site_name" content="AI Paper Archaeology" />

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image" />
    <meta property="twitter:url" content="https://hsiehchenwei.github.io/ai-paper-archaeology/instructgpt-tutorial/02-methodology.html" />
    <meta property="twitter:title" content="RLHF è¨“ç·´ä¸‰éƒ¨æ›²ï¼šSFT â†’ RM â†’ PPO | InstructGPT è«–æ–‡æ·±åº¦è§£æ" />
    <meta property="twitter:description" content="æ·±å…¥è§£æ InstructGPT çš„ RLHF ä¸‰éšæ®µè¨“ç·´æµç¨‹ï¼šç›£ç£å¼å¾®èª¿ã€çå‹µæ¨¡å‹ã€å¼·åŒ–å­¸ç¿’å„ªåŒ–ã€‚ç¾ä»£ LLM è¨“ç·´çš„é»ƒé‡‘æ¨™æº–æ¶æ§‹ã€‚" />
    <meta property="twitter:image" content="https://hsiehchenwei.github.io/ai-paper-archaeology/instructgpt-tutorial/images/original/InstructGPT_Diagram3.1.png" />

    <!-- Structured Data / Schema.org -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "RLHF è¨“ç·´ä¸‰éƒ¨æ›²ï¼šSFT â†’ RM â†’ PPO | InstructGPT è«–æ–‡æ·±åº¦è§£æ",
      "description": "æ·±å…¥è§£æ InstructGPT çš„ RLHF (Reinforcement Learning from Human Feedback) ä¸‰éšæ®µè¨“ç·´æµç¨‹ï¼šç›£ç£å¼å¾®èª¿ (SFT)ã€çå‹µæ¨¡å‹ (RM)ã€å¼·åŒ–å­¸ç¿’å„ªåŒ– (PPO)ã€‚",
      "image": "https://hsiehchenwei.github.io/ai-paper-archaeology/instructgpt-tutorial/images/original/InstructGPT_Diagram3.1.png",
      "author": {
        "@type": "Person",
        "name": "è¬æ‰¿ç·¯ (Chen Wei Hsieh)"
      },
      "publisher": {
        "@type": "Organization",
        "name": "AI Paper Archaeology",
        "url": "https://hsiehchenwei.github.io/ai-paper-archaeology/"
      },
      "datePublished": "2022-03-04",
      "dateModified": "2026-01-05",
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://hsiehchenwei.github.io/ai-paper-archaeology/instructgpt-tutorial/02-methodology.html"
      },
      "about": [
        { "@type": "Thing", "name": "InstructGPT" },
        { "@type": "Thing", "name": "RLHF" },
        { "@type": "Thing", "name": "Reinforcement Learning from Human Feedback" },
        { "@type": "Thing", "name": "Supervised Fine-Tuning" },
        { "@type": "Thing", "name": "Reward Model" },
        { "@type": "Thing", "name": "PPO" },
        { "@type": "Thing", "name": "Proximal Policy Optimization" }
      ],
      "inLanguage": "zh-TW"
    }
    </script>

    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <div class="breadcrumb">
            <a href="index.html">InstructGPT æ·±åº¦è§£æ</a>
            <span>/</span>
            <span class="current">02. æ ¸å¿ƒæ–¹æ³• (RLHF)</span>
        </div>

        <h1>ç¬¬ 2 é ï¼šå¦‚ä½•æ•™æœƒ AI è½æ‡‚äººè©±ï¼Ÿ (RLHF ä¸‰æ­¥é©Ÿ)</h1>
        
        <div class="header-section">
            <p class="lead">é€™ç¯‡è«–æ–‡æœ€ç¶“å…¸çš„è²¢ç»ï¼Œå°±æ˜¯å®šç¾©äº†ç¾ä»£å¤§æ¨¡å‹è¨“ç·´çš„é»ƒé‡‘æ¨™æº–æµç¨‹ï¼šSFT -> RM -> PPOã€‚</p>
        </div>

        <section class="section-block">
            <h2>é€™å¼µåœ–å®šç¾©äº†æœªä¾†å¹¾å¹´çš„ AI ç™¼å±•</h2>
            
            <!-- åŸæ–‡æ–¹æ³•è«– -->
            <div class="original-text">
                <h3>ğŸ“– Method: High-level Methodology (åŸæ–‡å®Œæ•´æ®µè½)</h3>
                <blockquote>
                    <span data-translation="æˆ‘å€‘çš„æ–¹æ³•éµå¾ª Ziegler ç­‰äººï¼ˆ2019ï¼‰å’Œ Stiennon ç­‰äººï¼ˆ2020ï¼‰çš„æ–¹æ³•">Our methodology follows that of Ziegler et al. (2019) and Stiennon et al. (2020), who applied it in the stylistic continuation and summarization domains.</span>
                    <span data-translation="æˆ‘å€‘å¾ä¸€å€‹é è¨“ç·´èªè¨€æ¨¡å‹ã€ä¸€çµ„æç¤ºåˆ†å¸ƒï¼Œä»¥åŠä¸€çµ„è¨“ç·´æœ‰ç´ çš„äººé¡æ¨™è¨»è€…é–‹å§‹">We start with a pretrained language model, a distribution of prompts on which we want our model to produce aligned outputs, and a team of trained human labelers.</span>
                    <span data-translation="ç„¶å¾Œæˆ‘å€‘æ‡‰ç”¨ä»¥ä¸‹ä¸‰å€‹æ­¥é©Ÿï¼š">We then apply the following three steps:</span>
                    <br><br>
                    <strong>Step 1: <span data-translation="æ”¶é›†ç¤ºç¯„æ•¸æ“šï¼Œè¨“ç·´ç›£ç£å¼ç­–ç•¥">Collect demonstration data, and train a supervised policy.</span></strong>
                    <span data-translation="æ¨™è¨»è€…åœ¨è¼¸å…¥æç¤ºåˆ†å¸ƒä¸Šæä¾›æœŸæœ›è¡Œç‚ºçš„ç¤ºç¯„">Our labelers provide demonstrations of the desired behavior on the input prompt distribution.</span>
                    <span data-translation="ç„¶å¾Œæˆ‘å€‘ä½¿ç”¨ç›£ç£å¼å­¸ç¿’åœ¨é€™äº›æ•¸æ“šä¸Šå¾®èª¿é è¨“ç·´çš„ GPT-3 æ¨¡å‹">We then fine-tune a pretrained GPT-3 model on this data using supervised learning.</span>
                    <br><br>
                    <strong>Step 2: <span data-translation="æ”¶é›†æ¯”è¼ƒæ•¸æ“šï¼Œè¨“ç·´çå‹µæ¨¡å‹">Collect comparison data, and train a reward model.</span></strong>
                    <span data-translation="æˆ‘å€‘æ”¶é›†æ¨¡å‹è¼¸å‡ºä¹‹é–“çš„æ¯”è¼ƒæ•¸æ“šé›†ï¼Œæ¨™è¨»è€…æŒ‡å‡ºå°æ–¼çµ¦å®šè¼¸å…¥ä»–å€‘æ›´åå¥½å“ªå€‹è¼¸å‡º">We collect a dataset of comparisons between model outputs, where labelers indicate which output they prefer for a given input.</span>
                    <span data-translation="ç„¶å¾Œæˆ‘å€‘è¨“ç·´çå‹µæ¨¡å‹ä¾†é æ¸¬äººé¡åå¥½çš„è¼¸å‡º">We then train a reward model to predict the human-preferred output.</span>
                    <br><br>
                    <strong>Step 3: <span data-translation="ä½¿ç”¨ PPO é‡å°çå‹µæ¨¡å‹å„ªåŒ–ç­–ç•¥">Optimize a policy against the reward model using PPO.</span></strong>
                    <span data-translation="æˆ‘å€‘ä½¿ç”¨ RM çš„è¼¸å‡ºä½œç‚ºæ¨™é‡çå‹µ">We use the output of the RM as a scalar reward.</span>
                    <span data-translation="æˆ‘å€‘ä½¿ç”¨ PPO æ¼”ç®—æ³•å¾®èª¿ç›£ç£å¼ç­–ç•¥ä»¥å„ªåŒ–é€™å€‹çå‹µ">We fine-tune the supervised policy to optimize this reward using the PPO algorithm.</span>
                    <br><br>
                    <span data-translation="æ­¥é©Ÿ 2 å’Œ 3 å¯ä»¥æŒçºŒè¿­ä»£ï¼›åœ¨ç•¶å‰æœ€ä½³ç­–ç•¥ä¸Šæ”¶é›†æ›´å¤šæ¯”è¼ƒæ•¸æ“šï¼Œç”¨æ–¼è¨“ç·´æ–°çš„ RM ç„¶å¾Œæ–°çš„ç­–ç•¥">Steps 2 and 3 can be iterated continuously; more comparison data is collected on the current best policy, which is used to train a new RM and then a new policy.</span>
                </blockquote>
            </div>
            
            <div class="figure figure-original">
                <img src="images/original/InstructGPT_Diagram3.1.png" alt="InstructGPT ä¸‰éšæ®µè¨“ç·´æµç¨‹åœ– (Figure 2)" style="max-width: 100%; height: auto; border-radius: var(--radius-md); box-shadow: var(--shadow-lg);" />
                <div class="caption">
                    <strong>Figure 2 (åŸæ–‡ Figure 2):</strong> InstructGPT çš„ä¸‰éšæ®µè¨“ç·´æµç¨‹åœ–ã€‚<br>
                    é€™å¼µåœ–éå¸¸é‡è¦ï¼Œå¹¾ä¹æ‰€æœ‰å¾Œä¾†çš„æ¨¡å‹ï¼ˆLLaMA, Claude ç­‰ï¼‰éƒ½éµå¾ªé€™å€‹æ¶æ§‹ã€‚
                </div>
            </div>

            <!-- AI ç”Ÿæˆæ¼«ç•«ï¼šæ›´è¦ªæ°‘çš„ç‰ˆæœ¬ -->
            <div class="figure figure-ai">
                <img src="../images/user_generate_image_20260101164104_86dd.png" alt="RLHF Training Comic" style="max-width: 100%; border-radius: var(--radius-md); box-shadow: var(--shadow-lg);">
                <div class="caption">
                    ğŸ’¡ <strong>AI æ¼«ç•«ï¼šRLHF è¨“ç·´æµç¨‹ï¼ˆè¦ªæ°‘ç‰ˆï¼‰</strong>
                </div>
            </div>

            <div class="analogy">
                <h4>ğŸ’¡ ç”Ÿæ´»é¡æ¯”ï¼šè¨“ç·´è¶…ç´šæ­Œæ˜Ÿ (The Voice)</h4>
                <p>æˆ‘å€‘å¯ä»¥æŠŠè¨“ç·´ AI æ¯”å–»æˆåŸ¹é¤Šä¸€å€‹è¶…ç´šæ­Œæ‰‹ï¼š</p>
                <ul>
                    <li><strong>Step 1 (SFT): å°å¸«ç¤ºç¯„</strong><br>
                    äººé¡å°å¸«è¦ªè‡ªå”±ä¸€éï¼ˆå¯«å‡ºæ­£ç¢ºç­”æ¡ˆï¼‰ï¼ŒAI æ­Œæ‰‹è·Ÿè‘—å­¸ã€‚é€™æ˜¯æ¨¡ä»¿å­¸ç¿’ã€‚</li>
                    
                    <li><strong>Step 2 (RM): è©•å¯©è©•åˆ†</strong><br>
                    AI å”±å¥½å¹¾ç¨®ç‰ˆæœ¬ï¼Œäººé¡è©•å¯©ä¸å”±äº†ï¼Œåªè² è²¬æ’åï¼ˆA æ¯” B å¥½ï¼‰ã€‚æˆ‘å€‘è¨“ç·´ä¸€å€‹ã€ŒAI è©•å¯©ã€ä¾†æ¨¡ä»¿äººé¡çš„å£å‘³ã€‚</li>
                    
                    <li><strong>Step 3 (PPO): è‡ªæˆ‘ä¿®ç…‰</strong><br>
                    AI æ­Œæ‰‹è‡ªå·±é—œåœ¨æˆ¿é–“ç·´å”±ï¼Œå”±å®Œçµ¦ã€ŒAI è©•å¯©ã€è½ï¼Œå¦‚æœåˆ†æ•¸é«˜å°±ä¿æŒï¼Œåˆ†æ•¸ä½å°±ä¿®æ­£ã€‚é€™æ¨£å¯ä»¥ç„¡é™ç·´ç¿’ï¼</li>
                </ul>
            </div>
        </section>

        <section class="section-block">
            <h2>ç¬¬ä¸€æ­¥ï¼šç›£ç£å¼å¾®èª¿ (Supervised Fine-Tuning, SFT)</h2>
            
            <div class="text-pair">
                <div class="original-text">
                    Step 1: Collect demonstration data, and train a supervised policy. Our labelers provide demonstrations of the desired behavior on the input prompt distribution... We then fine-tune a pretrained GPT-3 model on this data using supervised learning.
                </div>
                <div class="translation">
                    æ­¥é©Ÿ 1ï¼šæ”¶é›†ç¤ºç¯„è³‡æ–™ï¼Œä¸¦è¨“ç·´ç›£ç£å¼ç­–ç•¥ã€‚æˆ‘å€‘çš„æ¨™è¨»è€…é‡å°è¼¸å…¥æç¤ºæä¾›æœŸæœ›è¡Œç‚ºçš„ç¤ºç¯„... ç„¶å¾Œæˆ‘å€‘åˆ©ç”¨é€™äº›è³‡æ–™é€éç›£ç£å¼å­¸ç¿’å¾®èª¿ GPT-3ã€‚
                </div>
            </div>

            <div class="explanation">
                <h4>ğŸ” æ·±åº¦è§£æ</h4>
                <p>é€™æ˜¯æœ€ç›´è¦ºçš„ä¸€æ­¥ã€‚GPT-3 åŸæœ¬åªæœƒæ¥é¾ï¼Œç¾åœ¨æˆ‘å€‘çµ¦å®ƒçœ‹äººé¡æ˜¯æ€éº¼å›ç­”å•é¡Œçš„ã€‚</p>
                <ul>
                    <li><strong>Input:</strong> "è«‹å‘å…­æ­²å°å­©è§£é‡‹ç™»é™¸æœˆçƒã€‚"</li>
                    <li><strong>Human Output:</strong> "å¾ˆä¹…ä»¥å‰ï¼Œäººé¡åè‘—å¤§ç«ç®­é£›åˆ°äº†æœˆäº®ä¸Š..."</li>
                </ul>
                <p>æ¨¡å‹é€éå­¸ç¿’é€™äº›æˆå°çš„è³‡æ–™ï¼Œå­¸æœƒäº†ã€Œå°è©±çš„æ ¼å¼ã€ã€‚</p>
                <p><strong>ç¼ºé»ï¼š</strong> äººé¡å¯«ç­”æ¡ˆå¾ˆè²´ï¼Œå¾ˆæ…¢ï¼Œå¾ˆé›£å¤§è¦æ¨¡ç”Ÿç”¢ã€‚</p>
            </div>
        </section>

        <section class="section-block">
            <h2>ç¬¬äºŒæ­¥ï¼šçå‹µæ¨¡å‹ (Reward Model, RM)</h2>
            
            <div class="text-pair">
                <div class="original-text">
                    Step 2: Collect comparison data, and train a reward model. We collect a dataset of comparisons between model outputs, where labelers indicate which output they prefer... We then train a reward model to predict the human-preferred output.
                </div>
                <div class="translation">
                    æ­¥é©Ÿ 2ï¼šæ”¶é›†æ¯”è¼ƒè³‡æ–™ï¼Œä¸¦è¨“ç·´çå‹µæ¨¡å‹ã€‚æˆ‘å€‘æ”¶é›†æ¨¡å‹è¼¸å‡ºçš„æ¯”è¼ƒè³‡æ–™ï¼Œæ¨™è¨»è€…æœƒæŒ‡å‡ºä»–å€‘æ›´å–œæ­¡å“ªä¸€å€‹è¼¸å‡º... ç„¶å¾Œæˆ‘å€‘è¨“ç·´ä¸€å€‹çå‹µæ¨¡å‹ä¾†é æ¸¬äººé¡åå¥½çš„è¼¸å‡ºã€‚
                </div>
            </div>

            <div class="key-concept">
                <h4>ç‚ºä»€éº¼è¦ã€Œæ¯”è¼ƒã€è€Œä¸æ˜¯ã€Œæ‰“åˆ†ã€ï¼Ÿ</h4>
                <p>è«–æ–‡ç™¼ç¾ï¼Œè®“äººç›´æ¥çµ¦ 1-10 åˆ†å¾ˆé›£çµ±ä¸€æ¨™æº–ï¼ˆä½ çš„ 7 åˆ†å¯èƒ½æ˜¯æˆ‘çš„ 5 åˆ†ï¼‰ã€‚</p>
                <p>ä½†æ˜¯ï¼Œè®“äººåˆ¤æ–· <strong>ã€ŒA æ¯” B å¥½ã€</strong> å»éå¸¸å®¹æ˜“ä¸”ä¸€è‡´ã€‚é€™å°±æ˜¯ <strong>Ranking (æ’å)</strong> çš„åŠ›é‡ã€‚</p>
            </div>

            <div class="math-block">
                <h4>ğŸ“ æ•¸å­¸å…¬å¼ï¼šLoss Function</h4>
                <p>çå‹µæ¨¡å‹çš„è¨“ç·´ç›®æ¨™æ˜¯æœ€å°åŒ–ä»¥ä¸‹ Lossï¼š</p>
                <p>\[ \text{loss}(\theta) = - \mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log(\sigma(r_\theta(x, y_w) - r_\theta(x, y_l))) \right] \]</p>
                <ul>
                    <li>\(r_\theta(x, y)\)ï¼šçå‹µæ¨¡å‹å°è¼¸å…¥ \(x\) å’Œå›ç­” \(y\) çš„è©•åˆ†ã€‚</li>
                    <li>\(y_w\)ï¼šå‹å‡ºçš„å›ç­” (Winner)ã€‚</li>
                    <li>\(y_l\)ï¼šè½æ•—çš„å›ç­” (Loser)ã€‚</li>
                    <li><strong>ç›´è§€æ„ç¾©ï¼š</strong> æˆ‘å€‘å¸Œæœ› Winner çš„åˆ†æ•¸æ¯” Loser çš„åˆ†æ•¸é«˜è¶Šå¤šè¶Šå¥½ï¼ˆè®“å·®å€¼ç¶“é Sigmoid å¾Œæ¥è¿‘ 1ï¼‰ã€‚</li>
                </ul>
            </div>
        </section>

        <section class="section-block">
            <h2>ç¬¬ä¸‰æ­¥ï¼šå¼·åŒ–å­¸ç¿’ (PPO)</h2>
            
            <div class="text-pair">
                <div class="original-text">
                    Step 3: Optimize a policy against the reward model using PPO. We use the output of the RM as a scalar reward. We fine-tune the supervised policy to optimize this reward...
                </div>
                <div class="translation">
                    æ­¥é©Ÿ 3ï¼šä½¿ç”¨ PPO é‡å°çå‹µæ¨¡å‹å„ªåŒ–ç­–ç•¥ã€‚æˆ‘å€‘å°‡çå‹µæ¨¡å‹çš„è¼¸å‡ºä½œç‚ºç´”é‡çå‹µã€‚æˆ‘å€‘å¾®èª¿ç›£ç£å¼ç­–ç•¥ä»¥æœ€å¤§åŒ–æ­¤çå‹µ...
                </div>
            </div>

            <div class="explanation">
                <h4>ğŸ” æ·±åº¦è§£æï¼šç‚ºä»€éº¼éœ€è¦ PPOï¼Ÿ</h4>
                <p>æœ‰äº† RM (AI è©•å¯©)ï¼Œæˆ‘å€‘å°±æœ‰äº†ä¸€å€‹ä¸éœ€è¦äººé¡ä»‹å…¥ä¹Ÿèƒ½å¿«é€Ÿæ‰“åˆ†çš„æ©Ÿåˆ¶ã€‚PPO (Proximal Policy Optimization) æ˜¯ä¸€ç¨®å¼·åŒ–å­¸ç¿’æ¼”ç®—æ³•ï¼Œå®ƒçš„ä½œç”¨æ˜¯è®“æ¨¡å‹ä¸æ–·å˜—è©¦ç”Ÿæˆæ–°çš„å›ç­”ï¼Œæƒ³è¾¦æ³•é¨™é RM æ‹¿åˆ°é«˜åˆ†ã€‚</p>
                <p><strong>æ³¨æ„ï¼š</strong> ç‚ºäº†é˜²æ­¢æ¨¡å‹ç‚ºäº†æ‹¿é«˜åˆ†è€Œã€Œèµ°ç«å…¥é­”ã€ï¼ˆä¾‹å¦‚ä¸€ç›´é‡è¤‡æŸå€‹é«˜åˆ†é—œéµå­—ï¼‰ï¼ŒLoss function ä¸­é€šå¸¸æœƒåŠ å…¥ä¸€å€‹ KL Divergence æ‡²ç½°é …ï¼Œé™åˆ¶å®ƒä¸èƒ½åé›¢åŸå§‹æ¨¡å‹å¤ªé ã€‚</p>
            </div>
        </section>

        <div class="nav-bar">
            <a href="01-introduction.html" class="nav-btn">â† ä¸Šä¸€é ï¼šå¼•è¨€</a>
            <a href="index.html" class="nav-btn">ç›®éŒ„</a>
            <a href="03-results.html" class="nav-btn primary">ä¸‹ä¸€é ï¼šä¸»è¦çµæœ â†’</a>
        </div>
    </div>
</body>
</html>

