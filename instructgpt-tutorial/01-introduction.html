<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬ 1 é ï¼šç‚ºä»€éº¼éœ€è¦ InstructGPT? - InstructGPT è«–æ–‡æ·±åº¦è§£æ</title>
    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <!-- Breadcrumb -->
        <div class="breadcrumb">
            <a href="index.html">InstructGPT æ·±åº¦è§£æ</a>
            <span>/</span>
            <span class="current">01. ç°¡ä»‹èˆ‡å‹•æ©Ÿ</span>
        </div>

        <h1>ç¬¬ 1 é ï¼šç‚ºä»€éº¼ GPT-3 éœ€è¦ã€Œä¸Šå­¸ã€?</h1>
        
        <div class="header-section">
            <p class="lead">GPT-3 é›–ç„¶è®€éäº†ç¶²éš›ç¶²è·¯ï¼Œä½†å®ƒå…¶å¯¦ä¸æ‡‚ä½ åœ¨èªªä»€éº¼ã€‚å®ƒåªæœƒã€Œæ¥é¾ã€ã€‚é€™ç¯‡è«–æ–‡å°±æ˜¯è¦è§£æ±ºé€™å€‹å•é¡Œã€‚</p>
            <div class="paper-meta">
                <div class="meta-item">
                    <strong>è«–æ–‡æ¨™é¡Œ</strong>
                    Training language models to follow instructions with human feedback
                </div>
                <div class="meta-item">
                    <strong>ç™¼è¡¨å¹´ä»½</strong>
                    2022 (NeurIPS)
                </div>
                <div class="meta-item">
                    <strong>é—œéµè²¢ç»</strong>
                    RLHF (äººé¡å›é¥‹å¼·åŒ–å­¸ç¿’)
                </div>
            </div>
        </div>

        <section class="section-block">
            <h2>ğŸ“„ Abstract (æ‘˜è¦å®Œæ•´åŸæ–‡)</h2>
            
            <div class="text-pair">
                <div class="original-text">
                    <blockquote>
                        <span data-translation="æŠŠèªè¨€æ¨¡å‹åšå¾—æ›´å¤§ï¼Œä¸¦ä¸ä»£è¡¨å®ƒå°±æ›´æœƒéµå¾ªä½¿ç”¨è€…çš„æ„åœ–">Making language models bigger does not inherently make them better at following a user's intent.</span>
                        <span data-translation="ä¾‹å¦‚ï¼Œå¤§å‹èªè¨€æ¨¡å‹å¯èƒ½æœƒç”¢ç”Ÿä¸çœŸå¯¦ã€æœ‰æ¯’ï¼Œæˆ–æ ¹æœ¬å°ä½¿ç”¨è€…æ²’å¹«åŠ©çš„è¼¸å‡º">For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user.</span>
                        <span data-translation="æ›å¥è©±èªªï¼Œé€™äº›æ¨¡å‹èˆ‡ä½¿ç”¨è€…ä¸¦æœªã€Œå°é½Šã€">In other words, these models are not <em>aligned</em> with their users.</span>
                        <br><br>
                        <span data-translation="åœ¨æœ¬è«–æ–‡ä¸­ï¼Œæˆ‘å€‘å±•ç¤ºäº†ä¸€ç¨®é€éäººé¡å›é¥‹å¾®èª¿ä¾†è®“èªè¨€æ¨¡å‹èˆ‡ä½¿ç”¨è€…æ„åœ–å°é½Šçš„æ–¹æ³•">In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.</span>
                        <span data-translation="æˆ‘å€‘æ”¶é›†æ¨™è¨»è€…ç¤ºç¯„çš„æœŸæœ›æ¨¡å‹è¡Œç‚ºè³‡æ–™é›†ï¼Œç”¨æ–¼é€éç›£ç£å¼å­¸ç¿’å¾®èª¿ GPT-3">Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning.</span>
                        <br><br>
                        <span data-translation="æ¥è‘—æˆ‘å€‘æ”¶é›†æ¨¡å‹è¼¸å‡ºçš„æ’åºè³‡æ–™é›†ï¼Œç”¨æ–¼é€éäººé¡å›é¥‹å¼·åŒ–å­¸ç¿’é€²ä¸€æ­¥å¾®èª¿">We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback.</span>
                        <span data-translation="æˆ‘å€‘å°‡æˆæœæ¨¡å‹ç¨±ç‚º InstructGPT">We call the resulting models <em>InstructGPT</em>.</span>
                        <br><br>
                        <span data-translation="1.3B åƒæ•¸çš„ InstructGPT æ¯” 175B GPT-3 æ›´å—åå¥½ï¼Œå„˜ç®¡åƒæ•¸æ•¸é‡å°‘äº† 100 å€">In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.</span>
                        <span data-translation="InstructGPT åœ¨çœŸå¯¦æ€§ä¸Šæœ‰æ‰€æå‡ï¼Œæœ‰æ¯’è¼¸å‡ºç”Ÿæˆæ¸›å°‘ï¼ŒåŒæ™‚åœ¨å…¬é–‹ NLP è³‡æ–™é›†ä¸Šçš„æ€§èƒ½é€€åŒ–æ¥µå°">Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets.</span>
                        <br><br>
                        <span data-translation="å„˜ç®¡ InstructGPT ä»æœƒçŠ¯ç°¡å–®éŒ¯èª¤ï¼Œæˆ‘å€‘çš„çµæœè¡¨æ˜ï¼Œä½¿ç”¨äººé¡å›é¥‹é€²è¡Œå¾®èª¿æ˜¯è®“èªè¨€æ¨¡å‹èˆ‡äººé¡æ„åœ–å°é½Šçš„æœ‰å‰æ™¯æ–¹å‘">Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.</span>
                    </blockquote>
                </div>
                <div class="translation">
                    <h3>ğŸŒ å®Œæ•´ç¿»è­¯</h3>
                    <p>æŠŠèªè¨€æ¨¡å‹åšå¾—æ›´å¤§ï¼Œä¸¦ä¸ä»£è¡¨å®ƒå°±æ›´æœƒéµå¾ªä½¿ç”¨è€…çš„æ„åœ–ã€‚ä¾‹å¦‚ï¼Œå¤§å‹èªè¨€æ¨¡å‹å¯èƒ½æœƒç”¢ç”Ÿä¸çœŸå¯¦ã€æœ‰æ¯’ï¼Œæˆ–æ ¹æœ¬å°ä½¿ç”¨è€…æ²’å¹«åŠ©çš„è¼¸å‡ºã€‚æ›å¥è©±èªªï¼Œé€™äº›æ¨¡å‹èˆ‡ä½¿ç”¨è€…ä¸¦æœªã€Œå°é½Šã€ã€‚</p>
                    <p>åœ¨æœ¬è«–æ–‡ä¸­ï¼Œæˆ‘å€‘å±•ç¤ºäº†ä¸€ç¨®é€éäººé¡å›é¥‹å¾®èª¿ä¾†è®“èªè¨€æ¨¡å‹èˆ‡ä½¿ç”¨è€…æ„åœ–å°é½Šçš„æ–¹æ³•ã€‚æˆ‘å€‘å¾æ¨™è¨»è€…æ’°å¯«çš„æç¤ºèˆ‡ OpenAI API æäº¤çš„æç¤ºé–‹å§‹ï¼Œæ”¶é›†æ¨™è¨»è€…ç¤ºç¯„çš„æœŸæœ›æ¨¡å‹è¡Œç‚ºè³‡æ–™é›†ï¼Œç”¨æ–¼é€éç›£ç£å¼å­¸ç¿’å¾®èª¿ GPT-3ã€‚æ¥è‘—æˆ‘å€‘æ”¶é›†æ¨¡å‹è¼¸å‡ºçš„æ’åºè³‡æ–™é›†ï¼Œç”¨æ–¼é€éäººé¡å›é¥‹å¼·åŒ–å­¸ç¿’é€²ä¸€æ­¥å¾®èª¿é€™å€‹ç›£ç£å¼æ¨¡å‹ã€‚æˆ‘å€‘å°‡æˆæœæ¨¡å‹ç¨±ç‚º InstructGPTã€‚</p>
                    <p>åœ¨æˆ‘å€‘çš„æç¤ºåˆ†å¸ƒä¸Šé€²è¡Œçš„äººé¡è©•ä¼°ä¸­ï¼Œ1.3B åƒæ•¸çš„ InstructGPT æ¨¡å‹çš„è¼¸å‡ºæ¯” 175B GPT-3 çš„è¼¸å‡ºæ›´å—åå¥½ï¼Œå„˜ç®¡åƒæ•¸æ•¸é‡å°‘äº† 100 å€ã€‚æ­¤å¤–ï¼ŒInstructGPT æ¨¡å‹åœ¨çœŸå¯¦æ€§ä¸Šæœ‰æ‰€æå‡ï¼Œæœ‰æ¯’è¼¸å‡ºç”Ÿæˆæ¸›å°‘ï¼ŒåŒæ™‚åœ¨å…¬é–‹ NLP è³‡æ–™é›†ä¸Šçš„æ€§èƒ½é€€åŒ–æ¥µå°ã€‚å„˜ç®¡ InstructGPT ä»æœƒçŠ¯ç°¡å–®éŒ¯èª¤ï¼Œæˆ‘å€‘çš„çµæœè¡¨æ˜ï¼Œä½¿ç”¨äººé¡å›é¥‹é€²è¡Œå¾®èª¿æ˜¯è®“èªè¨€æ¨¡å‹èˆ‡äººé¡æ„åœ–å°é½Šçš„æœ‰å‰æ™¯æ–¹å‘ã€‚</p>
                </div>
            </div>
        </section>

        <section class="section-block">
            <h2>1. å•é¡Œï¼šå¤§ä¸ä¸€å®šå¥½ (Bigger is not better)</h2>
            
            <div class="text-pair">
                <div class="original-text">
                    Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not <em>aligned</em> with their users.
                </div>
                <div class="translation">
                    æŠŠèªè¨€æ¨¡å‹åšå¾—æ›´å¤§ï¼Œä¸¦ä¸ä»£è¡¨å®ƒå°±æ›´è½è©±ã€‚ä¾‹å¦‚ï¼Œå¤§å‹èªè¨€æ¨¡å‹å¯èƒ½æœƒç”¢ç”Ÿä¸çœŸå¯¦ã€æœ‰æ¯’ï¼Œæˆ–æ ¹æœ¬å°ä½¿ç”¨è€…æ²’å¹«åŠ©çš„å›ç­”ã€‚æ›å¥è©±èªªï¼Œé€™äº›æ¨¡å‹èˆ‡ä½¿ç”¨è€…çš„æ„åœ–ä¸¦æœªã€Œå°é½Š (Aligned)ã€ã€‚
                </div>
            </div>

            <div class="explanation">
                <h4>ğŸ” æ·±åº¦è§£æï¼šä»€éº¼æ˜¯ Alignment (å°é½Š)?</h4>
                <p>åœ¨ AI é ˜åŸŸï¼Œã€Œå°é½Šã€æ˜¯æŒ‡è®“ AI çš„ç›®æ¨™èˆ‡äººé¡çš„ç›®æ¨™ä¸€è‡´ã€‚GPT-3 çš„è¨“ç·´ç›®æ¨™åªæœ‰ä¸€å€‹ï¼š<strong>é æ¸¬ä¸‹ä¸€å€‹å­— (Next Token Prediction)</strong>ã€‚</p>
                <p>é€™å°è‡´äº†ä¸€å€‹åš´é‡çš„è„«ç¯€ï¼š</p>
                <ul>
                    <li><strong>ä½¿ç”¨è€…çš„ç›®æ¨™ï¼š</strong> "å¹«æˆ‘å¯«ä¸€å°é“æ­‰ä¿¡ã€‚"</li>
                    <li><strong>GPT-3 çš„ç›®æ¨™ï¼š</strong> "é€™å¥è©±å¾Œé¢é€šå¸¸æ¥ä»€éº¼ï¼Ÿ" (å®ƒå¯èƒ½æœƒæ¥è‘—å¯« "é‚„æœ‰ä¸€å°æ˜¯çµ¦è€é—†çš„..." æˆ–æ˜¯ç·¨é€ ä¸€å€‹æ•…äº‹ï¼Œè€Œä¸æ˜¯çœŸçš„å¯«ä¿¡)</li>
                </ul>
            </div>

            <div class="analogy">
                <h4>ğŸ’¡ ç”Ÿæ´»é¡æ¯”ï¼šç‹‚é‡çš„å¤©æ‰ vs. è¨“ç·´æœ‰ç´ çš„ç®¡å®¶</h4>
                <p><strong>GPT-3 å°±åƒä¸€å€‹ä½åœ¨æ·±å±±è£¡çš„ç‹‚é‡å¤©æ‰ï¼š</strong></p>
                <ul>
                    <li>ä»–è®€éåœ–æ›¸é¤¨è£¡æ‰€æœ‰çš„æ›¸ï¼ŒçŸ¥è­˜æ·µåšã€‚</li>
                    <li>ä½†å¦‚æœä½ å•ä»–ï¼šã€Œè«‹å¹«æˆ‘æ‹¿ä¸€æ¯æ°´ã€‚ã€</li>
                    <li>ä»–å¯èƒ½æœƒå›ç­”ï¼šã€Œæ°´æ˜¯ä¸€ç¨®åŒ–å­¸ç‰©è³ªï¼Œåˆ†å­å¼ H2O...ã€ç„¶å¾Œç¹¼çºŒè¬›æ°´çš„æ­·å²ï¼Œå®Œå…¨ç„¡è¦–ä½ å£æ¸´çš„äº‹å¯¦ã€‚</li>
                </ul>
                <p><strong>InstructGPT å°±åƒæ˜¯æŠŠé€™ä½å¤©æ‰é€å»ã€Œç¦®å„€å­¸æ ¡ã€ï¼š</strong></p>
                <ul>
                    <li>ä»–å­¸æœƒäº†è½æ‡‚å‘½ä»¤ã€‚</li>
                    <li>ç•¶ä½ èªªã€Œæ‹¿æ°´ã€æ™‚ï¼Œä»–çŸ¥é“ä½ è¦çš„æ˜¯è¡Œå‹•ï¼Œè€Œä¸æ˜¯å®šç¾©ã€‚</li>
                    <li>é€™å°±æ˜¯ <strong>Instruction Tuning (æŒ‡ä»¤å¾®èª¿)</strong> çš„æ„ç¾©ã€‚</li>
                </ul>
            </div>

            <div class="analogy">
                <h4>ğŸ”§ å·¥ç¨‹é¡æ¯”ï¼šUnit Tests vs. Acceptance Tests</h4>
                <p>å°æ–¼è»Ÿé«”å·¥ç¨‹å¸«ä¾†èªªï¼š</p>
                <ul>
                    <li><strong>GPT-3 (Pre-training) é€šéäº† Unit Testsï¼š</strong> æ¯å€‹å‡½æ•¸ (Token) éƒ½èƒ½è·‘ï¼Œæ²’æœ‰èªæ³•éŒ¯èª¤ï¼Œé‚è¼¯è‡ªæ´½ã€‚</li>
                    <li><strong>InstructGPT (Alignment) é€šéäº† Acceptance Tests (UAT)ï¼š</strong> ç”¨æˆ¶å¯¦éš›ä½¿ç”¨æ™‚ï¼ŒåŠŸèƒ½ç¬¦åˆéœ€æ±‚ï¼Œé«”é©—è‰¯å¥½ã€‚</li>
                </ul>
                <p>ä¸€å€‹é€šéæ‰€æœ‰å–®å…ƒæ¸¬è©¦çš„è»Ÿé«”ï¼Œå¯èƒ½æ ¹æœ¬ä¸æ˜¯ç”¨æˆ¶æƒ³è¦çš„åŠŸèƒ½ã€‚Alignment å°±æ˜¯åœ¨åš UATã€‚</p>
            </div>
        </section>

        <section class="section-block">
            <h2>2. è§£æ±ºæ–¹æ¡ˆï¼šRLHF</h2>
            
            <div class="text-pair">
                <div class="original-text">
                    In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback... We call the resulting models <em>InstructGPT</em>.
                </div>
                <div class="translation">
                    åœ¨æœ¬è«–æ–‡ä¸­ï¼Œæˆ‘å€‘å±•ç¤ºäº†ä¸€ç¨®é€éäººé¡å›é¥‹å¾®èª¿ (Fine-tuning with Human Feedback) ä¾†è®“èªè¨€æ¨¡å‹èˆ‡ä½¿ç”¨è€…æ„åœ–å°é½Šçš„æ–¹æ³•... æˆ‘å€‘å°‡æˆæœæ¨¡å‹ç¨±ç‚º <em>InstructGPT</em>ã€‚
                </div>
            </div>

            <div class="key-concept">
                <h4>ğŸ¯ æ ¸å¿ƒæŠ€è¡“ï¼šRLHF</h4>
                <p>å…¨åæ˜¯ <strong>Reinforcement Learning from Human Feedback (å¾äººé¡å›é¥‹ä¸­å¼·åŒ–å­¸ç¿’)</strong>ã€‚é€™æ˜¯é€™ç¯‡è«–æ–‡çš„é‡‘é‘°åŒ™ã€‚</p>
                <p>å®ƒå¼•å…¥äº†ã€Œäººé¡è€å¸«ã€ä¾†çµ¦ AI æ‰“åˆ†æ•¸ï¼Œè€Œä¸æ˜¯åªé æ¨™æº–ç­”æ¡ˆã€‚</p>
            </div>

            <!-- AI Generated Concept Image -->
            <div class="figure figure-ai">
                <img src="../images/user_generate_image_20260101163757_aff9.png" alt="Alignment Visualization" style="max-width: 100%; border-radius: var(--radius-md); box-shadow: var(--shadow-lg);">
                <div class="caption">
                    ğŸ’¡ <strong>AI åœ–è§£ï¼šAlignment çš„éç¨‹</strong><br>
                    å·¦é‚Šï¼šGPT-3 åƒå€‹äº‚å°„ç®­çš„é«˜æ‰‹ï¼Œé›–ç„¶æœ‰åŠ›ä½†å°„ä¸æº–é¶å¿ƒã€‚<br>
                    å³é‚Šï¼šRLHF å°±åƒæ˜¯æ•™ç·´åœ¨æ—é‚Šä¿®æ­£ä»–çš„å§¿å‹¢ï¼Œè®“ä»–æ¯æ¬¡éƒ½å°„ä¸­ã€Œäººé¡æ„åœ–ã€çš„ç´…å¿ƒã€‚
                </div>
            </div>
        </section>

        <section class="section-block">
            <h2>3. é©šäººçš„çµæœ</h2>

            <div class="figure figure-original">
                <!-- åŸæ–‡ PDF åœ–ç‰‡é€£çµï¼Œå‡è¨­ä½¿ç”¨è€…æœ‰ viewer æˆ–å·²è½‰æª” -->
                <embed src="images/original/main-graph-no-facets.pdf" type="application/pdf" width="100%" height="500px" />
                <div class="caption">
                    <strong>Figure 1 (åŸæ–‡):</strong> äººé¡è©•ä¼°å‹ç‡åœ–ã€‚<br>
                    è«‹æ³¨æ„çœ‹æœ€å·¦é‚Šçš„é•·æ¢ (InstructGPT 1.3B) æ¯”æœ€å³é‚Šçš„é•·æ¢ (GPT-3 175B) é‚„è¦é«˜ï¼<br>
                    é€™æ„å‘³è‘—ï¼š<strong>ä¸€å€‹ç¸®å°äº† 100 å€çš„æ¨¡å‹ï¼Œç¶“é RLHF å¾Œï¼Œç«Ÿç„¶æ¯”åŸå§‹çš„è¶…å¤§æ¨¡å‹é‚„å¥½ç”¨ã€‚</strong>
                </div>
            </div>

            <div class="analogy">
                <h4>ğŸ¤– AI é«”é©—é€£çµï¼šç‚ºä»€éº¼ ChatGPT é€™éº¼æœ‰ç¦®è²Œï¼Ÿ</h4>
                <p>ä½ æœ‰æ²’æœ‰ç™¼ç¾ï¼Œç„¡è«–ä½ å• ChatGPT å¤šå¥‡æ€ªçš„å•é¡Œï¼Œå®ƒé€šå¸¸éƒ½æœƒå¾ˆå®¢æ°£åœ°å›ç­”ï¼Œæˆ–è€…å§”å©‰åœ°æ‹’çµ•ï¼Ÿ</p>
                <p>é€™å°±æ˜¯ InstructGPT (åŠå¾ŒçºŒçš„ ChatGPT) è¨“ç·´å‡ºä¾†çš„çµæœã€‚å®ƒè¢«è¨“ç·´æˆè¦ç¬¦åˆ <strong>3H åŸå‰‡</strong>ï¼š</p>
                <ul>
                    <li><strong>Helpful (æœ‰å¹«åŠ©)</strong>ï¼šç›¡åŠ›è§£æ±ºå•é¡Œã€‚</li>
                    <li><strong>Honest (èª å¯¦)</strong>ï¼šä¸èªªè¬Š (é›–ç„¶é‚„æ˜¯æœƒæœ‰å¹»è¦º)ã€‚</li>
                    <li><strong>Harmless (ç„¡å®³)</strong>ï¼šä¸ç”¢ç”Ÿæ”»æ“Šæ€§æˆ–å±éšªå…§å®¹ã€‚</li>
                </ul>
                <p><strong>âš ï¸ Reality Check:</strong> é›–ç„¶å®ƒå—éè¨“ç·´ï¼Œä½†ã€Œå¹»è¦ºã€ä»ç„¶å­˜åœ¨ã€‚å®ƒå¯èƒ½é‚„æ˜¯æœƒä¸€æœ¬æ­£ç¶“åœ°èƒ¡èªªå…«é“ï¼Œå› ç‚ºå®ƒçš„æœ¬è³ªä»ç„¶æ˜¯æ©Ÿç‡é æ¸¬ã€‚</p>
            </div>
        </section>

        <section class="section-block">
            <h2>ğŸ“Š æ ¸å¿ƒç™¼ç¾ (Main Findings)</h2>
            
            <div class="key-findings">
                <div class="original-text">
                    <h3>åŸæ–‡ï¼šè«–æ–‡çš„å…­å¤§æ ¸å¿ƒçµè«–</h3>
                    
                    <h4>Finding 1: å°æ¨¡å‹æ‰“æ•—å¤§æ¨¡å‹</h4>
                    <blockquote>
                        <strong>Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.</strong>
                        On our test set, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having over 100x fewer parameters. 
                        Outputs from our 175B InstructGPT are preferred to 175B GPT-3 outputs 85 Â± 3% of the time.
                    </blockquote>
                    
                    <h4>Finding 2: æ›´çœŸå¯¦</h4>
                    <blockquote>
                        <strong>InstructGPT models show improvements in truthfulness over GPT-3.</strong>
                        On the TruthfulQA benchmark, InstructGPT generates truthful and informative answers about twice as often as GPT-3.
                        On closed-domain tasks, InstructGPT models make up information not present in the input about half as often as GPT-3 (a 21% vs. 41% hallucination rate, respectively).
                    </blockquote>
                    
                    <h4>Finding 3: æ¯’æ€§é™ä½</h4>
                    <blockquote>
                        <strong>InstructGPT shows small improvements in toxicity over GPT-3, but not bias.</strong>
                        InstructGPT models generate about 25% fewer toxic outputs than GPT-3 when prompted to be respectful.
                    </blockquote>
                    
                    <h4>Finding 4: å°é½Šç¨… (Alignment Tax)</h4>
                    <blockquote>
                        <strong>We can minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure.</strong>
                        During RLHF fine-tuning, we observe performance regressions compared to GPT-3 on certain public NLP datasets.
                        This is an example of an "alignment tax" since our alignment procedure comes at the cost of lower performance on certain tasks that we may care about.
                    </blockquote>
                    
                    <h4>Finding 5: æ³›åŒ–èƒ½åŠ›</h4>
                    <blockquote>
                        <strong>InstructGPT models show promising generalization to instructions outside of the RLHF fine-tuning distribution.</strong>
                        It is able to follow instructions for summarizing code, answer questions about code, and sometimes follows instructions in different languages, despite these instructions being very rare in the fine-tuning distribution.
                    </blockquote>
                    
                    <h4>Finding 6: ä»æœ‰ç¼ºé™·</h4>
                    <blockquote>
                        <strong>InstructGPT still makes simple mistakes.</strong>
                        For example, InstructGPT can still fail to follow instructions, make up facts, give long hedging answers to simple questions, or fail to detect instructions with false premises.
                    </blockquote>
                </div>
            </div>

            <div class="explanation">
                <h4>ğŸ¯ é€™äº›æ•¸å­—æ„å‘³è‘—ä»€éº¼ï¼Ÿ</h4>
                
                <div class="comparison-grid">
                    <div class="comparison-item">
                        <h5>1ï¸âƒ£ 1.3B æ‰“æ•— 175B</h5>
                        <p><strong>éœ‡æ’¼ç¨‹åº¦ï¼šâ­â­â­â­â­</strong></p>
                        <p>é€™å°±åƒä¸€å€‹å°å­¸ç”Ÿï¼ˆ1.3Bï¼‰æ¯”åšå£«ç”Ÿï¼ˆ175Bï¼‰æ›´æœƒå›ç­”å•é¡Œï¼åƒæ•¸å°‘äº† <strong>100 å€</strong>ï¼Œä½†äººé¡æ›´å–œæ­¡å®ƒçš„ç­”æ¡ˆã€‚</p>
                        <p class="chatgpt-connection">ğŸ’¬ <strong>ChatGPT ç¶“é©—</strong>ï¼šé€™å°±æ˜¯ç‚ºä»€éº¼ ChatGPT ä¸åªæ˜¯æŠŠæ¨¡å‹åšå¤§ï¼Œè€Œæ˜¯è¦å°é½Šã€‚</p>
                    </div>
                    
                    <div class="comparison-item">
                        <h5>2ï¸âƒ£ çœŸå¯¦æ€§æå‡ 2 å€</h5>
                        <p><strong>å¹»è¦ºç‡ï¼šGPT-3 (41%) â†’ InstructGPT (21%)</strong></p>
                        <p>æƒ³åƒä½ å• AI ä¸€å€‹æ­·å²å•é¡Œã€‚GPT-3 æœ‰ 41% çš„æ©Ÿç‡æœƒç·¨æ•…äº‹ï¼ŒInstructGPT åªæœ‰ 21%ã€‚é€²æ­¥äº†ä¸€åŠï¼</p>
                        <p class="chatgpt-connection">ğŸ’¬ <strong>ChatGPT ç¶“é©—</strong>ï¼šä½†ä½ ä»ç„¶æœƒé‡åˆ°å®ƒã€Œä¸€æœ¬æ­£ç¶“åœ°èƒ¡èªªå…«é“ã€çš„æ™‚å€™ï¼Œå°å§ï¼Ÿé€™å°±æ˜¯é‚£ 21%ã€‚</p>
                    </div>
                    
                    <div class="comparison-item">
                        <h5>3ï¸âƒ£ å°é½Šç¨… (Alignment Tax)</h5>
                        <p><strong>ä»£åƒ¹ï¼šå…¬é–‹ NLP è³‡æ–™é›†æ€§èƒ½ä¸‹é™</strong></p>
                        <p>ç‚ºäº†è®“ AIã€Œè½è©±ã€ï¼Œåœ¨æŸäº›å­¸è¡“æ¸¬è©¦é¡Œä¸Šçš„åˆ†æ•¸æœƒé™ä½ã€‚é€™æ˜¯å¿…é ˆä»˜å‡ºçš„ä»£åƒ¹ã€‚</p>
                        <p class="analogy-box">ğŸ—ï¸ <strong>é¡æ¯”</strong>ï¼šå°±åƒè¨“ç·´å»ºç¯‰å·¥äººéµå®ˆå®‰å…¨è¦ç¯„ï¼Œé›–ç„¶è“‹æˆ¿å­é€Ÿåº¦è®Šæ…¢äº†ï¼Œä½†æ›´å®‰å…¨ï¼</p>
                    </div>
                    
                    <div class="comparison-item">
                        <h5>4ï¸âƒ£ 85% å‹ç‡</h5>
                        <p><strong>175B InstructGPT vs 175B GPT-3ï¼š85% åå¥½ç‡</strong></p>
                        <p>åŒæ¨£å¤§å°çš„æ¨¡å‹ï¼Œåªæ˜¯è¨“ç·´æ–¹æ³•ä¸åŒï¼Œäººé¡å°±å£“å€’æ€§åœ°åå¥½ InstructGPTã€‚</p>
                        <p class="chatgpt-connection">ğŸ’¬ <strong>ChatGPT ç¶“é©—</strong>ï¼šé€™å°±æ˜¯ç‚ºä»€éº¼ ChatGPT æ¯”ã€ŒåŸå‘³ GPT-3ã€å¥½ç”¨å¤ªå¤šçš„åŸå› ï¼</p>
                    </div>
                </div>
            </div>
        </section>

        <div class="nav-bar">
            <a href="index.html" class="nav-btn">â† å›ç›®éŒ„</a>
            <a href="02-methodology.html" class="nav-btn primary">ä¸‹ä¸€é ï¼šè¨“ç·´ä¸‰éƒ¨æ›² (RLHF) â†’</a>
        </div>
    </div>
</body>
</html>

