<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Primary Meta Tags -->
    <title>Hope 模型與實驗結果 - Nested Learning 論文導讀 | AI Paper Archaeology</title>
    <meta
      name="description"
      content="實戰驗證：Self-Modifying Titans 與 Hope 模型。在持續學習、長上下文理解與語言建模任務上的表現如何？深入解析 Hope 模型在 CLINC、Banking、DBpedia、BABILong 等基準測試中的優異表現。"
    />
    <meta
      name="keywords"
      content="Hope 模型, Self-Modifying Titans, 持續學習, Continual Learning, 長上下文理解, Long Context, BABILong, CLINC, 實驗結果, Nested Learning"
    />
    <meta name="author" content="謝承緯 (Chen Wei Hsieh)" />
    <meta name="robots" content="index, follow" />
    <link
      rel="canonical"
      href="https://hsiehchenwei.github.io/ai-paper-archaeology/nested-learning-tutorial/06-hope-experiments.html"
    />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article" />
    <meta
      property="og:url"
      content="https://hsiehchenwei.github.io/ai-paper-archaeology/nested-learning-tutorial/06-hope-experiments.html"
    />
    <meta
      property="og:title"
      content="Hope 模型與實驗結果 - Nested Learning 論文導讀"
    />
    <meta
      property="og:description"
      content="實戰驗證：Self-Modifying Titans 與 Hope 模型。在持續學習、長上下文理解與語言建模任務上的表現如何？"
    />
    <meta
      property="og:image"
      content="https://hsiehchenwei.github.io/ai-paper-archaeology/nested-learning-tutorial/images/generated/hero-06-hope.png"
    />
    <meta property="og:locale" content="zh_TW" />
    <meta property="og:site_name" content="AI Paper Archaeology" />

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image" />
    <meta
      property="twitter:url"
      content="https://hsiehchenwei.github.io/ai-paper-archaeology/nested-learning-tutorial/06-hope-experiments.html"
    />
    <meta
      property="twitter:title"
      content="Hope 模型與實驗結果 - Nested Learning 論文導讀"
    />
    <meta
      property="twitter:description"
      content="實戰驗證：Self-Modifying Titans 與 Hope 模型。在持續學習、長上下文理解與語言建模任務上的表現如何？"
    />
    <meta
      property="twitter:image"
      content="https://hsiehchenwei.github.io/ai-paper-archaeology/nested-learning-tutorial/images/generated/hero-06-hope.png"
    />

    <link rel="stylesheet" href="../styles/global.css">
    <link rel="stylesheet" href="../styles/paper-reading.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@400;700&family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <!-- Breadcrumb -->
        <div class="breadcrumb">
            <a href="../index.html">🏠 首頁</a>
            <span>/</span>
            <a href="index.html">🔗 Nested Learning</a>
            <span>/</span>
            <span class="current">06 Hope 模型與實驗結果</span>
        </div>

        <!-- Hero Section -->
        <div class="hero-section" style="background-image: url('images/generated/hero-06-hope.png');">
            <div class="hero-overlay"></div>
            <div class="hero-content">
                <h1>Hope：自我進化的希望</h1>
                <div class="hero-subtitle">當理論付諸實踐，我們得到了一個能自我修改的模型</div>
                <div class="hero-meta">Chapter 6 · 實證分析</div>
            </div>
        </div>

        <!-- Story Container -->
        <div class="story-container">
            <div class="story-lead">
                所有的理論都很美好，但它真的管用嗎？讓我們來看看 Hope (A Self-Referential Learning Module with Continuum Memory) 模型的實戰表現。
            </div>

            <p class="drop-cap">
                Hope 不僅僅是一個新模型，它是 Nested Learning 理念的集大成者。它結合了 Self-Modifying Titans（自我修改的泰坦）架構和 Continuum Memory System（連續體記憶系統），實現了「自我參照」的學習能力。
            </p>

            <p>
                最令人興奮的特性是：<strong>它學會了如何學習</strong>。它的內層優化器不僅僅是預先寫好的數學公式，而是一個可以被訓練、被修改的學習模組。
            </p>

            <div class="original-quote">
                "Our Hope architecture based on a self-modifying Titans and CMS improves continual learning and long-context reasoning capabilities, while remaining competitive as a general backbone."
                <br><br>
                <strong>📄 論文原文翻譯：</strong> 我們的 Hope 架構基於 Self-Modifying Titans 和 CMS，改進了持續學習和長上下文推理能力，同時作為通用骨幹網路仍保持競爭力。
            </div>

            <div class="section-divider"><span>✦</span></div>

            <h2>實驗 1：持續學習能力 (Continual Learning)</h2>

            <p>
                這是本次論文的重頭戲。在 Class-Incremental Learning 任務中，模型需要不斷學習新的類別，同時不忘記舊的類別。
            </p>

            <div class="bento-grid">
                <div class="bento-card">
                    <img src="images/original/CLINC.png" alt="CLINC Results" style="width:100%">
                    <p style="text-align:center; font-size:0.9em; color:#666">CLINC (意圖分類)</p>
                </div>
                <div class="bento-card">
                    <img src="images/original/Banking.png" alt="Banking Results" style="width:100%">
                    <p style="text-align:center; font-size:0.9em; color:#666">Banking (銀行業務)</p>
                </div>
                <div class="bento-card">
                    <img src="images/original/DBpedia.png" alt="DBpedia Results" style="width:100%">
                    <p style="text-align:center; font-size:0.9em; color:#666">DBpedia (維基百科)</p>
                </div>
            </div>

            <div class="myboxi">
                <strong>📊 解讀：</strong> 
                看看那條紅線（Hope）！在所有測試中，Hope 都顯著超越了其他方法（包括 Llama-3 的 In-context Learning 和 EWC）。這證明了多層級記憶設計確實能有效抵抗災難性遺忘。
            </div>

            <h2>實驗 2：長上下文理解 (Long Context)</h2>

            <p>
                如果模型記性好，那它能在像大海撈針一樣的任務中找到答案嗎？
            </p>

            <!-- Original Image: NIAH Levels -->
            <div class="figure figure-original">
                <img src="images/original/NIAH-levels.png" alt="Needle In A Haystack Results">
                <div class="caption">
                    <strong>🖼️ 論文原圖：</strong> 在「大海撈針」(Needle-in-a-Haystack) 測試中，隨著記憶層級（Level）的增加，Hope 的表現穩步提升。這驗證了 Nested Learning 的核心假設：更多的層級 = 更強的計算深度與記憶力。
                </div>
            </div>

            <p>
                而在更極端的 BABILong 測試（上下文長度達到 10M token）中，大多數模型（包括 GPT-4）在超過一定長度後表現都會崩潰。但 Hope 憑藉其 CMS 設計，成功維持了穩定的性能。
            </p>

            <!-- Original Image: BABILong -->
            <div class="figure figure-original">
                <img src="images/original/BABILong.png" alt="BABILong Results">
                <div class="caption">
                    <strong>🖼️ 論文原圖：</strong> BABILong 基準測試結果。當上下文長度超過 128K 時，大多數模型（包括 GPT-4）的性能都會急劇下降。但 Hope 能在 10M token 的極端長度下維持穩定表現，這歸功於其 CMS 設計。
                </div>
            </div>

            <h3>更多長上下文測試</h3>

            <div class="bento-grid">
                <div class="bento-card">
                    <img src="images/original/LongHelath-levels.png" alt="LongHealth Results" style="width:100%">
                    <p style="text-align:center; font-size:0.9em; color:#666">LongHealth (醫療問答)</p>
                </div>
                <div class="bento-card">
                    <img src="images/original/QASPER-levels.png" alt="QASPER Results" style="width:100%">
                    <p style="text-align:center; font-size:0.9em; color:#666">QASPER (論文問答)</p>
                </div>
            </div>

            <div class="myboxi">
                <strong>📊 解讀：</strong> 
                在 LongHealth（醫療問答）和 QASPER（論文問答）任務中，隨著記憶層級的增加，Hope 的表現持續提升。這證明了「更多層級 = 更強的長上下文理解能力」這一核心假設。
            </div>

            <h2>實驗 3：持續翻譯新語言 (Continual Translation)</h2>

            <!-- Original Image: ICL Translate -->
            <div class="figure figure-original">
                <img src="images/original/ICL-Translate.png" alt="ICL Translation Results">
                <div class="caption">
                    <strong>🖼️ 論文原圖：</strong> 持續學習翻譯任務 (CTNL)。模型需要先後學習兩種新語言（Manchu 和 Kalamang）並翻譯成英文。藍色點表示「連續學習」設定，紅色點表示「單獨學習」設定。可以看到 Hope-3（最多層級）幾乎不受災難性遺忘影響。
                </div>
            </div>

            <h2>實驗 4：語言建模 (Language Modeling)</h2>

            <p>
                除了特技表演，基本功如何？在標準的語言建模任務（WikiText, PIQA 等）上，Hope 也展現出了強大的實力。
            </p>

            <p>
                它不僅擊敗了同量級的 Transformer，甚至在參數效率上超越了許多現代的線性 RNN 模型（如 Mamba, RWKV）。這說明 Nested Learning 不僅是為了解決邊緣案例，它本身就是一個更高效的通用架構。
            </p>

            <div class="section-divider"><span>✦</span></div>

            <h2>結語：未來的路</h2>

            <p>
                Nested Learning 告訴我們，深度學習的未來可能不在於堆疊更多的靜態層，而在於設計更豐富的<strong>動態層級</strong>。
            </p>
            
            <p>
                從大腦的神經振盪到 Git 的分支管理，從優化器的記憶到模型的自我進化，這一切都指向同一個方向：<strong>學習是一個巢狀的過程</strong>。
            </p>

            <div class="original-quote">
                "While Hope and CMS have shown promising results in reducing catastrophic forgetting in the tasks we empirically studied, the undesirable phenomenon of catastrophic forgetting is not 'solved' in general. We view NL as a roadmap rather than a destination: it suggests that progress on continual learning, long-context reasoning, modern optimizers, and self-modifying models will come from better exploiting the extra design axis of 'levels' rather than from ever-deeper static networks."
                <br><br>
                <strong>📄 論文原文翻譯：</strong> 雖然 Hope 和 CMS 在我們實證研究的任務中顯示出減少災難性遺忘的可喜成果，但這一不良現象並沒有被「徹底解決」。我們將 NL 視為一張路線圖，而非終點：它暗示持續學習、長上下文推理、現代優化器和自我修改模型的進展，將來自於更好地利用「層級」這一額外設計軸，而非更深的靜態網路。
            </div>

            <div class="quote-block">
                「災難性遺忘並沒有被徹底解決，但我們找到了一張地圖。這張地圖告訴我們：向『內』看，去探索層級的深度，而不僅僅是網路的深度。」
            </div>

            <p style="text-align: center; margin-top: 60px;">
                <a href="https://arxiv.org/abs/2512.24695" target="_blank" class="btn-primary" style="background: var(--mag-primary); color: white; padding: 15px 30px; border-radius: 99px; text-decoration: none; font-weight: bold; box-shadow: 0 4px 15px rgba(59, 130, 246, 0.4);">閱讀完整論文</a>
            </p>

            <!-- Navigation -->
            <div class="chapter-navigation">
                <a href="05-continuum-memory.html" class="nav-button">⬅️ 上一章：連續體記憶系統</a>
                <a href="index.html" class="nav-link">🏠 返回目錄</a>
            </div>
        </div>
    </div>
</body>
</html>